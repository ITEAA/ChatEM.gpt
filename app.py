import os
import json
import fitz # PyMuPDF
import openai
import uuid
import re # ì •ê·œí‘œí˜„ì‹ ëª¨ë“ˆ ì¶”ê°€
import traceback # ì˜¤ë¥˜ ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤ ì¶œë ¥ì„ ìœ„í•œ ëª¨ë“ˆ ì¶”ê°€

import torch
from transformers import AutoTokenizer, AutoModel

from flask import Flask, request, jsonify, render_template, session
from flask_cors import CORS
# from werkzeug.utils import secure_filename # ì‚¬ìš©í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ì œê±°

from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import TfidfVectorizer

app = Flask(__name__)
CORS(app)
# ì„¸ì…˜ ê´€ë¦¬ë¥¼ ìœ„í•œ secret_key ì„¤ì • (ë°˜ë“œì‹œ ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” ê°•ë ¥í•œ ë¬´ì‘ìœ„ ë¬¸ìì—´ë¡œ ë³€ê²½í•´ì•¼ í•¨!)
app.secret_key = os.getenv("FLASK_SECRET_KEY", "change-this-to-a-super-secret-random-string")

# --- API í‚¤ ë¡œë”© ---
api_key = os.getenv("OPENAI_API_KEY")
if not api_key:
    raise ValueError("ì˜¤ë¥˜: OPENAI_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í”„ë¡œê·¸ë¨ì„ ì‹œì‘í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
openai.api_key = api_key

GG_CACHED_FILE = "gg_employment_cached.json"
user_states = {} # ì‚¬ìš©ìë³„ ëŒ€í™” ìƒíƒœë¥¼ ì €ì¥í•  ë”•ì…”ë„ˆë¦¬ (ì¸ë©”ëª¨ë¦¬)

# --- ê¸°ì—… ì •ë³´ ë¡œë”© ë° ì‚¬ì „ ì²˜ë¦¬ (TF-IDF ë²¡í„°í™”, KoBERT ì„ë² ë”©) ---
cached_companies = []
tfidf_vectorizer = None
company_tfidf_matrix = None

try:
    if not os.path.exists(GG_CACHED_FILE):
        print(f"ê²½ê³ : '{GG_CACHED_FILE}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë¹ˆ íšŒì‚¬ ëª©ë¡ìœ¼ë¡œ ì‹œì‘í•©ë‹ˆë‹¤.")
    else:
        with open(GG_CACHED_FILE, "r", encoding="utf-8") as f:
            cached_companies = json.load(f)
        print(f"âœ… '{GG_CACHED_FILE}'ì—ì„œ {len(cached_companies)}ê°œ ê¸°ì—… ì •ë³´ ë¡œë“œ ì„±ê³µ.")

    # KoBERT ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ (ì „ì—­ ë³€ìˆ˜ë¡œ ì„¤ì •)
    tokenizer = AutoTokenizer.from_pretrained("monologg/kobert")
    model = AutoModel.from_pretrained("monologg/kobert")
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)
    model.eval()
    print("âœ… KoBERT ëª¨ë¸ ë¡œë“œ ì„±ê³µ!")

    # --- ì„œë²„ ì‹œì‘ ì‹œ ëª¨ë“  ê¸°ì—… ì •ë³´ì— ëŒ€í•œ KoBERT ì„ë² ë”© ë¯¸ë¦¬ ê³„ì‚° ---
    print("ì„œë²„ ì‹œì‘ ì „, ê¸°ì—… ì •ë³´ KoBERT ì„ë² ë”©ì„ ì‹œì‘í•©ë‹ˆë‹¤... (ë°ì´í„° ì–‘ì— ë”°ë¼ ëª‡ ë¶„ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
    def get_kobert_embedding_for_startup(text_input):
        if not text_input:
            # í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆì„ ê²½ìš° 0ìœ¼ë¡œ ì±„ì›Œì§„ ì„ë² ë”© ë°˜í™˜
            return torch.zeros(model.config.hidden_size).to(device)
        try:
            inputs = tokenizer(text_input, return_tensors="pt", truncation=True, padding=True, max_length=512)
            inputs = {k: v.to(device) for k, v in inputs.items()}
            with torch.no_grad():
                outputs = model(**inputs)
            embedding = torch.mean(outputs.last_hidden_state, dim=1).squeeze()
            return embedding
        except Exception as e:
            print(f"âŒ ì„ë² ë”© ìƒì„± ì˜¤ë¥˜ (í…ìŠ¤íŠ¸: '{text_input[:30]}...'): {e}")
            traceback.print_exc() # ì˜¤ë¥˜ ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤ ì¶œë ¥
            return torch.zeros(model.config.hidden_size).to(device)

    for company in cached_companies:
        # 'ì±„ìš©ê³µê³ ëª…'ê³¼ 'íšŒì‚¬ëª…'ì„ ì¡°í•©í•˜ì—¬ ì„ë² ë”© ìƒì„±
        summary = f"{company.get('ì±„ìš©ê³µê³ ëª…', '')} {company.get('íšŒì‚¬ëª…', '')}"
        company['embedding'] = get_kobert_embedding_for_startup(summary)
    print("âœ… ëª¨ë“  ê¸°ì—… ì •ë³´ì˜ KoBERT ì„ë² ë”©ì´ ì™„ë£Œë˜ì–´ ë©”ëª¨ë¦¬ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

    # --- TF-IDF Vectorizer ë° ê¸°ì—…ë³„ TF-IDF í–‰ë ¬ ë¯¸ë¦¬ ê³„ì‚° ---
    print("ì„œë²„ ì‹œì‘ ì „, ê¸°ì—… ì •ë³´ TF-IDF ë²¡í„°í™”ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
    tfidf_vectorizer = TfidfVectorizer(min_df=1, ngram_range=(1, 2))
    company_summaries_for_tfidf = [
        f"{company.get('ì±„ìš©ê³µê³ ëª…', '')} {company.get('íšŒì‚¬ëª…', '')}" for company in cached_companies
    ]
    company_tfidf_matrix = tfidf_vectorizer.fit_transform(company_summaries_for_tfidf)
    print("âœ… ëª¨ë“  ê¸°ì—… ì •ë³´ì˜ TF-IDF ë²¡í„°í™”ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")

except json.JSONDecodeError as e:
    print(f"âŒ ì˜¤ë¥˜: '{GG_CACHED_FILE}' íŒŒì¼ì´ ìœ íš¨í•œ JSON í˜•ì‹ì´ ì•„ë‹™ë‹ˆë‹¤. ì˜¤ë¥˜: {e}")
    cached_companies = [] # ì˜¤ë¥˜ ë°œìƒ ì‹œ ë¹ˆ ëª©ë¡ìœ¼ë¡œ ì´ˆê¸°í™”
except Exception as e:
    print(f"âŒ ì´ˆê¸° ì„¤ì • ì¤‘ ì¹˜ëª…ì ì¸ ì˜¤ë¥˜ ë°œìƒ: {e}")
    traceback.print_exc()
    # í•„ìˆ˜ ëª¨ë“ˆ ë¡œë”© ì‹¤íŒ¨ ì‹œ ì•± ì‹œì‘ì„ ë§‰ìŠµë‹ˆë‹¤.
    raise RuntimeError("ì• í”Œë¦¬ì¼€ì´ì…˜ ì´ˆê¸° ì„¤ì •ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. í•„ìˆ˜ ëª¨ë¸ ë° ë°ì´í„° ë¡œë”© ì˜¤ë¥˜.")


# --- PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ í•¨ìˆ˜ ---
def extract_text_from_pdf(pdf_file_stream):
    try:
        doc = fitz.open(stream=pdf_file_stream.read(), filetype="pdf")
        text_content = []
        for page in doc:
            text_content.append(page.get_text())
        raw_text = "\n".join(text_content)

        # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ê°•í™”: ì—¬ëŸ¬ ê°œì˜ ê³µë°±(ì¤„ë°”ê¿ˆ, íƒ­ í¬í•¨)ì„ í•˜ë‚˜ì˜ ê³µë°±ìœ¼ë¡œ ì¹˜í™˜
        processed_text = re.sub(r'\s+', ' ', raw_text)
        # ì•ë’¤ ê³µë°± ì œê±°
        processed_text = processed_text.strip()

        # ë””ë²„ê¹…ì„ ìœ„í•´ ì¶”ì¶œëœ í…ìŠ¤íŠ¸ë¥¼ ì¶œë ¥
        print("\n--- PDFì—ì„œ ì¶”ì¶œëœ ì›ë³¸ í…ìŠ¤íŠ¸ (ì•ë¶€ë¶„ 500ì) ---")
        print(raw_text[:500])
        print("-------------------------------------------\n")
        print("--- PDFì—ì„œ ì¶”ì¶œ ë° ì „ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸ (ì•ë¶€ë¶„ 500ì) ---")
        print(processed_text[:500])
        print("---------------------------------------------------\n")

        return processed_text
    except Exception as e:
        print(f"âŒ PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        traceback.print_exc()
        return "" # ì˜¤ë¥˜ ë°œìƒ ì‹œ ë¹ˆ ë¬¸ìì—´ ë°˜í™˜


# --- GPTë¥¼ ì‚¬ìš©í•˜ì—¬ í‚¤ì›Œë“œ ì¶”ì¶œ í•¨ìˆ˜ ---
def extract_keywords(text):
    if not text:
        return []

    prompt = f"""
    ë‹¤ìŒ ìê¸°ì†Œê°œì„œ ë˜ëŠ” ì´ë ¥ì„œì—ì„œ í•µì‹¬ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•´ì¤˜.
    - 5~10ê°œ ì •ë„ ë½‘ì•„ì¤˜.
    - í‚¤ì›Œë“œëŠ” ì½¤ë§ˆ(,)ë¡œ êµ¬ë¶„í•´ì„œ ì¶œë ¥í•´ì¤˜.

    ë‚´ìš©:
    {text}
    """
    try:
        response = openai.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.4
        )
        result = response.choices[0].message.content
        return [kw.strip() for kw in result.split(",") if kw.strip()]
    except Exception as e:
        print(f"âŒ GPT í‚¤ì›Œë“œ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        traceback.print_exc()
        return []

# --- KoBERT ì„ë² ë”© ìƒì„± í•¨ìˆ˜ (ì‚¬ìš©ì í…ìŠ¤íŠ¸ìš©) ---
def get_kobert_embedding(text_input):
    if model is None or tokenizer is None or device is None or not text_input:
        # KoBERT ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ê±°ë‚˜ ì…ë ¥ í…ìŠ¤íŠ¸ê°€ ì—†ì„ ê²½ìš° 0 ë²¡í„° ë°˜í™˜
        return torch.zeros(768).to(device) # KoBERT ê¸°ë³¸ hidden_size
    try:
        inputs = tokenizer(text_input, return_tensors="pt", truncation=True, padding=True, max_length=512)
        inputs = {k: v.to(device) for k, v in inputs.items()}
        with torch.no_grad():
            outputs = model(**inputs)
        embedding = torch.mean(outputs.last_hidden_state, dim=1).squeeze()
        return embedding
    except Exception as e:
        print(f"âŒ KoBERT ì„ë² ë”© ìƒì„± ì˜¤ë¥˜ (í…ìŠ¤íŠ¸: '{text_input[:30]}...'): {e}")
        traceback.print_exc()
        return torch.zeros(768).to(device)

# --- KoBERT ìœ ì‚¬ë„ ê³„ì‚° í•¨ìˆ˜ ---
def kobert_similarity(user_text, companies):
    if not user_text or not companies:
        return []
    user_embedding = get_kobert_embedding(user_text)
    user_embedding_np = user_embedding.cpu().numpy().reshape(1, -1)

    results = []
    for c in companies:
        company_embedding = c.get('embedding') # ë¯¸ë¦¬ ê³„ì‚°ëœ ì„ë² ë”© ì‚¬ìš©
        if company_embedding is not None and not torch.all(company_embedding == 0):
            company_embedding_np = company_embedding.cpu().numpy().reshape(1, -1)
            score = cosine_similarity(user_embedding_np, company_embedding_np)[0][0]
            results.append((c, float(score)))
    return sorted(results, key=lambda x: x[1], reverse=True)

# --- TF-IDF ìœ ì‚¬ë„ ê³„ì‚° í•¨ìˆ˜ ---
def tfidf_similarity(user_text, companies):
    if not user_text or not companies or tfidf_vectorizer is None or company_tfidf_matrix is None:
        return []
    try:
        user_tfidf_vector = tfidf_vectorizer.transform([user_text])
        scores = cosine_similarity(user_tfidf_vector, company_tfidf_matrix).flatten()
        results = [(companies[i], float(scores[i])) for i in range(len(scores))]
        return results
    except Exception as e:
        print(f"âŒ TF-IDF ìœ ì‚¬ë„ ê³„ì‚° ì˜¤ë¥˜: {e}")
        traceback.print_exc()
        return []

# --- GPTë¥¼ ì‚¬ìš©í•˜ì—¬ ì¶”ì²œ ì´ìœ  ìƒì„± í•¨ìˆ˜ ---
def generate_reason_individual(user_text, company, score):
    prompt = f"""
    ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ íŠ¹ì„±ê³¼ ì„ í˜¸ë„ë¥¼ íŒŒì•…í•´ ê°€ì¥ ì í•©í•œ ê¸°ì—…ì„ ë§¤ì¹­ì‹œì¼œì£¼ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
    ì‚¬ìš©ìì˜ ìê¸°ì†Œê°œì„œ ë‚´ìš©ê³¼ ê¸°ì—… ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ, ì™œ ì´ ê¸°ì—…ì´ ì‚¬ìš©ìì—ê²Œ ì í•©í•œì§€ ìƒì„¸í•˜ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”.
    ì¹œì ˆí•˜ê³  ì „ë¬¸ì ì¸ ì–´ì¡°ë¡œ, ì„¤ëª…ì€ ìì—°ìŠ¤ëŸ½ê³  ëª…í™•í•˜ê²Œ ì „ë‹¬í•´ì•¼ í•©ë‹ˆë‹¤.
    ì‹œìŠ¤í…œ ì‘ë™ ê´€ë ¨ ë¬¸êµ¬("ë¶„ì„ëª¨ë“œì…ë‹ˆë‹¤" ë“±)ëŠ” ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.
    ë¶„ì„ì´ ì–´ë ¤ìš´ ê²½ìš° "í˜„ì¬ ì •ë³´ë§Œìœ¼ë¡œëŠ” ë¶„ì„ì´ ì–´ë µìŠµë‹ˆë‹¤"ì™€ ê°™ì´ ìì—°ìŠ¤ëŸ½ê²Œ ì•ˆë‚´í•´ì£¼ì„¸ìš”.

    [ê¸°ì—… ì •ë³´]
    - ê¸°ì—…ëª…: {company.get('íšŒì‚¬ëª…', 'ì •ë³´ ì—†ìŒ')}
    - ì±„ìš©ê³µê³ ëª…: {company.get('ì±„ìš©ê³µê³ ëª…', 'ì •ë³´ ì—†ìŒ')}
    - ìœ ì‚¬ë„ ì ìˆ˜: {round(score, 2)}

    [ì‚¬ìš©ì ìê¸°ì†Œê°œì„œ]
    {user_text}

    [ì„¤ëª… ì‹œì‘]
    """
    try:
        response = openai.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.4
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        print(f"âŒ GPT ì„¤ëª… ìƒì„± ì˜¤ë¥˜: {e}")
        traceback.print_exc()
        return "ì„¤ëª…ì„ ìƒì„±í•˜ëŠ” ë° ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."


# --- ê¸°ì—… ì¶”ì²œ ë¡œì§ í•¨ìˆ˜ (Hybrid ëª¨ë¸) ---
def make_recommendations(user_text, interest, region, salary, shown_companies_set=None, top_n=3):
    if shown_companies_set is None:
        shown_companies_set = set()

    if not user_text:
        return []

    # 1. TF-IDF ê¸°ë°˜ìœ¼ë¡œ 1ì°¨ í•„í„°ë§í•˜ì—¬ í›„ë³´ ê¸°ì—…êµ° ì„ ì •
    # (tfidf_similarity í•¨ìˆ˜ê°€ ì „ì²´ cached_companiesì— ëŒ€í•´ ë™ì‘í•˜ë„ë¡ ìˆ˜ì • í•„ìš”)
    # í˜„ì¬ tfidf_similarityëŠ” ì´ë¯¸ make_recommendations ë‚´ì—ì„œ í˜¸ì¶œë©ë‹ˆë‹¤.
    # make_recommendations ë‚´ë¶€ì˜ tfidf_scoresë¥¼ ìƒì„±í•˜ëŠ” ë¡œì§ì€ ì´ë¯¸ ì „ì²´ cached_companiesë¥¼ ëŒ€ìƒìœ¼ë¡œ í•©ë‹ˆë‹¤.

    # 2. KoBERT ê¸°ë°˜ ìœ ì‚¬ë„ ê³„ì‚° (ì „ì²´ ìºì‹±ëœ ê¸°ì—… ëŒ€ìƒ)
    kobert_ranked_companies = kobert_similarity(user_text, cached_companies)

    # 3. TF-IDF ê¸°ë°˜ ìœ ì‚¬ë„ ê³„ì‚° (ì „ì²´ ìºì‹±ëœ ê¸°ì—… ëŒ€ìƒ)
    tfidf_ranked_companies = tfidf_similarity(user_text, cached_companies)
    
    # 4. ì ìˆ˜ í•©ì‚°ì„ ìœ„í•´ íšŒì‚¬ ì •ë³´ë¥¼ keyë¡œ í•˜ëŠ” ë”•ì…”ë„ˆë¦¬ ìƒì„±
    tfidf_scores = {
        (c.get("íšŒì‚¬ëª…"), c.get("ì±„ìš©ê³µê³ ëª…")): score
        for c, score in tfidf_ranked_companies
    }

    # 5. Hybrid ì ìˆ˜ ê³„ì‚° (KoBERT ì ìˆ˜ + TF-IDF ì ìˆ˜)
    hybrid_scores = []
    for company, kobert_score in kobert_ranked_companies:
        company_key = (company.get("íšŒì‚¬ëª…"), company.get("ì±„ìš©ê³µê³ ëª…"))
        tfidf_score = tfidf_scores.get(company_key, 0.0)

        # ê°€ì¤‘ì¹˜ ì„¤ì • (KoBERT: 70%, TF-IDF: 30%) - ì´ ê°’ì€ ì¡°ì • ê°€ëŠ¥
        kobert_weight = 0.7
        tfidf_weight = 0.3
        
        # ì ìˆ˜ ì •ê·œí™” (í•„ìš”ì‹œ) - í˜„ì¬ëŠ” ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ìì²´ê°€ 0~1ì´ë¯€ë¡œ í° ë¬¸ì œëŠ” ì—†ì§€ë§Œ,
        # TF-IDF ìŠ¤ì½”ì–´ê°€ ë„ˆë¬´ ë‚®ê²Œ ë‚˜ì˜¤ëŠ” ê²½ìš° ê°€ì¤‘ì¹˜ ì¡°ì ˆì´ ì¤‘ìš”
        final_score = (kobert_weight * kobert_score) + (tfidf_weight * tfidf_score)
        
        # ì¶”ê°€ í•„í„°ë§ (ê´€ì‹¬ë¶„ì•¼, ì§€ì—­ ë“±)
        passes_filter = True
        summary = f"{company.get('ì±„ìš©ê³µê³ ëª…', '')} {company.get('íšŒì‚¬ëª…', '')}"
        
        # 'ê´€ì‹¬' í‚¤ì›Œë“œê°€ ì±„ìš©ê³µê³ ëª…/íšŒì‚¬ëª…ì— í¬í•¨ë˜ëŠ”ì§€ í™•ì¸
        if interest and interest.lower() not in summary.lower():
            passes_filter = False
        
        # 'ì§€ì—­' í‚¤ì›Œë“œê°€ ê·¼ë¬´ì§€ì—­ì— í¬í•¨ë˜ëŠ”ì§€ í™•ì¸
        if region and region.lower() not in company.get("ê·¼ë¬´ì§€ì—­", "").lower():
            passes_filter = False
        
        # ì—°ë´‰ í•„í„°ë§ì€ í˜„ì¬ êµ¬í˜„ë˜ì–´ ìˆì§€ ì•Šìœ¼ë¯€ë¡œ, í•„ìš”ì‹œ ì—¬ê¸°ì— ì¶”ê°€
        # if salary:
        #    try:
        #        min_salary_req = int(salary)
        #        company_salary_info = company.get("ì—°ë´‰ì •ë³´", "") # ì‹¤ì œ ë°ì´í„° í‚¤ì— ë”°ë¼ ë³€ê²½
        #        # ... ì—°ë´‰ ì •ë³´ íŒŒì‹± ë° ë¹„êµ ë¡œì§ ì¶”ê°€ ...
        #    except ValueError:
        #        pass # ìœ íš¨í•˜ì§€ ì•Šì€ ì—°ë´‰ ì…ë ¥ì€ ë¬´ì‹œ

        if passes_filter:
            hybrid_scores.append((company, final_score))

    # ìµœì¢… ì ìˆ˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
    hybrid_scores.sort(key=lambda x: x[1], reverse=True)
    
    # ì´ë¯¸ ë³´ì—¬ì¤€ ê³µê³  ì œì™¸í•˜ê³  ìƒìœ„ Nê°œ ì„ íƒ
    results = []
    for comp, sim in hybrid_scores:
        comp_id = (comp.get("íšŒì‚¬ëª…"), comp.get("ì±„ìš©ê³µê³ ëª…"))
        if comp_id not in shown_companies_set:
            shown_companies_set.add(comp_id) # ì´ë¯¸ ë³´ì—¬ì¤€ íšŒì‚¬ ëª©ë¡ì— ì¶”ê°€
            results.append((comp, sim))
        if len(results) >= top_n:
            break
            
    return results


# --- Flask ë¼ìš°íŠ¸ ì„¤ì • ---
@app.route("/")
def index():
    return render_template("index.html")

@app.route("/chat", methods=["POST"])
def chat():
    # ì‚¬ìš©ì IDê°€ ì„¸ì…˜ì— ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±
    if 'user_id' not in session:
        session['user_id'] = str(uuid.uuid4())
    user_id = session['user_id']
    
    # user_states ë”•ì…”ë„ˆë¦¬ì—ì„œ í˜„ì¬ ì‚¬ìš©ì ìƒíƒœ ë¡œë“œ
    state = user_states.get(user_id, {
        "shown": set(), # ì´ë¯¸ ì¶”ì²œëœ ê¸°ì—… ëª©ë¡ (ì¤‘ë³µ ì¶”ì²œ ë°©ì§€)
        "user_text": None, # ì‚¬ìš©ìì˜ ìê¸°ì†Œê°œì„œ/ì´ë ¥ì„œ í…ìŠ¤íŠ¸
        "interest": None, # ì‚¬ìš©ì ê´€ì‹¬ ë¶„ì•¼
        "region": None, # ì‚¬ìš©ì í¬ë§ ê·¼ë¬´ì§€
        "salary": None # ì‚¬ìš©ì í¬ë§ ì—°ë´‰
    })

    message = request.form.get("message", "").strip()
    file = request.files.get("file")

    try:
        # 1. íŒŒì¼ ì²¨ë¶€ ì‹œ ìê¸°ì†Œê°œì„œ/ì´ë ¥ì„œ ì¶”ì¶œ
        if file and file.filename != '':
            user_text = extract_text_from_pdf(file)
            if user_text:
                state["user_text"] = user_text
                # íŒŒì¼ ì—…ë¡œë“œ ì‹œì—ëŠ” ê¸°ì¡´ ì¶”ì²œ ì´ë ¥ì„ ì´ˆê¸°í™”
                state["shown"] = set()
                user_states[user_id] = state
                return jsonify({"reply": "ê°ì‚¬í•©ë‹ˆë‹¤. ì´ë ¥ì„œ/ìê¸°ì†Œê°œì„œ ë‚´ìš©ì´ ì„±ê³µì ìœ¼ë¡œ ë¶„ì„ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ì œ **ê´€ì‹¬ ë¶„ì•¼, í¬ë§ ê·¼ë¬´ì§€, ì—°ë´‰**ì„ 'í’ˆì§ˆ, ì„œìš¸, 3000ë§Œì›' ë˜ëŠ” 'ì—†ìŒ, ì—†ìŒ, ì—†ìŒ'ê³¼ ê°™ì´ ì…ë ¥í•´ ì£¼ì„¸ìš”."})
            else:
                return jsonify({"reply": "PDF íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. íŒŒì¼ì´ ìŠ¤ìº”ëœ ì´ë¯¸ì§€ ê¸°ë°˜ì´ê±°ë‚˜ ì†ìƒë˜ì—ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. í…ìŠ¤íŠ¸ë¥¼ ì§ì ‘ ì…ë ¥í•´ ì£¼ì‹œê±°ë‚˜ ë‹¤ë¥¸ íŒŒì¼ì„ ì‹œë„í•´ ì£¼ì‹œê² ì–´ìš”?"})

        # 2. íŒŒì¼ ì—†ì´ ë©”ì‹œì§€ë§Œ ìˆì„ ê²½ìš°
        if state["user_text"] is None:
            # ë©”ì‹œì§€ ê¸¸ì´ê°€ ê¸¸ê±°ë‚˜ íŠ¹ì • í‚¤ì›Œë“œë¥¼ í¬í•¨í•˜ë©´ ìê¸°ì†Œê°œì„œ/ì´ë ¥ì„œë¡œ ê°„ì£¼
            if len(message.split()) > 30 or "ì´ë ¥ì„œ" in message or "ìê¸°ì†Œê°œì„œ" in message:
                state["user_text"] = message
                state["shown"] = set() # í…ìŠ¤íŠ¸ ì§ì ‘ ì…ë ¥ ì‹œì—ë„ ê¸°ì¡´ ì¶”ì²œ ì´ë ¥ ì´ˆê¸°í™”
                user_states[user_id] = state
                return jsonify({"reply": "ì´ë ¥ì„œ/ìê¸°ì†Œê°œì„œ ë‚´ìš©ì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤. ì´ì œ **ê´€ì‹¬ ë¶„ì•¼, í¬ë§ ê·¼ë¬´ì§€, ì—°ë´‰**ì„ 'í’ˆì§ˆ, ì„œìš¸, 3000ë§Œì›' ë˜ëŠ” 'ì—†ìŒ, ì—†ìŒ, ì—†ìŒ'ê³¼ ê°™ì´ ì…ë ¥í•´ ì£¼ì„¸ìš”."})
            else:
                # ìê¸°ì†Œê°œì„œ/ì´ë ¥ì„œê°€ ì—†ëŠ” ê²½ìš° ì¼ë°˜ ìƒë‹´ ëª¨ë“œ ë©”ì‹œì§€
                return jsonify({"reply": "ê°œì¸ë³„ ë§ì¶¤ ë¶„ì„ì„ ìœ„í•´ì„œëŠ” ìê¸°ì†Œê°œì„œ í˜¹ì€ ì´ë ¥ì„œê°€ í•„ìš”í•©ë‹ˆë‹¤. íŒŒì¼ì„ ì²¨ë¶€í•´ ì£¼ì‹œê±°ë‚˜ ë‚´ìš©ì„ ì§ì ‘ ì…ë ¥í•´ ì£¼ì‹œë©´ ìƒì„¸í•œ ë¶„ì„ì„ ì œê³µí•´ ë“œë¦¬ê² ìŠµë‹ˆë‹¤."})

        # 3. ìê¸°ì†Œê°œì„œ/ì´ë ¥ì„œê°€ ì…ë ¥ë˜ì—ˆê³ , ì‚¬ìš©ì ì„ í˜¸ë„ ì •ë³´ê°€ ì—†ëŠ” ê²½ìš°
        if state["user_text"] is not None and state["interest"] is None:
            if "," in message:
                parts = [p.strip() for p in message.split(",")]
                state["interest"] = parts[0] if len(parts) > 0 and parts[0].lower() != "ì—†ìŒ" else ""
                state["region"] = parts[1] if len(parts) > 1 and parts[1].lower() != "ì—†ìŒ" else ""
                state["salary"] = parts[2].replace("ë§Œì›", "") if len(parts) > 2 and parts[2].lower() != "ì—†ìŒ" else ""
                user_states[user_id] = state

                # ì„ í˜¸ë„ ì…ë ¥ í›„ ì²« ì¶”ì²œ ì‹œì‘
                new_recommendations = make_recommendations(
                    user_text=state["user_text"],
                    interest=state.get("interest"),
                    region=state.get("region"),
                    salary=state.get("salary"),
                    shown_companies_set=state["shown"],
                    top_n=3 # ì²« ì¶”ì²œì€ 3ê°œ
                )

                if not new_recommendations:
                    return jsonify({"reply": "ì•„ì‰½ê²Œë„ í˜„ì¬ ì¡°ê±´ì— ë§ëŠ” ê¸°ì—…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì¡°ê±´ì„ ë§ì”€í•´ì£¼ì‹œê±°ë‚˜ 'ì¶”ì²œ ì´ˆê¸°í™”'ë¥¼ í†µí•´ ë‹¤ì‹œ ì‹œì‘í•´ ì£¼ì‹œê² ì–´ìš”?"})

                explanations = []
                for company, score in new_recommendations:
                    # ì„ë² ë”© ì •ë³´ëŠ” í´ë¼ì´ì–¸íŠ¸ì—ê²Œ ë³´ë‚¼ í•„ìš” ì—†ìœ¼ë¯€ë¡œ ì œê±°
                    company_info_for_gpt = {k: v for k, v in company.items() if k != 'embedding'}
                    reason = generate_reason_individual(state["user_text"], company_info_for_gpt, score)
                    explanations.append(f"**ê¸°ì—…ëª…**: {company_info_for_gpt.get('íšŒì‚¬ëª…', 'ì •ë³´ ì—†ìŒ')}\n**ì±„ìš©ê³µê³ ëª…**: {company_info_for_gpt.get('ì±„ìš©ê³µê³ ëª…', 'ì •ë³´ ì—†ìŒ')}\n**ì¢…í•© ì ìˆ˜**: {round(score,2)}\n**ì„¤ëª…**: {reason}\n")

                reply = "\n\n".join(explanations)
                reply += "\n\nğŸ“Œ ë” ê¶ê¸ˆí•œ ì ì´ë‚˜ ê³ ë ¤í•˜ê³  ì‹¶ì€ ì¡°ê±´ì´ ìˆë‹¤ë©´ ë§ì”€í•´ ì£¼ì„¸ìš”. ì¶”ê°€ë¡œ ë°˜ì˜í•´ ë“œë¦´ê²Œìš”! ì˜ˆë¥¼ ë“¤ì–´ 'ë” ì¶”ì²œí•´ì¤˜'ë¼ê³  ë§ì”€í•˜ì‹œë©´ ë‹¤ë¥¸ ê¸°ì—…ì„ ì°¾ì•„ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
                return jsonify({"reply": reply})
            else:
                return jsonify({"reply": "ê´€ì‹¬ ë¶„ì•¼, í¬ë§ ê·¼ë¬´ì§€, ì—°ë´‰ì„ 'í’ˆì§ˆ, ì„œìš¸, 3000ë§Œì›' ë˜ëŠ” 'ì—†ìŒ, ì—†ìŒ, ì—†ìŒ'ê³¼ ê°™ì´ ì½¤ë§ˆ(,)ë¡œ êµ¬ë¶„í•´ì„œ ì…ë ¥í•´ ì£¼ì„¸ìš”."})

        # 4. "ë” ì¶”ì²œí•´ì¤˜" ìš”ì²­ ì²˜ë¦¬
        if state["user_text"] is not None and state["interest"] is not None and "ë” ì¶”ì²œí•´ì¤˜" in message:
            new_recommendations = make_recommendations(
                user_text=state["user_text"],
                interest=state.get("interest"),
                region=state.get("region"),
                salary=state.get("salary"),
                shown_companies_set=state["shown"],
                top_n=1 # ì¶”ê°€ ì¶”ì²œì€ 1ê°œì”©
            )

            if not new_recommendations:
                return jsonify({"reply": "ë” ì´ìƒ ì¶”ì²œí•  ê¸°ì—…ì´ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì¡°ê±´ì„ ë§ì”€í•´ì£¼ì‹œê±°ë‚˜ 'ì¶”ì²œ ì´ˆê¸°í™”'ë¥¼ í†µí•´ ë‹¤ì‹œ ì‹œì‘í•´ ì£¼ì‹œê² ì–´ìš”?"})

            explanations = []
            for company, score in new_recommendations:
                company_info_for_gpt = {k: v for k, v in company.items() if k != 'embedding'}
                reason = generate_reason_individual(state["user_text"], company_info_for_gpt, score)
                explanations.append(f"**ê¸°ì—…ëª…**: {company_info_for_gpt.get('íšŒì‚¬ëª…', 'ì •ë³´ ì—†ìŒ')}\n**ì±„ìš©ê³µê³ ëª…**: {company_info_for_gpt.get('ì±„ìš©ê³µê³ ëª…', 'ì •ë³´ ì—†ìŒ')}\n**ì¢…í•© ì ìˆ˜**: {round(score,2)}\n**ì„¤ëª…**: {reason}\n")

            reply = "\n\n".join(explanations)
            reply += "\n\nğŸ“Œ ë” ê¶ê¸ˆí•œ ì ì´ë‚˜ ê³ ë ¤í•˜ê³  ì‹¶ì€ ì¡°ê±´ì´ ìˆë‹¤ë©´ ë§ì”€í•´ ì£¼ì„¸ìš”. ì¶”ê°€ë¡œ ë°˜ì˜í•´ ë“œë¦´ê²Œìš”! ë˜ëŠ” 'ì¶”ì²œ ì´ˆê¸°í™”'ë¼ê³  ë§ì”€í•˜ì‹œë©´ ì²˜ìŒë¶€í„° ë‹¤ì‹œ ì‹œì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
            return jsonify({"reply": reply})
            
        # 5. "ì¶”ì²œ ì´ˆê¸°í™”" ìš”ì²­ ì²˜ë¦¬
        if "ì¶”ì²œ ì´ˆê¸°í™”" in message:
            user_states[user_id] = {"shown": set(), "user_text": None, "interest": None, "region": None, "salary": None}
            return jsonify({"reply": "ì¶”ì²œ ìƒíƒœê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤. ìƒˆë¡œìš´ ìê¸°ì†Œê°œì„œ/ì´ë ¥ì„œ íŒŒì¼ì„ ì²¨ë¶€í•˜ì‹œê±°ë‚˜ ë‚´ìš©ì„ ì§ì ‘ ì…ë ¥í•´ ì£¼ì„¸ìš”."})

        # ê¸°íƒ€ ì¼ë°˜ ë©”ì‹œì§€ ì²˜ë¦¬ (ìê¸°ì†Œê°œì„œ/ì´ë ¥ì„œ ë° ì„ í˜¸ë„ ì…ë ¥ í›„)
        # ì—¬ê¸°ì— ì¼ë°˜ì ì¸ ì§ˆì˜ì‘ë‹µ ë¡œì§ì„ ì¶”ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        # ì˜ˆ: ê¸°ì—… ì •ë³´ë¥¼ ë¬»ëŠ” ì§ˆë¬¸, ë©´ì ‘ ê´€ë ¨ ì§ˆë¬¸ ë“±.
        # í˜„ì¬ëŠ” ì´ ë‹¨ê³„ì—ì„œ ë‹¤ë¥¸ ì§ˆë¬¸ì´ ì˜¤ë©´ ì´í•´í•˜ì§€ ëª»í•œë‹¤ê³  ì‘ë‹µí•©ë‹ˆë‹¤.
        return jsonify({"reply": "ë¬´ìŠ¨ ë§ì”€ì´ì‹ ì§€ ì •í™•íˆ ì´í•´í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ìê¸°ì†Œê°œì„œ/ì´ë ¥ì„œë¥¼ ì²¨ë¶€í•´ ì£¼ì‹œê±°ë‚˜, 'ì¶”ì²œ ì´ˆê¸°í™”'ë¥¼ í†µí•´ ë‹¤ì‹œ ì‹œì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."})

    except Exception as e:
        print(f"âŒ ì„œë²„ ì—ëŸ¬: {e}")
        traceback.print_exc() # ì„œë²„ ì „ì²´ ì˜¤ë¥˜ ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤ ì¶œë ¥
        return jsonify({"reply": f"âŒ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)} ë¶ˆí¸ì„ ë“œë ¤ ì£„ì†¡í•©ë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”."}), 500

if __name__ == "__main__":
    app.run(debug=True, host="0.0.0.0", port=8080)
