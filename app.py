import os
import json
import fitz  # PyMuPDF
import openai
import uuid
from flask import Flask, request, jsonify, render_template, session
from flask_cors import CORS
from werkzeug.utils import secure_filename
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import TfidfVectorizer  # [TF-IDF ì¶”ê°€]
import torch
from transformers import AutoTokenizer, AutoModel

app = Flask(__name__)
CORS(app)
app.secret_key = "change-this-to-a-super-secret-random-string"

# --- API í‚¤ ë¡œë”© ---
api_key = os.getenv("OPENAI_API_KEY")
if not api_key:
    raise ValueError("ì˜¤ë¥˜: OPENAI_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í”„ë¡œê·¸ë¨ì„ ì‹œì‘í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
openai.api_key = api_key

GG_CACHED_FILE = "gg_employment_cached.json"
user_states = {}

try:
    with open(GG_CACHED_FILE, "r", encoding="utf-8") as f:
        cached_companies = json.load(f)
except FileNotFoundError:
    print(f"ê²½ê³ : '{GG_CACHED_FILE}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë¹ˆ íšŒì‚¬ ëª©ë¡ìœ¼ë¡œ ì‹œì‘í•©ë‹ˆë‹¤.")
    cached_companies = []
except json.JSONDecodeError:
    print(f"ì˜¤ë¥˜: '{GG_CACHED_FILE}' íŒŒì¼ì´ ìœ íš¨í•œ JSON í˜•ì‹ì´ ì•„ë‹™ë‹ˆë‹¤.")
    cached_companies = []

# --- KoBERT ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ ---
try:
    tokenizer = AutoTokenizer.from_pretrained("monologg/kobert")
    model = AutoModel.from_pretrained("monologg/kobert")
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)
    model.eval()
    print("KoBERT ëª¨ë¸ ë¡œë“œ ì„±ê³µ!")
except Exception as e:
    print(f"âŒ KoBERT ëª¨ë¸ ë¡œë“œ ì˜¤ë¥˜: {e}")
    raise e

# --- ì„œë²„ ì‹œì‘ ì‹œ ëª¨ë“  ê¸°ì—… ì •ë³´ì— ëŒ€í•œ ì„ë² ë”© ë¯¸ë¦¬ ê³„ì‚° ---
print("ì„œë²„ ì‹œì‘ ì „, ê¸°ì—… ì •ë³´ ì„ë² ë”©ì„ ì‹œì‘í•©ë‹ˆë‹¤... (ë°ì´í„° ì–‘ì— ë”°ë¼ ëª‡ ë¶„ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
def get_kobert_embedding_for_startup(text):
    if not text:
        return torch.zeros(model.config.hidden_size).to(device)
    try:
        inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=512)
        inputs = {k: v.to(device) for k, v in inputs.items()}
        with torch.no_grad():
            outputs = model(**inputs)
        embedding = torch.mean(outputs.last_hidden_state, dim=1).squeeze()
        return embedding
    except Exception as e:
        print(f"âŒ ì„ë² ë”© ìƒì„± ì˜¤ë¥˜ (í…ìŠ¤íŠ¸: '{text[:30]}...'): {e}")
        return torch.zeros(model.config.hidden_size).to(device)

for company in cached_companies:
    summary = f"{company.get('ì±„ìš©ê³µê³ ëª…', '')} {company.get('íšŒì‚¬ëª…', '')}"
    company['embedding'] = get_kobert_embedding_for_startup(summary)
print("âœ… ëª¨ë“  ê¸°ì—… ì •ë³´ì˜ KoBERT ì„ë² ë”©ì´ ì™„ë£Œë˜ì–´ ë©”ëª¨ë¦¬ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

# --- [TF-IDF ì¶”ê°€] TF-IDF Vectorizer ë° ê¸°ì—…ë³„ TF-IDF í–‰ë ¬ ë¯¸ë¦¬ ê³„ì‚° ---
print("ì„œë²„ ì‹œì‘ ì „, ê¸°ì—… ì •ë³´ TF-IDF ë²¡í„°í™”ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
tfidf_vectorizer = TfidfVectorizer(min_df=1, ngram_range=(1, 2))
company_summaries_for_tfidf = [
    f"{company.get('ì±„ìš©ê³µê³ ëª…', '')} {company.get('íšŒì‚¬ëª…', '')}" for company in cached_companies
]
company_tfidf_matrix = tfidf_vectorizer.fit_transform(company_summaries_for_tfidf)
print("âœ… ëª¨ë“  ê¸°ì—… ì •ë³´ì˜ TF-IDF ë²¡í„°í™”ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")


# --- PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ í•¨ìˆ˜ ---
def extract_text_from_pdf(pdf_file):
    doc = fitz.open(stream=pdf_file.read(), filetype="pdf")
    text = "\n".join(page.get_text() for page in doc)
    return text.strip()

# --- GPTë¥¼ ì‚¬ìš©í•˜ì—¬ í‚¤ì›Œë“œ ì¶”ì¶œ í•¨ìˆ˜ ---
def extract_keywords(text):
    prompt = f"""
    ë‹¤ìŒ ìê¸°ì†Œê°œì„œ ë˜ëŠ” ì´ë ¥ì„œì—ì„œ í•µì‹¬ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•´ì¤˜.
    - 5~10ê°œ ì •ë„ ë½‘ì•„ì¤˜.
    - í‚¤ì›Œë“œëŠ” ì½¤ë§ˆ(,)ë¡œ êµ¬ë¶„í•´ì„œ ì¶œë ¥í•´ì¤˜.

    ë‚´ìš©:
    {text}
    """
    try:
        response = openai.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.4
        )
        result = response.choices[0].message.content
        return [kw.strip() for kw in result.split(",") if kw.strip()]
    except Exception as e:
        print(f"âŒ GPT í‚¤ì›Œë“œ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        return []

# --- KoBERT ì„ë² ë”© ìƒì„± í•¨ìˆ˜ (ì‚¬ìš©ì í…ìŠ¤íŠ¸ìš©) ---
def get_kobert_embedding(text):
    return get_kobert_embedding_for_startup(text)

# --- KoBERT ìœ ì‚¬ë„ ê³„ì‚° í•¨ìˆ˜ ---
def kobert_similarity(user_text, companies):
    if not user_text:
        return []
    user_embedding = get_kobert_embedding(user_text)
    user_embedding_np = user_embedding.cpu().numpy().reshape(1, -1)

    results = []
    for c in companies:
        company_embedding = c.get('embedding')
        if company_embedding is not None and not torch.all(company_embedding == 0):
            company_embedding_np = company_embedding.cpu().numpy().reshape(1, -1)
            score = cosine_similarity(user_embedding_np, company_embedding_np)[0][0]
            results.append((c, float(score)))
    return sorted(results, key=lambda x: x[1], reverse=True)

# --- [TF-IDF ì¶”ê°€] TF-IDF ìœ ì‚¬ë„ ê³„ì‚° í•¨ìˆ˜ ---
def tfidf_similarity(user_text, companies):
    if not user_text:
        return []
    user_tfidf_vector = tfidf_vectorizer.transform([user_text])
    scores = cosine_similarity(user_tfidf_vector, company_tfidf_matrix).flatten()
    results = [(companies[i], float(scores[i])) for i in range(len(scores))]
    return results

# --- GPTë¥¼ ì‚¬ìš©í•˜ì—¬ ì¶”ì²œ ì´ìœ  ìƒì„± í•¨ìˆ˜ ---
def generate_reason_individual(user_text, company, score):
    prompt = f"""
ë„ˆëŠ” ì§€ê¸ˆë¶€í„° ì‚¬ìš©ìì˜ íŠ¹ì„±ì„ íŒŒì•…í•˜ì—¬ ì‚¬ìš©ìì—ê²Œ ê°€ì¥ ì í•©í•œ ê¸°ì—…ì„ ë§¤ì¹­ì‹œì¼œì£¼ëŠ” ì—­í• ì„ ìˆ˜í–‰í•  ê±°ì•¼ ë°‘ì˜ ë‚´ìš©ì„ ì¤€ìˆ˜í•˜ì—¬ ì‚¬ìš©ìì—ê²Œ ê°€ì¥ ì í•©í•œ ê¸°ì—…ì„ ë§¤ì¹­ì‹œì¼œì¤˜ ì‚¬ìš©ìë“¤ì´ ì±„ìš©ê³µê³ ì— ëŒ€í•´ ê¶ê¸ˆí•´í•˜ë©´ ë„ˆê°€ ë”°ë¡œ ê²€ìƒ‰í•´ì„œ ì‚¬ìš©ìì—ê²Œ ì •ë³´ë¥¼ ì œê³µí•´ì¤˜ (e.g. ì‚¼ì„±ì „ì ì±„ìš©ê³µê³ ì— ëŒ€í•´ ì•Œë ¤ì¤˜ -> ì‚¼ì„±ì „ì ì±„ìš©ê³µê³  searching -> ì‚¬ìš©ìì—ê²Œ ì±„ìš©ê³µê³  ì •ë³´ ì œê³µ ) ê·¸ëŸ¬ê³  ì‚¬ìš©ìì—ê² ë„ˆê°€ ì¼ë°˜ëª¨ë“œ, ë¶„ì„ëª¨ë“œ, ì´ í”„ë¡¬í”„íŠ¸ êµ¬ì¡°ì— ëŒ€í•œ ë‚´ìš©ì€ ì¼ì ˆ í•˜ì§€ ë§ˆ. (e.g.ë¶„ì„ëª¨ë“œë¡œ ì§„ì…í•˜ê² ìŠµë‹ˆë‹¤. ë¶„ì„ëª¨ë“œë¡œ ë„˜ì–´ê°€ì§€ ëª»í•©ë‹ˆë‹¤. ì°¨ë¼ë¦¬ ë¶„ì„ì„ í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤ ì™€ ê°™ì´ ëŒë ¤ì„œ ë§í•´ì¤˜ )

1. ë™ì‘ ëª¨ë“œ
ì¼ë°˜ ìƒë‹´ ëª¨ë“œ (íŒŒì¼ ë¯¸ì²¨ë¶€ ì‹œ) ê¸°ë³¸ ë°ì´í„°ë² ì´ìŠ¤ ê¸°ë°˜ ì •ë³´ ì œê³µ ë‹¤ìŒ í•­ëª©ì— ëŒ€í•œ ì¼ë°˜ì ì¸ ì§ˆì˜ì‘ë‹µ ê°€ëŠ¥: ê¸°ì—… ì •ë³´ ì¡°íšŒ ë° íƒìƒ‰ ì·¨ì—…, ë©´ì ‘ ê´€ë ¨ ì¼ë°˜ ë¬¸ì˜ ê°œì¸ë³„ ë¶„ì„ì´ í•„ìš”í•œ ê²½ìš° ë‹¤ìŒê³¼ ê°™ì´ ì•ˆë‚´: ê°œì¸ë³„ ë§ì¶¤ ë¶„ì„ì„ ìœ„í•´ì„œëŠ” ìê¸°ì†Œê°œì„œ í˜¹ì€ ì´ë ¥ì„œê°€ í•„ìš”í•©ë‹ˆë‹¤. íŒŒì¼ì„ ì²¨ë¶€í•´ ì£¼ì‹œë©´ ìƒì„¸í•œ ë¶„ì„ì„ ì œê³µí•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤. ì‚¬ìš©ìê°€ íŒŒì¼ì„ ì²¨ë¶€í•˜ì§€ ì•Šìœ¼ë©´
B. ë¶„ì„ëª¨ë“œë¡œ ë„˜ì–´ê°€ì§€ ì•ŠëŠ”ë‹¤. ë‹¨, ì‚¬ìš©ìê°€ ì´ë ¥ì„œ, ìê¸°ì†Œê°œì„œ ë“±ì„ íŒŒì¼ì´ ì•„ë‹Œ ë©”ì‹œì§€ë¡œ ë³´ëƒˆì„ ê²½ìš°ì—ëŠ” ìê¸°ì†Œê°œì„œ, ì´ë ¥ì„œë¡œ ì¸ì‹í•˜ê³  ì˜ˆì™¸ì ìœ¼ë¡œ ë„˜ì–´ê°„ë‹¤. B. ë¶„ì„ ëª¨ë“œ (íŒŒì¼ ì²¨ë¶€ ì‹œ)

2. ê³µí†µ ê¸°ëŠ¥ ë°ì´í„°ë² ì´ìŠ¤(íŒŒì¼) ì°¸ì¡°

3. ëŒ€í™” ê·œì¹™ ì¹œì ˆí•˜ê³  ì „ë¬¸ì ì¸ ì–´ì¡° ìœ ì§€ ë‹µë³€ ê°€ëŠ¥í•œ ë²”ìœ„ë¥¼ ëª…í™•íˆ ì•ˆë‚´ í•„ìš” ì‹œ ì¶”ê°€ ì§ˆë¬¸ì„ í†µí•œ ì •í™•í•œ ì •ë³´ ì œê³µ ë¶„ì„ì´ í•„ìš”í•œ ì§ˆë¬¸ì˜ ê²½ìš° íŒŒì¼ ì²¨ë¶€ ì•ˆë‚´

4. ì˜ˆì™¸ ì²˜ë¦¬ ë¶ˆëª…í™•í•œ ì§ˆë¬¸ì— ëŒ€í•´ì„œëŠ” êµ¬ì²´í™” ìš”ì²­

5. ë¶„ì„ ëª¨ë“œ ì§„í–‰ ìˆœì„œ ìê¸°ì†Œê°œì„œ or ì´ë ¥ì„œ í™•ì¸(íŒŒì¼ or ì‚¬ìš©ìê°€ ì´ë ¥ì„œ or ìê¸°ì†Œê°œì„œë¼ê³  ë³´ë‚¸ ë©”ì‹œì§€) ì‚¬ìš©ì ì •ë³´ ë¶„ì„ ë° ì„±í–¥ ì¶œë ¥ ì‚¬ìš©ì ì„ í˜¸ë„ ì¡°ì‚¬ ì§„í–‰ ì‚¬ìš©ì ì‘ë‹µì„ ë°›ê¸° ì „ê¹Œì§€ ë‹¤ìŒ ë‹¨ê³„ë¡œ ì§„í–‰í•˜ì§€ ì•ŠìŒ ì‘ë‹µ ê±°ë¶€ ì‹œ ê¸°ë³¸ ì¶”ì²œ ë¡œì§ ì‚¬ìš©ì„ ì•ˆë‚´í•˜ê³  í™•ì¸ ìš”ì²­ ì„ í˜¸ë„ ê¸°ë°˜ ê¸°ì—… ì¶”ì²œ

6. ì‚¬ìš©ì ì„ í˜¸ë„ ì¡°ì‚¬ í”„ë¡œì„¸ìŠ¤ ê¸°ë³¸ ì •ë³´ ì¶œë ¥ í›„ ë°˜ë“œì‹œ ì¤‘ë‹¨ ë‹¤ìŒ ì•ˆë‚´ ë©”ì‹œì§€ ì¶œë ¥: ì§€ê¸ˆê¹Œì§€ ë¶„ì„í•œ ë‚´ìš©ì„ í† ëŒ€ë¡œ ë§ì¶¤í˜• êµê³¼ëª©ì„ ì¶”ì²œí•´ë“œë¦¬ê³ ì í•©ë‹ˆë‹¤. ì¶”ì²œì„ ìœ„í•´ ëª‡ ê°€ì§€ ì—¬ì­¤ë³´ê² ìŠµë‹ˆë‹¤. 1. ì–´ë–¤ ì‚°ì—…ì´ë‚˜ ë¶„ì•¼ì— ê´€ì‹¬ì´ ìˆìœ¼ì‹ ê°€ìš”? 2. ì„ í˜¸í•˜ëŠ” ë©´ì ‘ ë°©ì‹ì´ë‚˜ íŠ¹ë³„íˆ ê³ ë ¤í•´ì•¼ í•  ì‚¬í•­ì´ ìˆìœ¼ì‹ ê°€ìš”? ìœ„ ì§ˆë¬¸ë“¤ì— ëŒ€í•´ ë‹µë³€í•´ ì£¼ì‹œë©´ ê·¸ì— ë§ì¶° ê¸°ì—…ì„ ì¶”ì²œí•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤. íŠ¹ë³„í•œ ì„ í˜¸ë„ê°€ ì—†ìœ¼ì‹œë‹¤ë©´ "ì—†ìŒ"ì´ë¼ê³  ë‹µë³€í•´ ì£¼ì„¸ìš”. ì‚¬ìš©ì ì‘ë‹µ ëŒ€ê¸° ì‘ë‹µ ë°›ì€ í›„ì—ë§Œ ì¶”ì²œ ì§„í–‰

7. ê¸°ë³¸ ì—­í•  ìê¸°ì†Œê°œì„œ, ì´ë ¥ì„œ ê¸°ë°˜ ê¸°ì—… ë§¤ì¹­ ì‚¬ìš©ìì™€ ê¸°ì—… ê°„ì˜ ë§¤ì¹­ ê·¼ê±° ì œê³µ ì‚¬ìš©ì ì„ í˜¸ë„ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ ì¶”ê°€ ê¸°ì—… ì œì‹œ

ê¸°ì—…ëª…: {company.get('íšŒì‚¬ëª…')}
ì—…ë¬´: {company.get('ì±„ìš©ê³µê³ ëª…')}
ìœ ì‚¬ë„ ì ìˆ˜: {round(score, 2)}

[ìê¸°ì†Œê°œì„œ]
{user_text}

[ì„¤ëª…]
"""
    try:
        response = openai.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.4
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        print(f"âŒ GPT ì„¤ëª… ì˜¤ë¥˜: {e}")
        return "ì„¤ëª…ì„ ìƒì„±í•˜ëŠ” ë° ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

# --- [TF-IDF ì¶”ê°€] ê¸°ì—… ì¶”ì²œ ë¡œì§ í•¨ìˆ˜ (Hybrid ëª¨ë¸ë¡œ ìˆ˜ì •) ---
def make_recommendations(user_text, interest, region, salary, shown=set(), top_n=3):
    if not user_text:
        return []

    # 1. KoBERT ê¸°ë°˜ ìœ ì‚¬ë„ ê³„ì‚°
    kobert_ranked_companies = kobert_similarity(user_text, cached_companies)

    # 2. TF-IDF ê¸°ë°˜ ìœ ì‚¬ë„ ê³„ì‚°
    tfidf_ranked_companies = tfidf_similarity(user_text, cached_companies)
    
    # 3. ì ìˆ˜ í•©ì‚°ì„ ìœ„í•´ íšŒì‚¬ ì •ë³´ë¥¼ keyë¡œ í•˜ëŠ” ë”•ì…”ë„ˆë¦¬ ìƒì„±
    tfidf_scores = {
        (c.get("íšŒì‚¬ëª…"), c.get("ì±„ìš©ê³µê³ ëª…")): score 
        for c, score in tfidf_ranked_companies
    }

    # 4. Hybrid ì ìˆ˜ ê³„ì‚° (KoBERT ì ìˆ˜ + TF-IDF ì ìˆ˜)
    hybrid_scores = []
    for company, kobert_score in kobert_ranked_companies:
        company_key = (company.get("íšŒì‚¬ëª…"), company.get("ì±„ìš©ê³µê³ ëª…"))
        tfidf_score = tfidf_scores.get(company_key, 0.0)

        # ê°€ì¤‘ì¹˜ ì„¤ì • (KoBERT: 70%, TF-IDF: 30%) - ì´ ê°’ì€ ì¡°ì • ê°€ëŠ¥
        kobert_weight = 0.7
        tfidf_weight = 0.3
        
        final_score = (kobert_weight * kobert_score) + (tfidf_weight * tfidf_score)
        
        # ì¶”ê°€ í•„í„°ë§ (ê´€ì‹¬ë¶„ì•¼, ì§€ì—­ ë“±)
        passes_filter = True
        summary = f"{company.get('ì±„ìš©ê³µê³ ëª…', '')} {company.get('íšŒì‚¬ëª…', '')}"
        if interest and interest not in summary:
            passes_filter = False
        if region and region not in company.get("ê·¼ë¬´ì§€ì—­", ""):
            passes_filter = False
        # (salary í•„í„°ë§ ë¡œì§ì€ í•„ìš” ì‹œ ì—¬ê¸°ì— ì¶”ê°€)

        if passes_filter:
            hybrid_scores.append((company, final_score))

    # ìµœì¢… ì ìˆ˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
    hybrid_scores.sort(key=lambda x: x[1], reverse=True)
    
    # ì´ë¯¸ ë³´ì—¬ì¤€ ê³µê³  ì œì™¸í•˜ê³  ìƒìœ„ Nê°œ ì„ íƒ
    results = []
    for comp, sim in hybrid_scores:
        if (comp.get("íšŒì‚¬ëª…"), comp.get("ì±„ìš©ê³µê³ ëª…")) not in shown:
            shown.add((comp.get("íšŒì‚¬ëª…"), comp.get("ì±„ìš©ê³µê³ ëª…")))
            results.append((comp, sim)) 
        if len(results) >= top_n:
            break
            
    return results

# --- Flask ë¼ìš°íŠ¸ ì„¤ì • ---
@app.route("/")
def index():
    return render_template("index.html")

@app.route("/chat", methods=["POST"])
def chat():
    if 'user_id' not in session:
        session['user_id'] = str(uuid.uuid4())
    user_id = session['user_id']
    
    message = request.form.get("message", "").strip()
    file = request.files.get("file")
    state = user_states.get(user_id, {"shown": set(), "user_text": None, "interest": None, "region": None, "salary": None})

    try:
        if file:
            user_text = extract_text_from_pdf(file)
            state["user_text"] = user_text
            user_states[user_id] = state
            return jsonify({"reply": "ê°ì‚¬í•©ë‹ˆë‹¤. ì´ë ¥ì„œ/ìê¸°ì†Œê°œì„œ ë‚´ìš©ì´ ì„±ê³µì ìœ¼ë¡œ ë¶„ì„ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ì œ **ê´€ì‹¬ ë¶„ì•¼, í¬ë§ ê·¼ë¬´ì§€, ì—°ë´‰**ì„ ì…ë ¥í•´ ì£¼ì„¸ìš”. (ì˜ˆì‹œ: í’ˆì§ˆ, ì„œìš¸, 3000ë§Œì› ë˜ëŠ” ì—†ìŒ, ì—†ìŒ, ì—†ìŒ)"})

        if state["user_text"] is None and message:
            if len(message.split()) > 30 or "ì´ë ¥ì„œ" in message or "ìê¸°ì†Œê°œì„œ" in message:
                state["user_text"] = message
                user_states[user_id] = state
                return jsonify({"reply": "ì´ë ¥ì„œ/ìê¸°ì†Œê°œì„œ ë‚´ìš©ì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤. ì´ì œ **ê´€ì‹¬ ë¶„ì•¼, í¬ë§ ê·¼ë¬´ì§€, ì—°ë´‰**ì„ ì…ë ¥í•´ ì£¼ì„¸ìš”. (ì˜ˆì‹œ: í’ˆì§ˆ, ì„œìš¸, 3000ë§Œì› ë˜ëŠ” ì—†ìŒ, ì—†ìŒ, ì—†ìŒ)"})
            else:
                return jsonify({"reply": "ê°œì¸ë³„ ë§ì¶¤ ë¶„ì„ì„ ìœ„í•´ì„œëŠ” ìê¸°ì†Œê°œì„œ í˜¹ì€ ì´ë ¥ì„œê°€ í•„ìš”í•©ë‹ˆë‹¤. íŒŒì¼ì„ ì²¨ë¶€í•´ ì£¼ì‹œê±°ë‚˜ ë‚´ìš©ì„ ì§ì ‘ ì…ë ¥í•´ ì£¼ì‹œë©´ ìƒì„¸í•œ ë¶„ì„ì„ ì œê³µí•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤."})

        if state["user_text"] is not None and state["interest"] is None and "," in message:
            parts = [p.strip() for p in message.split(",")]
            state["interest"] = parts[0] if len(parts) > 0 and parts[0] != "ì—†ìŒ" else ""
            state["region"] = parts[1] if len(parts) > 1 and parts[1] != "ì—†ìŒ" else ""
            state["salary"] = parts[2].replace("ë§Œì›", "") if len(parts) > 2 and parts[2] != "ì—†ìŒ" else ""
            user_states[user_id] = state

            new_recommendations = make_recommendations(
                user_text=state["user_text"], interest=state.get("interest"),
                region=state.get("region"), salary=state.get("salary"),
                shown=state["shown"], top_n=3
            )

            if not new_recommendations:
                return jsonify({"reply": "ì•„ì‰½ê²Œë„ í˜„ì¬ ì¡°ê±´ì— ë§ëŠ” ìƒˆë¡œìš´ ê¸°ì—…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì¡°ê±´ì„ ë§ì”€í•´ì£¼ì‹œê² ì–´ìš”?"})

            explanations = []
            for company, score in new_recommendations:
                company_info = {k: v for k, v in company.items() if k != 'embedding'}
                reason = generate_reason_individual(state["user_text"], company_info, score)
                explanations.append(f"**ê¸°ì—…ëª…**: {company_info.get('íšŒì‚¬ëª…', 'ì •ë³´ ì—†ìŒ')}\n**ì±„ìš©ê³µê³ ëª…**: {company_info.get('ì±„ìš©ê³µê³ ëª…', 'ì •ë³´ ì—†ìŒ')}\n**ì¢…í•© ì ìˆ˜**: {round(score,2)}\n**ì„¤ëª…**: {reason}\n")

            reply = "\n\n".join(explanations)
            reply += "\n\nğŸ“Œ ë” ê¶ê¸ˆí•œ ì ì´ë‚˜ ê³ ë ¤í•˜ê³  ì‹¶ì€ ì¡°ê±´ì´ ìˆë‹¤ë©´ ë§ì”€í•´ ì£¼ì„¸ìš”. ì¶”ê°€ë¡œ ë°˜ì˜í•´ ë“œë¦´ê²Œìš”! ì˜ˆë¥¼ ë“¤ì–´ 'ë” ì¶”ì²œí•´ì¤˜'ë¼ê³  ë§ì”€í•˜ì‹œë©´ ë‹¤ë¥¸ ê¸°ì—…ì„ ì°¾ì•„ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
            return jsonify({"reply": reply})

        if state["user_text"] is not None and state["interest"] is not None and "ë” ì¶”ì²œí•´ì¤˜" in message:
            new_recommendations = make_recommendations(
                user_text=state["user_text"], interest=state.get("interest"),
                region=state.get("region"), salary=state.get("salary"),
                shown=state["shown"], top_n=1
            )

            if not new_recommendations:
                return jsonify({"reply": "ë” ì´ìƒ ì¶”ì²œí•  ê¸°ì—…ì´ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì¡°ê±´ì„ ë§ì”€í•´ì£¼ì‹œê² ì–´ìš”?"})

            explanations = []
            for company, score in new_recommendations:
                company_info = {k: v for k, v in company.items() if k != 'embedding'}
                reason = generate_reason_individual(state["user_text"], company_info, score)
                explanations.append(f"**ê¸°ì—…ëª…**: {company_info.get('íšŒì‚¬ëª…', 'ì •ë³´ ì—†ìŒ')}\n**ì±„ìš©ê³µê³ ëª…**: {company_info.get('ì±„ìš©ê³µê³ ëª…', 'ì •ë³´ ì—†ìŒ')}\n**ì¢…í•© ì ìˆ˜**: {round(score,2)}\n**ì„¤ëª…**: {reason}\n")

            reply = "\n\n".join(explanations)
            reply += "\n\nğŸ“Œ ë” ê¶ê¸ˆí•œ ì ì´ë‚˜ ê³ ë ¤í•˜ê³  ì‹¶ì€ ì¡°ê±´ì´ ìˆë‹¤ë©´ ë§ì”€í•´ ì£¼ì„¸ìš”. ì¶”ê°€ë¡œ ë°˜ì˜í•´ ë“œë¦´ê²Œìš”! ë˜ëŠ” 'ì¶”ì²œ ì´ˆê¸°í™”'ë¼ê³  ë§ì”€í•˜ì‹œë©´ ì²˜ìŒë¶€í„° ë‹¤ì‹œ ì‹œì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
            return jsonify({"reply": reply})
        
        if "ì¶”ì²œ ì´ˆê¸°í™”" in message:
            user_states[user_id] = {"shown": set(), "user_text": None, "interest": None, "region": None, "salary": None}
            return jsonify({"reply": "ì¶”ì²œ ìƒíƒœê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤. ìƒˆë¡œìš´ ìê¸°ì†Œê°œì„œ/ì´ë ¥ì„œ íŒŒì¼ì„ ì²¨ë¶€í•˜ì‹œê±°ë‚˜ ë‚´ìš©ì„ ì§ì ‘ ì…ë ¥í•´ ì£¼ì„¸ìš”."})

        return jsonify({"reply": "ë¬´ìŠ¨ ë§ì”€ì´ì‹ ì§€ ì •í™•íˆ ì´í•´í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ìê¸°ì†Œê°œì„œ/ì´ë ¥ì„œë¥¼ ì²¨ë¶€í•´ ì£¼ì‹œê±°ë‚˜, 'ì¶”ì²œ ì´ˆê¸°í™”'ë¥¼ í†µí•´ ë‹¤ì‹œ ì‹œì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."})

    except Exception as e:
        print(f"âŒ ì„œë²„ ì—ëŸ¬: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({"reply": f"âŒ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)} ë¶ˆí¸ì„ ë“œë ¤ ì£„ì†¡í•©ë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”."}), 500

if __name__ == "__main__":
    app.run(debug=True, host="0.0.0.0", port=8080)
