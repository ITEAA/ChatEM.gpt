import os
import json
import fitz  # PyMuPDF
import openai
import xml.etree.ElementTree as ET
import requests
import pandas as pd

from flask import Flask, request, jsonify, render_template
from flask_cors import CORS
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

app = Flask(__name__)
CORS(app)

openai.api_key = os.getenv("OPENAI_API_KEY") or "your-api-key"
GG_API_KEY = "8af0f404ca144249be0cfab9728b619b"

user_states = {}

# Load company data from both static file and cached API result
with open("ChatEM_top20_companies.json", "r", encoding="utf-8") as f:
    static_company_data = json.load(f)

with open("gg_employment_cached.json", "r", encoding="utf-8") as f:
    gg_company_data = json.load(f)

def extract_text_from_pdf(pdf_file):
    doc = fitz.open(stream=pdf_file.read(), filetype="pdf")
    text = "\n".join(page.get_text() for page in doc)
    return text.strip()

def extract_keywords(text):
    prompt = f"""
    ë‹¤ìŒ ìê¸°ì†Œê°œì„œ ë˜ëŠ” ì´ë ¥ì„œì—ì„œ í•µì‹¬ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•´ì¤˜.
    - 5~10ê°œ ì •ë„ ë½‘ì•„ì¤˜.
    - í‚¤ì›Œë“œëŠ” ì½¤ë§ˆ(,)ë¡œ êµ¬ë¶„í•´ì„œ ì¶œë ¥í•´ì¤˜.

    ë‚´ìš©:
    {text}
    """
    try:
        response = openai.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.4
        )
        result = response.choices[0].message.content
        return [kw.strip() for kw in result.split(",") if kw.strip()]
    except Exception as e:
        print(f"âŒ GPT í˜¸ì¶œ ì—ëŸ¬: {e}")
        return []

def filter_companies(companies, keywords, interest, region, salary):
    def score(company):
        base_score = 0
        summary = company.get("summary") or ""
        if any(kw in summary for kw in keywords):
            base_score += 1
        if interest and interest in summary:
            base_score += 0.3
        if region and region in company.get("ê·¼ë¬´ì§€ì—­", ""):
            base_score += 0.3
        if salary and str(salary) in company.get("ê¸‰ì—¬", ""):
            base_score += 0.2
        return base_score

    return sorted(companies, key=score, reverse=True)

def tfidf_similarity(user_text, companies):
    documents = [user_text] + [(c.get("summary") or c.get("ì±„ìš©ê³µê³ ëª…") or "") for c in companies]
    tfidf = TfidfVectorizer().fit_transform(documents)
    cosine_sim = cosine_similarity(tfidf[0:1], tfidf[1:]).flatten()
    return sorted(zip(companies, cosine_sim), key=lambda x: x[1], reverse=True)

def generate_reason(user_text, companies_with_scores, interest, region, salary):
    explanations = []
    for company, score in companies_with_scores:
        prompt = f"""
ë‹¹ì‹ ì€ ì±„ìš© ì»¨ì„¤í„´íŠ¸ì…ë‹ˆë‹¤.
ì•„ë˜ ìê¸°ì†Œê°œì„œì™€ ì‚¬ìš©ì ì¡°ê±´ì„ ì°¸ê³ í•˜ì—¬ ë‹¤ìŒ ê¸°ì—…ì— ì™œ ì í•©í•œì§€ ë¶„ì„ê°€ ì‹œì ìœ¼ë¡œ ì„¤ëª…í•´ ì£¼ì„¸ìš”.

ê¸°ì—…ëª…: {company.get("íšŒì‚¬ëª…") or company.get("name")}
ì—…ë¬´: {company.get("ì±„ìš©ê³µê³ ëª…") or company.get("summary")}
ìœ ì‚¬ë„ ì ìˆ˜: {round(score, 3)}

[ìê¸°ì†Œê°œì„œ]
{user_text}

[ì‚¬ìš©ì ì…ë ¥ ì¡°ê±´]
ê´€ì‹¬ ë¶„ì•¼: {interest or "(ì…ë ¥ ì•ˆë¨)"}
í¬ë§ ê·¼ë¬´ì§€: {region or "(ì…ë ¥ ì•ˆë¨)"}
í¬ë§ ì—°ë´‰: {salary or "(ì…ë ¥ ì•ˆë¨)"}
"""
        try:
            response = openai.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[{"role": "user", "content": prompt}],
                temperature=0.3
            )
            explanations.append(response.choices[0].message.content.strip())
        except Exception as e:
            explanations.append(f"âŒ ì„¤ëª… ìƒì„± ì˜¤ë¥˜: {e}")
    return "\n\n".join(explanations) + "\n\nğŸ“Œ ë” ê¶ê¸ˆí•œ ì ì´ë‚˜ ê³ ë ¤í•˜ê³  ì‹¶ì€ ì¡°ê±´ì´ ìˆë‹¤ë©´ ë§ì”€í•´ ì£¼ì„¸ìš”. ì¶”ê°€ë¡œ ë°˜ì˜í•´ ë“œë¦´ê²Œìš”!"

@app.route("/")
def index():
    return render_template("index.html")

@app.route("/chat", methods=["POST"])
def chat():
    user_id = request.remote_addr
    message = request.form.get("message", "").strip()
    file = request.files.get("file")
    state = user_states.get(user_id, {})

    try:
        if file:
            state["user_text"] = extract_text_from_pdf(file)

        if message and "," in message and "ë§Œì›" in message:
            parts = [p.strip().replace("ë§Œì›", "") for p in message.split(",")]
            state["interest"] = parts[0] if len(parts) > 0 else ""
            state["region"] = parts[1] if len(parts) > 1 else ""
            state["salary"] = parts[2] if len(parts) > 2 else ""

        if message and "user_text" not in state:
            state["user_text"] = message

        if "user_text" in state:
            keywords = extract_keywords(state["user_text"])
            combined_data = static_company_data + gg_company_data
            filtered = filter_companies(combined_data, keywords, state.get("interest"), state.get("region"), state.get("salary"))
            matched = tfidf_similarity(state["user_text"], filtered)
            selected = matched[:3] if "ë” ì¶”ì²œí•´ì¤˜" not in message else [matched[3]]
            explanation = generate_reason(state["user_text"], selected, state.get("interest"), state.get("region"), state.get("salary"))
            return jsonify({"reply": explanation})

        missing = []
        if "user_text" not in state:
            missing.append("ìê¸°ì†Œê°œì„œ ë˜ëŠ” ì´ë ¥ì„œ")
        if missing:
            return jsonify({"reply": f"ë¨¼ì € {', '.join(missing)}ë¥¼ ì…ë ¥í•´ ì£¼ì„¸ìš”."})

        return jsonify({"reply": "ì…ë ¥ì„ ì¸ì‹í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”."})

    except Exception as e:
        print(f"âŒ ì„œë²„ ì—ëŸ¬: {e}")
        return jsonify({"reply": f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"}), 500

if __name__ == "__main__":
    app.run(debug=True, host="0.0.0.0", port=8080)
