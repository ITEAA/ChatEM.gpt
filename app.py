import os
import json
import fitz # PyMuPDF
import openai
import uuid
import re # ì •ê·œí‘œí˜„ì‹ ëª¨ë“ˆ ì¶”ê°€
import traceback # ì˜¤ë¥˜ ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤ ì¶œë ¥ì„ ìœ„í•œ ëª¨ë“ˆ ì¶”ê°€

# --- Word2Vec/Doc2Vec ë° í•œêµ­ì–´ ì²˜ë¦¬ ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì¶”ê°€ ---
from gensim.models.doc2vec import Doc2Vec, TaggedDocument
from konlpy.tag import Okt # í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„ê¸° (Okt ì‚¬ìš©)
import numpy as np # ë²¡í„° ì—°ì‚°ì„ ìœ„í•´ numpy ì¶”ê°€

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

from flask import Flask, request, jsonify, render_template, session
from flask_cors import CORS

app = Flask(__name__)
CORS(app)
app.secret_key = os.getenv("FLASK_SECRET_KEY", "your-super-secret-random-key-here-for-production")

# --- API í‚¤ ë¡œë”© ---
api_key = os.getenv("OPENAI_API_KEY")
if not api_key:
    raise ValueError("ì˜¤ë¥˜: OPENAI_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í”„ë¡œê·¸ë¨ì„ ì‹œì‘í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
openai.api_key = api_key

GG_CACHED_FILE = "gg_employment_cached.json"
user_states = {} # ì‚¬ìš©ìë³„ ëŒ€í™” ìƒíƒœë¥¼ ì €ì¥í•  ë”•ì…”ë„ˆë¦¬ (ì¸ë©”ëª¨ë¦¬)

# Doc2Vec ëª¨ë¸ ì´ˆê¸°í™”
doc2vec_model = None
okt_tokenizer = None # KoNLPy Okt í† í¬ë‚˜ì´ì € ì´ˆê¸°í™”

# ê¸°ì—… ì •ë³´ ë° ë²¡í„° ì´ˆê¸°í™”
cached_companies = []
tfidf_vectorizer = None
company_tfidf_matrix = None

# --- í•œêµ­ì–´ í…ìŠ¤íŠ¸ë¥¼ í˜•íƒœì†Œë¡œ ë¶„ë¦¬í•˜ëŠ” í•¨ìˆ˜ ---
def tokenize_korean_text(text):
    if okt_tokenizer is None:
        # ëª¨ë¸ ë¡œë”© ì „ì— í˜¸ì¶œë  ê²½ìš°ë¥¼ ëŒ€ë¹„
        return []
    # ëª…ì‚¬, ë™ì‚¬, í˜•ìš©ì‚¬ ë“± ì˜ë¯¸ ìˆëŠ” í’ˆì‚¬ë§Œ ì¶”ì¶œ (í•„ìš”ì— ë”°ë¼ í’ˆì‚¬ ëª©ë¡ ì¡°ì •)
    return [word for word, pos in okt_tokenizer.pos(text) if pos in ['Noun', 'Verb', 'Adjective', 'Adverb']]

# --- Doc2Vec ì„ë² ë”© ìƒì„± í•¨ìˆ˜ ---
def get_doc2vec_embedding(text_input):
    if doc2vec_model is None or not text_input:
        # ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ê±°ë‚˜ ì…ë ¥ í…ìŠ¤íŠ¸ê°€ ì—†ì„ ê²½ìš° 0 ë²¡í„° ë°˜í™˜
        return np.zeros(300) # Doc2Vec vector_sizeì— ë§ì¶° 0 ë²¡í„° ë°˜í™˜ (ê¸°ë³¸ 300)
    try:
        tokens = tokenize_korean_text(text_input)
        if not tokens: # í† í°ì´ ì—†ì„ ê²½ìš° ë¹ˆ ë²¡í„° ë°˜í™˜ ë°©ì§€
            return np.zeros(300)
        # ìƒˆë¡œìš´ ë¬¸ì„œì— ëŒ€í•œ ë²¡í„° ì¶”ë¡ 
        return doc2vec_model.infer_vector(tokens)
    except Exception as e:
        print(f"âŒ Doc2Vec ì„ë² ë”© ìƒì„± ì˜¤ë¥˜ (í…ìŠ¤íŠ¸: '{text_input[:30]}...'): {e}")
        traceback.print_exc()
        return np.zeros(300)

try:
    if not os.path.exists(GG_CACHED_FILE):
        print(f"ê²½ê³ : '{GG_CACHED_FILE}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë¹ˆ íšŒì‚¬ ëª©ë¡ìœ¼ë¡œ ì‹œì‘í•©ë‹ˆë‹¤.")
    else:
        with open(GG_CACHED_FILE, "r", encoding="utf-8") as f:
            cached_companies = json.load(f)
        print(f"âœ… '{GG_CACHED_FILE}'ì—ì„œ {len(cached_companies)}ê°œ ê¸°ì—… ì •ë³´ ë¡œë“œ ì„±ê³µ.")

    # KoNLPy Okt í† í¬ë‚˜ì´ì € ë¡œë“œ
    okt_tokenizer = Okt()
    print("âœ… KoNLPy Okt í† í¬ë‚˜ì´ì € ë¡œë“œ ì„±ê³µ!")

    # --- ì„œë²„ ì‹œì‘ ì‹œ Doc2Vec ëª¨ë¸ í•™ìŠµ ë° ê¸°ì—… ì •ë³´ ì„ë² ë”© ë¯¸ë¦¬ ê³„ì‚° ---
    print("ì„œë²„ ì‹œì‘ ì „, Doc2Vec ëª¨ë¸ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤... (ë°ì´í„° ì–‘ì— ë”°ë¼ ì‹œê°„ ì†Œìš”)")
    documents_for_training = []
    for i, company in enumerate(cached_companies):
        # 'ì±„ìš©ê³µê³ ëª…', 'íšŒì‚¬ëª…', 'summary'ë¥¼ ì¡°í•©í•˜ì—¬ í•™ìŠµ ë°ì´í„° ìƒì„±
        text = f"{company.get('ì±„ìš©ê³µê³ ëª…', '')} {company.get('íšŒì‚¬ëª…', '')} {company.get('summary', '')}"
        tokens = tokenize_korean_text(text)
        if tokens: # ë¹ˆ í…ìŠ¤íŠ¸ëŠ” í•™ìŠµì—ì„œ ì œì™¸
            documents_for_training.append(TaggedDocument(tokens, [f'company_{i}']))

    # Doc2Vec ëª¨ë¸ í•™ìŠµ (vector_size, window, min_count, epochs ë“± íŒŒë¼ë¯¸í„° ì¡°ì • ê°€ëŠ¥)
    # vector_size: ì„ë² ë”© ë²¡í„°ì˜ ì°¨ì› (ì¼ë°˜ì ìœ¼ë¡œ 100~300)
    # window: ì£¼ë³€ ë‹¨ì–´ ê³ ë ¤ ë²”ìœ„
    # min_count: ìµœì†Œ ë“±ì¥ íšŸìˆ˜ ì´í•˜ì˜ ë‹¨ì–´ ë¬´ì‹œ
    # workers: í•™ìŠµì— ì‚¬ìš©í•  ìŠ¤ë ˆë“œ ìˆ˜
    # epochs: í•™ìŠµ ë°˜ë³µ íšŸìˆ˜
    if documents_for_training:
        doc2vec_model = Doc2Vec(
            documents_for_training,
            vector_size=300, # KoBERTì˜ 768ì°¨ì›ê³¼ëŠ” ë‹¤ë¦„. ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì— ì˜í–¥.
            window=5,
            min_count=5, # ë„ˆë¬´ ì ì€ ë‹¨ì–´ëŠ” ë¬´ì‹œ
            workers=4,
            epochs=20
        )
        print("âœ… Doc2Vec ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!")

        # í•™ìŠµëœ ëª¨ë¸ë¡œ ëª¨ë“  ê¸°ì—… ì •ë³´ì— ëŒ€í•œ ì„ë² ë”© ë¯¸ë¦¬ ê³„ì‚°
        for i, company in enumerate(cached_companies):
            # í•™ìŠµì— ì‚¬ìš©ëœ ë¬¸ì„œì˜ íƒœê·¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ë²¡í„° ê°€ì ¸ì˜¤ê¸°
            company['embedding'] = doc2vec_model.dv[f'company_{i}']
        print("âœ… ëª¨ë“  ê¸°ì—… ì •ë³´ì˜ Doc2Vec ì„ë² ë”©ì´ ì™„ë£Œë˜ì–´ ë©”ëª¨ë¦¬ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    else:
        print("âš ï¸ í•™ìŠµí•  ê¸°ì—… ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. Doc2Vec ëª¨ë¸ì´ í•™ìŠµë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    # --- TF-IDF Vectorizer ë° ê¸°ì—…ë³„ TF-IDF í–‰ë ¬ ë¯¸ë¦¬ ê³„ì‚° ---
    print("ì„œë²„ ì‹œì‘ ì „, ê¸°ì—… ì •ë³´ TF-IDF ë²¡í„°í™”ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
    # TF-IDFë„ í˜•íƒœì†Œ ë¶„ì„ëœ í…ìŠ¤íŠ¸ë¥¼ ì‚¬ìš©í•˜ë„ë¡ ë³€ê²½
    company_summaries_for_tfidf = []
    for company in cached_companies:
        text = f"{company.get('ì±„ìš©ê³µê³ ëª…', '')} {company.get('íšŒì‚¬ëª…', '')} {company.get('summary', '')}"
        company_summaries_for_tfidf.append(" ".join(tokenize_korean_text(text))) # í† í°í™”ëœ ë‹¨ì–´ë“¤ì„ ê³µë°±ìœ¼ë¡œ ì¡°ì¸

    tfidf_vectorizer = TfidfVectorizer(min_df=1, ngram_range=(1, 2))
    if company_summaries_for_tfidf:
        company_tfidf_matrix = tfidf_vectorizer.fit_transform(company_summaries_for_tfidf)
        print("âœ… ëª¨ë“  ê¸°ì—… ì •ë³´ì˜ TF-IDF ë²¡í„°í™”ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
    else:
        print("âš ï¸ TF-IDF ë²¡í„°í™”ë¥¼ ìœ„í•œ ê¸°ì—… ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")

except json.JSONDecodeError as e:
    print(f"âŒ ì˜¤ë¥˜: '{GG_CACHED_FILE}' íŒŒì¼ì´ ìœ íš¨í•œ JSON í˜•ì‹ì´ ì•„ë‹™ë‹ˆë‹¤. ì˜¤ë¥˜: {e}")
    cached_companies = []
    raise RuntimeError(f"ê¸°ì—… ì •ë³´ íŒŒì¼ ë¡œë”©ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ì˜¤ë¥˜: {e}")
except Exception as e:
    print(f"âŒ ì´ˆê¸° ì„¤ì • ì¤‘ ì¹˜ëª…ì ì¸ ì˜¤ë¥˜ ë°œìƒ: {e}")
    traceback.print_exc()
    raise RuntimeError(f"ì• í”Œë¦¬ì¼€ì´ì…˜ ì´ˆê¸° ì„¤ì •ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ì˜¤ë¥˜: {e}")


# --- PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ í•¨ìˆ˜ (ë³€ê²½ ì—†ìŒ) ---
def extract_text_from_pdf(pdf_file_stream):
    try:
        doc = fitz.open(stream=pdf_file_stream.read(), filetype="pdf")
        text_content = []
        for page in doc:
            text_content.append(page.get_text())
        raw_text = "\n".join(text_content)

        processed_text = re.sub(r'\s+', ' ', raw_text)
        processed_text = processed_text.strip()

        print("\n--- PDFì—ì„œ ì¶”ì¶œëœ ì›ë³¸ í…ìŠ¤íŠ¸ (ì•ë¶€ë¶„ 500ì) ---")
        print(raw_text[:500])
        print("-------------------------------------------\n")
        print("--- PDFì—ì„œ ì¶”ì¶œ ë° ì „ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸ (ì•ë¶€ë¶„ 500ì) ---")
        print(processed_text[:500])
        print("---------------------------------------------------\n")

        return processed_text
    except Exception as e:
        print(f"âŒ PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        traceback.print_exc()
        return ""


# --- GPTë¥¼ ì‚¬ìš©í•˜ì—¬ í‚¤ì›Œë“œ ì¶”ì¶œ í•¨ìˆ˜ (ë³€ê²½ ì—†ìŒ) ---
def extract_keywords(text):
    if not text:
        return []

    prompt = f"""
    ë‹¤ìŒ ìê¸°ì†Œê°œì„œ ë˜ëŠ” ì´ë ¥ì„œì—ì„œ í•µì‹¬ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•´ì¤˜.
    - 5~10ê°œ ì •ë„ ë½‘ì•„ì¤˜.
    - í‚¤ì›Œë“œëŠ” ì½¤ë§ˆ(,)ë¡œ êµ¬ë¶„í•´ì„œ ì¶œë ¥í•´ì¤˜.

    ë‚´ìš©:
    {text}
    """
    try:
        response = openai.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.4
        )
        result = response.choices[0].message.content
        return [kw.strip() for kw in result.split(",") if kw.strip()]
    except Exception as e:
        print(f"âŒ GPT í‚¤ì›Œë“œ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        traceback.print_exc()
        return []

# --- Doc2Vec ìœ ì‚¬ë„ ê³„ì‚° í•¨ìˆ˜ (KoBERT ëŒ€ì²´) ---
def doc2vec_similarity(user_text, companies):
    if not user_text or not companies or doc2vec_model is None:
        return []
    
    # ì‚¬ìš©ì í…ìŠ¤íŠ¸ í˜•íƒœì†Œ ë¶„ì„ í›„ ì„ë² ë”© ìƒì„±
    user_embedding = get_doc2vec_embedding(user_text)
    user_embedding_np = user_embedding.reshape(1, -1)

    results = []
    for c in companies:
        company_embedding = c.get('embedding') # ë¯¸ë¦¬ ê³„ì‚°ëœ Doc2Vec ì„ë² ë”© ì‚¬ìš©
        if company_embedding is not None and not np.all(company_embedding == 0):
            company_embedding_np = company_embedding.reshape(1, -1)
            score = cosine_similarity(user_embedding_np, company_embedding_np)[0][0]
            results.append((c, float(score)))
    return sorted(results, key=lambda x: x[1], reverse=True)

# --- TF-IDF ìœ ì‚¬ë„ ê³„ì‚° í•¨ìˆ˜ (TF-IDF ë²¡í„°í™”ì— í˜•íƒœì†Œ ë¶„ì„ëœ í…ìŠ¤íŠ¸ ì‚¬ìš©) ---
def tfidf_similarity(user_text, companies):
    if not user_text or not companies or tfidf_vectorizer is None or company_tfidf_matrix is None:
        return []
    try:
        # ì‚¬ìš©ì í…ìŠ¤íŠ¸ë„ í˜•íƒœì†Œ ë¶„ì„ í›„ TF-IDF ë²¡í„°í™”
        user_tokens = tokenize_korean_text(user_text)
        user_tfidf_vector = tfidf_vectorizer.transform([" ".join(user_tokens)])
        
        scores = cosine_similarity(user_tfidf_vector, company_tfidf_matrix).flatten()
        results = [(companies[i], float(scores[i])) for i in range(len(scores))]
        return results
    except Exception as e:
        print(f"âŒ TF-IDF ìœ ì‚¬ë„ ê³„ì‚° ì˜¤ë¥˜: {e}")
        traceback.print_exc()
        return []

# --- GPTë¥¼ ì‚¬ìš©í•˜ì—¬ ì¶”ì²œ ì´ìœ  ìƒì„± í•¨ìˆ˜ (ë³€ê²½ ì—†ìŒ) ---
def generate_reason_individual(user_text, company, score):
    prompt = f"""
    ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ íŠ¹ì„±ê³¼ ì„ í˜¸ë„ë¥¼ íŒŒì•…í•´ ê°€ì¥ ì í•©í•œ ê¸°ì—…ì„ ë§¤ì¹­ì‹œì¼œì£¼ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
    ì‚¬ìš©ìì˜ ìê¸°ì†Œê°œì„œ ë‚´ìš©ê³¼ ê¸°ì—… ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ, ì™œ ì´ ê¸°ì—…ì´ ì‚¬ìš©ìì—ê²Œ ì í•©í•œì§€ ìƒì„¸í•˜ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”.
    ì¹œì ˆí•˜ê³  ì „ë¬¸ì ì¸ ì–´ì¡°ë¡œ, ì„¤ëª…ì€ ìì—°ìŠ¤ëŸ½ê³  ëª…í™•í•˜ê²Œ ì „ë‹¬í•´ì•¼ í•©ë‹ˆë‹¤.
    ì‹œìŠ¤í…œ ì‘ë™ ê´€ë ¨ ë¬¸êµ¬("ë¶„ì„ëª¨ë“œì…ë‹ˆë‹¤" ë“±)ëŠ” ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.
    ë¶„ì„ì´ ì–´ë ¤ìš´ ê²½ìš° "í˜„ì¬ ì •ë³´ë§Œìœ¼ë¡œëŠ” ë¶„ì„ì´ ì–´ë µìŠµë‹ˆë‹¤"ì™€ ê°™ì´ ìì—°ìŠ¤ëŸ½ê²Œ ì•ˆë‚´í•´ì£¼ì„¸ìš”.

    [ê¸°ì—… ì •ë³´]
    - ê¸°ì—…ëª…: {company.get('name', 'ì •ë³´ ì—†ìŒ')}
    - ì±„ìš©ê³µê³ ëª…: {company.get('summary', 'ì •ë³´ ì—†ìŒ')}
    - ìœ ì‚¬ë„ ì ìˆ˜: {round(score, 2)}

    [ì‚¬ìš©ì ìê¸°ì†Œê°œì„œ]
    {user_text}

    [ì„¤ëª… ì‹œì‘]
    """
    try:
        response = openai.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.4
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        print(f"âŒ GPT ì„¤ëª… ìƒì„± ì˜¤ë¥˜: {e}")
        traceback.print_exc()
        return "ì„¤ëª…ì„ ìƒì„±í•˜ëŠ” ë° ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

# --- ì—°ë´‰ ì •ë³´ íŒŒì‹± í—¬í¼ í•¨ìˆ˜ (ë³€ê²½ ì—†ìŒ) ---
def parse_salary_info(summary_text):
    min_salary = 0
    max_salary = float('inf')

    match_annual = re.search(r'ì—°ë´‰ (\d+)(?:ë§Œì›)?(?: ~ (\d+)(?:ë§Œì›)?)?', summary_text)
    if match_annual:
        min_salary = int(match_annual.group(1))
        max_salary = int(match_annual.group(2)) if match_annual.group(2) else min_salary
        return min_salary, max_salary

    match_monthly = re.search(r'ì›”ê¸‰ (\d+)(?:ë§Œì›)?(?: ~ (\d+)(?:ë§Œì›)?)?', summary_text)
    if match_monthly:
        min_monthly = int(match_monthly.group(1))
        min_salary = min_monthly * 12
        max_monthly = int(match_monthly.group(2)) if match_monthly.group(2) else min_monthly
        max_salary = max_monthly * 12
        return min_salary, max_salary

    match_hourly = re.search(r'ì‹œê¸‰ (\d+)', summary_text)
    if match_hourly:
        hourly_wage = int(match_hourly.group(1))
        min_salary = (hourly_wage * 209 * 12) / 10000
        max_salary = min_salary
        return int(min_salary), int(max_salary)
        
    return 0, float('inf')

# --- ê¸°ì—… í•„í„°ë§ ë¡œì§ í•¨ìˆ˜ (ë³€ê²½ ì—†ìŒ) ---
def apply_company_filters(company, interest, region, salary):
    passes_filter = True
        
    if interest:
        summary_lower = company.get('summary', '').lower()
        industry_lower = company.get('industry', '').lower()
        interest_lower = interest.lower()
        if interest_lower not in summary_lower and interest_lower not in industry_lower:
            passes_filter = False
            
    if region and region.lower() not in company.get("region", "").lower():
        passes_filter = False
            
    if salary:
        try:
            min_salary_req = int(salary)
            company_min_salary, company_max_salary = parse_salary_info(company.get("summary", ""))

            if min_salary_req > company_max_salary:
                passes_filter = False
                
        except ValueError:
            print(f"ê²½ê³ : ìœ íš¨í•˜ì§€ ì•Šì€ ì—°ë´‰ ì…ë ¥ '{salary}' ë˜ëŠ” ê¸°ì—… ì—°ë´‰ ì •ë³´ íŒŒì‹± ì˜¤ë¥˜")
            pass

    return passes_filter

# --- ê¸°ì—… ì¶”ì²œ ë¡œì§ í•¨ìˆ˜ (Hybrid ëª¨ë¸ - Doc2Vec + TF-IDF) ---
def make_recommendations(user_text, interest, region, salary, shown_companies_set=None, top_n=3):
    if shown_companies_set is None:
        shown_companies_set = set()

    if not user_text or not cached_companies:
        return []

    # 1. Doc2Vec ê¸°ë°˜ ìœ ì‚¬ë„ ê³„ì‚° (ëª¨ë“  ìºì‹±ëœ ê¸°ì—… ëŒ€ìƒ)
    doc2vec_ranked_companies = doc2vec_similarity(user_text, cached_companies)

    # 2. TF-IDF ê¸°ë°˜ ìœ ì‚¬ë„ ê³„ì‚° (ëª¨ë“  ìºì‹±ëœ ê¸°ì—… ëŒ€ìƒ)
    tfidf_ranked_companies = tfidf_similarity(user_text, cached_companies)
        
    # 3. ì ìˆ˜ í•©ì‚°ì„ ìœ„í•´ íšŒì‚¬ ì •ë³´ë¥¼ keyë¡œ í•˜ëŠ” ë”•ì…”ë„ˆë¦¬ ìƒì„±
    # (company_keyëŠ” nameê³¼ summaryë¥¼ ì¡°í•©í•˜ì—¬ ê³ ìœ ì„±ì„ í™•ë³´. ì‹¤ì œ ë°ì´í„°ì— ë”°ë¼ ID ë“± ë‹¤ë¥¸ ê³ ìœ  ì‹ë³„ì ì‚¬ìš© ê¶Œì¥)
    tfidf_scores = {
        (c.get("name"), c.get("summary")): score
        for c, score in tfidf_ranked_companies
    }

    # 4. Hybrid ì ìˆ˜ ê³„ì‚° (Doc2Vec ì ìˆ˜ + TF-IDF ì ìˆ˜) ë° í•„í„°ë§ ì ìš©
    hybrid_scores = []
    for company, doc2vec_score in doc2vec_ranked_companies:
        company_key = (company.get("name"), company.get("summary"))
        tfidf_score = tfidf_scores.get(company_key, 0.0)

        # ê°€ì¤‘ì¹˜ ì„¤ì • (Doc2Vec: 70%, TF-IDF: 30%) - ì´ ê°’ì€ ì¡°ì • ê°€ëŠ¥
        doc2vec_weight = 0.7
        tfidf_weight = 0.3
            
        final_score = (doc2vec_weight * doc2vec_score) + (tfidf_weight * tfidf_score)
            
        # ì¶”ê°€ í•„í„°ë§ (ê´€ì‹¬ë¶„ì•¼, ì§€ì—­, ì—°ë´‰)
        if apply_company_filters(company, interest, region, salary):
            hybrid_scores.append((company, final_score))

    # ìµœì¢… ì ìˆ˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
    hybrid_scores.sort(key=lambda x: x[1], reverse=True)
        
    # ì´ë¯¸ ë³´ì—¬ì¤€ ê³µê³  ì œì™¸í•˜ê³  ìƒìœ„ Nê°œ ì„ íƒ
    results = []
    for comp, sim in hybrid_scores:
        comp_id_str = json.dumps((comp.get("name"), comp.get("summary")), ensure_ascii=False)
        if comp_id_str not in shown_companies_set:
            shown_companies_set.add(comp_id_str)
            results.append((comp, sim))
        if len(results) >= top_n:
            break
            
    return results


# --- Flask ë¼ìš°íŠ¸ ì„¤ì • ---
@app.route("/")
def index():
    return render_template("index.html")

# --- ì‚¬ìš©ì ìƒíƒœ ê´€ë¦¬ í—¬í¼ í•¨ìˆ˜ ---
def _get_user_state(user_id):
    return user_states.get(user_id, {
        "shown": set(), # ì´ë¯¸ ì¶”ì²œëœ ê¸°ì—… ëª©ë¡ (ì¤‘ë³µ ì¶”ì²œ ë°©ì§€)
        "user_text": None, # ì‚¬ìš©ìì˜ ìê¸°ì†Œê°œì„œ/ì´ë ¥ì„œ í…ìŠ¤íŠ¸
        "interest": None, # ì‚¬ìš©ì ê´€ì‹¬ ë¶„ì•¼
        "region": None, # ì‚¬ìš©ì í¬ë§ ê·¼ë¬´ì§€
        "salary": None # ì‚¬ìš©ì í¬ë§ ì—°ë´‰
    })

def _update_user_state(user_id, state):
    user_states[user_id] = state

# --- ì¶”ì²œ ê²°ê³¼ ì‘ë‹µ ìƒì„± í—¬í¼ í•¨ìˆ˜ (ì¤‘ë³µ ë¡œì§ ì œê±°) ---
def _generate_recommendation_response(user_text, recommendations, additional_message=""):
    if not recommendations:
        return {"reply": "ì•„ì‰½ê²Œë„ í˜„ì¬ ì¡°ê±´ì— ë§ëŠ” ê¸°ì—…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì¡°ê±´ì„ ë§ì”€í•´ì£¼ì‹œê±°ë‚˜ 'ì¶”ì²œ ì´ˆê¸°í™”'ë¥¼ í†µí•´ ë‹¤ì‹œ ì‹œì‘í•´ ì£¼ì‹œê² ì–´ìš”?"}

    explanations = []
    for company, score in recommendations:
        # ì„ë² ë”© ì •ë³´ëŠ” í´ë¼ì´ì–¸íŠ¸ì—ê²Œ ë³´ë‚¼ í•„ìš” ì—†ìœ¼ë¯€ë¡œ ì œê±°
        company_info_for_gpt = {k: v for k, v in company.items() if k not in ['embedding']}
        reason = generate_reason_individual(user_text, company_info_for_gpt, score)
        explanations.append(f"**ê¸°ì—…ëª…**: {company_info_for_gpt.get('name', 'ì •ë³´ ì—†ìŒ')}\n**ì±„ìš©ê³µê³ ëª…**: {company_info_for_gpt.get('summary', 'ì •ë³´ ì—†ìŒ')}\n**ì¢…í•© ì ìˆ˜**: {round(score,2)}\n**ì„¤ëª…**: {reason}\n")
    
    reply = "\n\n".join(explanations)
    reply += f"\n\nğŸ“Œ {additional_message if additional_message else 'ë” ê¶ê¸ˆí•œ ì ì´ë‚˜ ê³ ë ¤í•˜ê³  ì‹¶ì€ ì¡°ê±´ì´ ìˆë‹¤ë©´ ë§ì”€í•´ ì£¼ì„¸ìš”. ì¶”ê°€ë¡œ ë°˜ì˜í•´ ë“œë¦´ê²Œìš”!'}"
    return {"reply": reply}

# --- Chat ë¼ìš°íŠ¸ í•¸ë“¤ëŸ¬ í—¬í¼ í•¨ìˆ˜ë“¤ ---
def _handle_pdf_upload(file_stream, user_id, state):
    user_text = extract_text_from_pdf(file_stream)
    if user_text:
        state["user_text"] = user_text
        state["shown"] = set() # íŒŒì¼ ì—…ë¡œë“œ ì‹œì—ëŠ” ê¸°ì¡´ ì¶”ì²œ ì´ë ¥ì„ ì´ˆê¸°í™”
        _update_user_state(user_id, state)
        return {"reply": "ê°ì‚¬í•©ë‹ˆë‹¤. ì´ë ¥ì„œ/ìê¸°ì†Œê°œì„œ ë‚´ìš©ì´ ì„±ê³µì ìœ¼ë¡œ ë¶„ì„ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ì œ **ê´€ì‹¬ ë¶„ì•¼, í¬ë§ ê·¼ë¬´ì§€, ì—°ë´‰**ì„ 'í’ˆì§ˆ, ì„œìš¸, 3000ë§Œì›' ë˜ëŠ” 'ì—†ìŒ, ì—†ìŒ, ì—†ìŒ'ê³¼ ê°™ì´ ì…ë ¥í•´ ì£¼ì„¸ìš”."}
    else:
        return {"reply": "PDF íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. íŒŒì¼ì´ ìŠ¤ìº”ëœ ì´ë¯¸ì§€ ê¸°ë°˜ì´ê±°ë‚˜ ì†ìƒë˜ì—ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. í…ìŠ¤íŠ¸ë¥¼ ì§ì ‘ ì…ë ¥í•´ ì£¼ì‹œê±°ë‚˜ ë‹¤ë¥¸ íŒŒì¼ì„ ì‹œë„í•´ ì£¼ì‹œê² ì–´ìš”?"}

def _handle_initial_text_input(message, user_id, state):
    state["user_text"] = message
    state["shown"] = set() # í…ìŠ¤íŠ¸ ì§ì ‘ ì…ë ¥ ì‹œì—ë„ ê¸°ì¡´ ì¶”ì²œ ì´ë ¥ ì´ˆê¸°í™”
    _update_user_state(user_id, state)
    return {"reply": "ì´ë ¥ì„œ/ìê¸°ì†Œê°œì„œ ë‚´ìš©ì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤. ì´ì œ **ê´€ì‹¬ ë¶„ì•¼, í¬ë§ ê·¼ë¬´ì§€, ì—°ë´‰**ì„ 'í’ˆì§ˆ, ì„œìš¸, 3000ë§Œì›' ë˜ëŠ” 'ì—†ìŒ, ì—†ìŒ, ì—†ìŒ'ê³¼ ê°™ì´ ì…ë ¥í•´ ì£¼ì„¸ìš”."}

def _handle_preference_input(message, user_id, state):
    if "," in message:
        parts = [p.strip() for p in message.split(",")]
        state["interest"] = parts[0] if len(parts) > 0 and parts[0].lower() != "ì—†ìŒ" else ""
        state["region"] = parts[1] if len(parts) > 1 and parts[1].lower() != "ì—†ìŒ" else ""
        state["salary"] = parts[2].replace("ë§Œì›", "") if len(parts) > 2 and parts[2].lower() != "ì—†ìŒ" else ""
        _update_user_state(user_id, state)

        new_recommendations = make_recommendations(
            user_text=state["user_text"],
            interest=state.get("interest"),
            region=state.get("region"),
            salary=state.get("salary"),
            shown_companies_set=state["shown"],
            top_n=3 # ì²« ì¶”ì²œì€ 3ê°œ
        )
        return _generate_recommendation_response(
            state["user_text"],
            new_recommendations,
            "ë” ê¶ê¸ˆí•œ ì ì´ë‚˜ ê³ ë ¤í•˜ê³  ì‹¶ì€ ì¡°ê±´ì´ ìˆë‹¤ë©´ ë§ì”€í•´ ì£¼ì„¸ìš”. ì¶”ê°€ë¡œ ë°˜ì˜í•´ ë“œë¦´ê²Œìš”! ì˜ˆë¥¼ ë“¤ì–´ 'ë” ì¶”ì²œí•´ì¤˜'ë¼ê³  ë§ì”€í•˜ì‹œë©´ ë‹¤ë¥¸ ê¸°ì—…ì„ ì°¾ì•„ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
        )
    else:
        return {"reply": "ê´€ì‹¬ ë¶„ì•¼, í¬ë§ ê·¼ë¬´ì§€, ì—°ë´‰ì„ 'í’ˆì§ˆ, ì„œìš¸, 3000ë§Œì›' ë˜ëŠ” 'ì—†ìŒ, ì—†ìŒ, ì—†ìŒ'ê³¼ ê°™ì´ ì½¤ë§ˆ(,)ë¡œ êµ¬ë¶„í•´ì„œ ì…ë ¥í•´ ì£¼ì„¸ìš”."}

def _handle_more_recommendations(user_id, state):
    new_recommendations = make_recommendations(
        user_text=state["user_text"],
        interest=state.get("interest"),
        region=state.get("region"),
        salary=state.get("salary"),
        shown_companies_set=state["shown"],
        top_n=1 # ì¶”ê°€ ì¶”ì²œì€ 1ê°œì”©
    )
    return _generate_recommendation_response(
        state["user_text"],
        new_recommendations,
        "ë” ê¶ê¸ˆí•œ ì ì´ë‚˜ ê³ ë ¤í•˜ê³  ì‹¶ì€ ì¡°ê±´ì´ ìˆë‹¤ë©´ ë§ì”€í•´ ì£¼ì„¸ìš”. ì¶”ê°€ë¡œ ë°˜ì˜í•´ ë“œë¦´ê²Œìš”! ë˜ëŠ” 'ì¶”ì²œ ì´ˆê¸°í™”'ë¼ê³  ë§ì”€í•˜ì‹œë©´ ì²˜ìŒë¶€í„° ë‹¤ì‹œ ì‹œì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
    )

def _handle_reset(user_id):
    user_states.pop(user_id, None)
    return {"reply": "ì¶”ì²œ ìƒíƒœê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤. ìƒˆë¡œìš´ ìê¸°ì†Œê°œì„œ/ì´ë ¥ì„œ íŒŒì¼ì„ ì²¨ë¶€í•˜ì‹œê±°ë‚˜ ë‚´ìš©ì„ ì§ì ‘ ì…ë ¥í•´ ì£¼ì„¸ìš”."}


# --- Flask ë¼ìš°íŠ¸ ì„¤ì • ---
@app.route("/chat", methods=["POST"])
def chat():
    if 'user_id' not in session:
        session['user_id'] = str(uuid.uuid4())
    user_id = session['user_id']
    
    state = _get_user_state(user_id)

    message = request.form.get("message", "").strip()
    file = request.files.get("file")

    try:
        # 1. "ì¶”ì²œ ì´ˆê¸°í™”" ìš”ì²­ ì²˜ë¦¬ (ì–´ë–¤ ìƒíƒœì—ì„œë“  ìš°ì„  ì²˜ë¦¬)
        if "ì¶”ì²œ ì´ˆê¸°í™”" in message:
            return jsonify(_handle_reset(user_id))

        # 2. íŒŒì¼ ì²¨ë¶€ ì‹œ ìê¸°ì†Œê°œì„œ/ì´ë ¥ì„œ ì¶”ì¶œ
        if file and file.filename != '':
            return jsonify(_handle_pdf_upload(file, user_id, state))

        # 3. íŒŒì¼ ì—†ì´ ë©”ì‹œì§€ë§Œ ìˆì„ ê²½ìš°: ìê¸°ì†Œê°œì„œ/ì´ë ¥ì„œ ì…ë ¥ ì—¬ë¶€ í™•ì¸
        if state["user_text"] is None:
            # ë©”ì‹œì§€ ê¸¸ì´ê°€ ê¸¸ê±°ë‚˜ íŠ¹ì • í‚¤ì›Œë“œë¥¼ í¬í•¨í•˜ë©´ ìê¸°ì†Œê°œì„œ/ì´ë ¥ì„œë¡œ ê°„ì£¼
            if len(message.split()) > 30 or "ì´ë ¥ì„œ" in message or "ìê¸°ì†Œê°œì„œ" in message:
                return jsonify(_handle_initial_text_input(message, user_id, state))
            else:
                return jsonify({"reply": "ê°œì¸ë³„ ë§ì¶¤ ë¶„ì„ì„ ìœ„í•´ì„œëŠ” ìê¸°ì†Œê°œì„œ í˜¹ì€ ì´ë ¥ì„œê°€ í•„ìš”í•©ë‹ˆë‹¤. íŒŒì¼ì„ ì²¨ë¶€í•´ ì£¼ì‹œê±°ë‚˜ ë‚´ìš©ì„ ì§ì ‘ ì…ë ¥í•´ ì£¼ì‹œë©´ ìƒì„¸í•œ ë¶„ì„ì„ ì œê³µí•´ ë“œë¦¬ê² ìŠµë‹ˆë‹¤."})

        # 4. ìê¸°ì†Œê°œì„œ/ì´ë ¥ì„œê°€ ì…ë ¥ë˜ì—ˆê³ , ì‚¬ìš©ì ì„ í˜¸ë„ ì •ë³´ê°€ ì—†ëŠ” ê²½ìš°
        if state["interest"] is None:
            return jsonify(_handle_preference_input(message, user_id, state))

        # 5. "ë” ì¶”ì²œí•´ì¤˜" ìš”ì²­ ì²˜ë¦¬ (ëª¨ë“  í•„ìˆ˜ ì •ë³´ê°€ ìˆëŠ” ìƒíƒœì—ì„œ)
        if "ë” ì¶”ì²œí•´ì¤˜" in message:
            return jsonify(_handle_more_recommendations(user_id, state))
            
        # 6. ê¸°íƒ€ ì¼ë°˜ ë©”ì‹œì§€ ì²˜ë¦¬ (ìê¸°ì†Œê°œì„œ/ì´ë ¥ì„œ ë° ì„ í˜¸ë„ ì…ë ¥ í›„, ìœ„ ìš”ì²­ë“¤ì— í•´ë‹¹í•˜ì§€ ì•ŠëŠ” ê²½ìš°)
        return jsonify({"reply": "ë¬´ìŠ¨ ë§ì”€ì´ì‹ ì§€ ì •í™•íˆ ì´í•´í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ìê¸°ì†Œê°œì„œ/ì´ë ¥ì„œ ì •ë³´ë¥¼ ë³€ê²½í•˜ê±°ë‚˜ 'ì¶”ì²œ ì´ˆê¸°í™”'ë¥¼ í†µí•´ ë‹¤ì‹œ ì‹œì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."})

    except Exception as e:
        print(f"âŒ ì„œë²„ ì—ëŸ¬: {e}")
        traceback.print_exc()
        return jsonify({"reply": f"âŒ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)} ë¶ˆí¸ì„ ë“œë ¤ ì£„ì†¡í•©ë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”."}), 500

if __name__ == "__main__":
    app.run(debug=True, host="0.0.0.0", port=8080)
