import os
import json
import fitz # PyMuPDF
import openai
import uuid
import re # ì •ê·œí‘œí˜„ì‹ ëª¨ë“ˆ ì¶”ê°€
import traceback # ì˜¤ë¥˜ ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤ ì¶œë ¥ì„ ìœ„í•œ ëª¨ë“ˆ ì¶”ê°€

import torch
from transformers import AutoTokenizer, AutoModel
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

from flask import Flask, request, jsonify, render_template, session
from flask_cors import CORS

app = Flask(__name__)
CORS(app)
app.secret_key = os.getenv("FLASK_SECRET_KEY", "your-super-secret-random-key-here-for-production")

# --- API í‚¤ ë¡œë”© ---
api_key = os.getenv("OPENAI_API_KEY")
if not api_key:
    raise ValueError("ì˜¤ë¥˜: OPENAI_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í”„ë¡œê·¸ë¨ì„ ì‹œì‘í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
openai.api_key = api_key

GG_CACHED_FILE = "gg_employment_cached.json"
user_states = {} # ì‚¬ìš©ìë³„ ëŒ€í™” ìƒíƒœë¥¼ ì €ì¥í•  ë”•ì…”ë„ˆë¦¬ (ì¸ë©”ëª¨ë¦¬)

# KoBERT ëª¨ë¸ ë° ì¥ì¹˜ ë³€ìˆ˜ ì´ˆê¸°í™”
tokenizer = None
model = None
device = None

# ê¸°ì—… ì •ë³´ ë° ë²¡í„° ì´ˆê¸°í™”
cached_companies = []
tfidf_vectorizer = None
company_tfidf_matrix = None

# --- KoBERT ì„ë² ë”© ìƒì„± í•¨ìˆ˜ (ë‹¨ì¼ í•¨ìˆ˜ë¡œ í†µí•©) ---
def get_kobert_embedding(text_input):
    if model is None or tokenizer is None or device is None or not text_input:
        return torch.zeros(768).to(device) # KoBERT ê¸°ë³¸ hidden_size
    try:
        inputs = tokenizer(text_input, return_tensors="pt", truncation=True, padding=True, max_length=512)
        inputs = {k: v.to(device) for k, v in inputs.items()}
        with torch.no_grad():
            outputs = model(**inputs)
        embedding = torch.mean(outputs.last_hidden_state, dim=1).squeeze()
        return embedding
    except Exception as e:
        print(f"âŒ ì„ë² ë”© ìƒì„± ì˜¤ë¥˜ (í…ìŠ¤íŠ¸: '{text_input[:30]}...'): {e}")
        traceback.print_exc()
        return torch.zeros(768).to(device)

try:
    if not os.path.exists(GG_CACHED_FILE):
        print(f"ê²½ê³ : '{GG_CACHED_FILE}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë¹ˆ íšŒì‚¬ ëª©ë¡ìœ¼ë¡œ ì‹œì‘í•©ë‹ˆë‹¤.")
    else:
        with open(GG_CACHED_FILE, "r", encoding="utf-8") as f:
            cached_companies = json.load(f)
        print(f"âœ… '{GG_CACHED_FILE}'ì—ì„œ {len(cached_companies)}ê°œ ê¸°ì—… ì •ë³´ ë¡œë“œ ì„±ê³µ.")

    # KoBERT ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ
    tokenizer = AutoTokenizer.from_pretrained("monologg/kobert")
    model = AutoModel.from_pretrained("monologg/kobert")
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)
    model.eval()
    print("âœ… KoBERT ëª¨ë¸ ë¡œë“œ ì„±ê³µ!")

    # --- ì„œë²„ ì‹œì‘ ì‹œ ëª¨ë“  ê¸°ì—… ì •ë³´ì— ëŒ€í•œ KoBERT ì„ë² ë”© ë¯¸ë¦¬ ê³„ì‚° ---
    print("ì„œë²„ ì‹œì‘ ì „, ê¸°ì—… ì •ë³´ KoBERT ì„ë² ë”©ì„ ì‹œì‘í•©ë‹ˆë‹¤... (ë°ì´í„° ì–‘ì— ë”°ë¼ ëª‡ ë¶„ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
    for company in cached_companies:
        summary_text = f"{company.get('ì±„ìš©ê³µê³ ëª…', '')} {company.get('íšŒì‚¬ëª…', '')}"
        company['embedding'] = get_kobert_embedding(summary_text) # í†µí•©ëœ í•¨ìˆ˜ ì‚¬ìš©
    print("âœ… ëª¨ë“  ê¸°ì—… ì •ë³´ì˜ KoBERT ì„ë² ë”©ì´ ì™„ë£Œë˜ì–´ ë©”ëª¨ë¦¬ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

    # --- TF-IDF Vectorizer ë° ê¸°ì—…ë³„ TF-IDF í–‰ë ¬ ë¯¸ë¦¬ ê³„ì‚° ---
    print("ì„œë²„ ì‹œì‘ ì „, ê¸°ì—… ì •ë³´ TF-IDF ë²¡í„°í™”ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
    tfidf_vectorizer = TfidfVectorizer(min_df=1, ngram_range=(1, 2))
    company_summaries_for_tfidf = [
        f"{company.get('ì±„ìš©ê³µê³ ëª…', '')} {company.get('íšŒì‚¬ëª…', '')}" for company in cached_companies
    ]
    company_tfidf_matrix = tfidf_vectorizer.fit_transform(company_summaries_for_tfidf)
    print("âœ… ëª¨ë“  ê¸°ì—… ì •ë³´ì˜ TF-IDF ë²¡í„°í™”ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")

except json.JSONDecodeError as e:
    print(f"âŒ ì˜¤ë¥˜: '{GG_CACHED_FILE}' íŒŒì¼ì´ ìœ íš¨í•œ JSON í˜•ì‹ì´ ì•„ë‹™ë‹ˆë‹¤. ì˜¤ë¥˜: {e}")
    cached_companies = []
    raise RuntimeError(f"ê¸°ì—… ì •ë³´ íŒŒì¼ ë¡œë”©ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ì˜¤ë¥˜: {e}")
except Exception as e:
    print(f"âŒ ì´ˆê¸° ì„¤ì • ì¤‘ ì¹˜ëª…ì ì¸ ì˜¤ë¥˜ ë°œìƒ: {e}")
    traceback.print_exc()
    raise RuntimeError(f"ì• í”Œë¦¬ì¼€ì´ì…˜ ì´ˆê¸° ì„¤ì •ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ì˜¤ë¥˜: {e}")


# --- PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ í•¨ìˆ˜ ---
def extract_text_from_pdf(pdf_file_stream):
    try:
        doc = fitz.open(stream=pdf_file_stream.read(), filetype="pdf")
        text_content = []
        for page in doc:
            text_content.append(page.get_text())
        raw_text = "\n".join(text_content)

        processed_text = re.sub(r'\s+', ' ', raw_text).strip()

        # ë””ë²„ê¹… ì¶œë ¥ì€ ê·¸ëŒ€ë¡œ ìœ ì§€
        print("\n--- PDFì—ì„œ ì¶”ì¶œëœ ì›ë³¸ í…ìŠ¤íŠ¸ (ì•ë¶€ë¶„ 500ì) ---")
        print(raw_text[:500])
        print("-------------------------------------------\n")
        print("--- PDFì—ì„œ ì¶”ì¶œ ë° ì „ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸ (ì•ë¶€ë¶„ 500ì) ---")
        print(processed_text[:500])
        print("---------------------------------------------------\n")

        return processed_text
    except Exception as e:
        print(f"âŒ PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        traceback.print_exc()
        return ""


# --- GPTë¥¼ ì‚¬ìš©í•˜ì—¬ í‚¤ì›Œë“œ ì¶”ì¶œ í•¨ìˆ˜ ---
def extract_keywords(text):
    if not text:
        return []

    prompt = f"""
    ë‹¤ìŒ ìê¸°ì†Œê°œì„œ ë˜ëŠ” ì´ë ¥ì„œì—ì„œ í•µì‹¬ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•´ì¤˜.
    - 5~10ê°œ ì •ë„ ë½‘ì•„ì¤˜.
    - í‚¤ì›Œë“œëŠ” ì½¤ë§ˆ(,)ë¡œ êµ¬ë¶„í•´ì„œ ì¶œë ¥í•´ì¤˜.

    ë‚´ìš©:
    {text}
    """
    try:
        response = openai.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.4
        )
        result = response.choices[0].message.content
        return [kw.strip() for kw in result.split(",") if kw.strip()]
    except Exception as e:
        print(f"âŒ GPT í‚¤ì›Œë“œ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        traceback.print_exc()
        return []


# --- KoBERT ìœ ì‚¬ë„ ê³„ì‚° í•¨ìˆ˜ ---
def kobert_similarity(user_text, companies):
    if not user_text or not companies:
        return []
    user_embedding = get_kobert_embedding(user_text)
    user_embedding_np = user_embedding.cpu().numpy().reshape(1, -1)

    results = []
    for c in companies:
        company_embedding = c.get('embedding')
        if company_embedding is not None and not torch.all(company_embedding == 0):
            company_embedding_np = company_embedding.cpu().numpy().reshape(1, -1)
            score = cosine_similarity(user_embedding_np, company_embedding_np)[0][0]
            results.append((c, float(score)))
    return sorted(results, key=lambda x: x[1], reverse=True)

# --- TF-IDF ìœ ì‚¬ë„ ê³„ì‚° í•¨ìˆ˜ ---
def tfidf_similarity(user_text, companies):
    if not user_text or not companies or tfidf_vectorizer is None or company_tfidf_matrix is None:
        return []
    try:
        user_tfidf_vector = tfidf_vectorizer.transform([user_text])
        scores = cosine_similarity(user_tfidf_vector, company_tfidf_matrix).flatten()
        results = [(companies[i], float(scores[i])) for i in range(len(scores))]
        return results
    except Exception as e:
        print(f"âŒ TF-IDF ìœ ì‚¬ë„ ê³„ì‚° ì˜¤ë¥˜: {e}")
        traceback.print_exc()
        return []

# --- GPTë¥¼ ì‚¬ìš©í•˜ì—¬ ì¶”ì²œ ì´ìœ  ìƒì„± í•¨ìˆ˜ ---
def generate_reason_individual(user_text, company, score):
    prompt = f"""
    ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ íŠ¹ì„±ê³¼ ì„ í˜¸ë„ë¥¼ íŒŒì•…í•´ ê°€ì¥ ì í•©í•œ ê¸°ì—…ì„ ë§¤ì¹­ì‹œì¼œì£¼ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
    ì‚¬ìš©ìì˜ ìê¸°ì†Œê°œì„œ ë‚´ìš©ê³¼ ê¸°ì—… ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ, ì™œ ì´ ê¸°ì—…ì´ ì‚¬ìš©ìì—ê²Œ ì í•©í•œì§€ ìƒì„¸í•˜ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”.
    ì¹œì ˆí•˜ê³  ì „ë¬¸ì ì¸ ì–´ì¡°ë¡œ, ì„¤ëª…ì€ ìì—°ìŠ¤ëŸ½ê³  ëª…í™•í•˜ê²Œ ì „ë‹¬í•´ì•¼ í•©ë‹ˆë‹¤.
    ì‹œìŠ¤í…œ ì‘ë™ ê´€ë ¨ ë¬¸êµ¬("ë¶„ì„ëª¨ë“œì…ë‹ˆë‹¤" ë“±)ëŠ” ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.
    ë¶„ì„ì´ ì–´ë ¤ìš´ ê²½ìš° "í˜„ì¬ ì •ë³´ë§Œìœ¼ë¡œëŠ” ë¶„ì„ì´ ì–´ë µìŠµë‹ˆë‹¤"ì™€ ê°™ì´ ìì—°ìŠ¤ëŸ½ê²Œ ì•ˆë‚´í•´ì£¼ì„¸ìš”.

    [ê¸°ì—… ì •ë³´]
    - ê¸°ì—…ëª…: {company.get('íšŒì‚¬ëª…', 'ì •ë³´ ì—†ìŒ')}
    - ì±„ìš©ê³µê³ ëª…: {company.get('ì±„ìš©ê³µê³ ëª…', 'ì •ë³´ ì—†ìŒ')}
    - ìœ ì‚¬ë„ ì ìˆ˜: {round(score, 2)}

    [ì‚¬ìš©ì ìê¸°ì†Œê°œì„œ]
    {user_text}

    [ì„¤ëª… ì‹œì‘]
    """
    try:
        response = openai.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.4
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        print(f"âŒ GPT ì„¤ëª… ìƒì„± ì˜¤ë¥˜: {e}")
        traceback.print_exc()
        return "ì„¤ëª…ì„ ìƒì„±í•˜ëŠ” ë° ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

# --- ì—°ë´‰ ì •ë³´ íŒŒì‹± í—¬í¼ í•¨ìˆ˜ ---
def parse_salary_info(summary_text):
    min_salary = 0
    max_salary = float('inf')

    match_annual = re.search(r'ì—°ë´‰ (\d+)(?:ë§Œì›)?(?: ~ (\d+)(?:ë§Œì›)?)?', summary_text)
    if match_annual:
        min_salary = int(match_annual.group(1))
        max_salary = int(match_annual.group(2)) if match_annual.group(2) else min_salary
        return min_salary, max_salary

    match_monthly = re.search(r'ì›”ê¸‰ (\d+)(?:ë§Œì›)?(?: ~ (\d+)(?:ë§Œì›)?)?', summary_text)
    if match_monthly:
        min_monthly = int(match_monthly.group(1))
        min_salary = min_monthly * 12
        max_monthly = int(match_monthly.group(2)) if match_monthly.group(2) else min_monthly
        max_salary = max_monthly * 12
        return min_salary, max_salary

    match_hourly = re.search(r'ì‹œê¸‰ (\d+)', summary_text)
    if match_hourly:
        hourly_wage = int(match_hourly.group(1))
        min_salary = (hourly_wage * 209 * 12) / 10000
        max_salary = min_salary
        return int(min_salary), int(max_salary)

    return 0, float('inf')

# --- ê¸°ì—… í•„í„°ë§ ë¡œì§ í•¨ìˆ˜ ---
def apply_company_filters(company, interest, region, salary):
    passes_filter = True

    if interest:
        summary_lower = company.get('summary', '').lower()
        industry_lower = company.get('industry', '').lower()
        interest_lower = interest.lower()
        if interest_lower not in summary_lower and interest_lower not in industry_lower:
            passes_filter = False

    if region and region.lower() not in company.get("region", "").lower():
        passes_filter = False

    if salary:
        try:
            min_salary_req = int(salary)
            company_min_salary, company_max_salary = parse_salary_info(company.get("summary", ""))
            if min_salary_req > company_max_salary:
                passes_filter = False
        except ValueError:
            pass # ìœ íš¨í•˜ì§€ ì•Šì€ ì—°ë´‰ ì…ë ¥ ë˜ëŠ” íŒŒì‹± ì˜¤ë¥˜ëŠ” ë¬´ì‹œ

    return passes_filter

# --- ê¸°ì—… ì¶”ì²œ ë¡œì§ í•¨ìˆ˜ (Hybrid ëª¨ë¸) ---
def make_recommendations(user_text, interest, region, salary, shown_companies_set=None, top_n=3):
    if shown_companies_set is None:
        shown_companies_set = set()

    if not user_text or not cached_companies:
        return []

    kobert_ranked_companies = kobert_similarity(user_text, cached_companies)
    tfidf_ranked_companies = tfidf_similarity(user_text, cached_companies)

    tfidf_scores = {
        (c.get("name"), c.get("summary")): score
        for c, score in tfidf_ranked_companies
    }

    hybrid_scores = []
    for company, kobert_score in kobert_ranked_companies:
        company_key = (company.get("name"), company.get("summary"))
        tfidf_score = tfidf_scores.get(company_key, 0.0)

        kobert_weight = 0.7
        tfidf_weight = 0.3
        final_score = (kobert_weight * kobert_score) + (tfidf_weight * tfidf_score)

        if apply_company_filters(company, interest, region, salary):
            hybrid_scores.append((company, final_score))

    hybrid_scores.sort(key=lambda x: x[1], reverse=True)

    results = []
    for comp, sim in hybrid_scores:
        comp_id_str = json.dumps((comp.get("name"), comp.get("summary")), ensure_ascii=False)
        if comp_id_str not in shown_companies_set:
            shown_companies_set.add(comp_id_str)
            results.append((comp, sim))
        if len(results) >= top_n:
            break

    return results

# --- ì‚¬ìš©ì ìƒíƒœ ê´€ë¦¬ í—¬í¼ í•¨ìˆ˜ ---
def _get_user_state(user_id):
    return user_states.get(user_id, {
        "shown": set(),
        "user_text": None,
        "interest": None,
        "region": None,
        "salary": None
    })

def _update_user_state(user_id, state):
    user_states[user_id] = state

# --- ì¶”ì²œ ê²°ê³¼ ì‘ë‹µ ìƒì„± í—¬í¼ í•¨ìˆ˜ (ì¤‘ë³µ ë¡œì§ ì œê±°) ---
def _generate_recommendation_response(user_text, recommendations, additional_message=""):
    if not recommendations:
        return "ì•„ì‰½ê²Œë„ í˜„ì¬ ì¡°ê±´ì— ë§ëŠ” ê¸°ì—…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì¡°ê±´ì„ ë§ì”€í•´ì£¼ì‹œê±°ë‚˜ 'ì¶”ì²œ ì´ˆê¸°í™”'ë¥¼ í†µí•´ ë‹¤ì‹œ ì‹œì‘í•´ ì£¼ì‹œê² ì–´ìš”?"

    explanations = []
    for company, score in recommendations:
        company_info_for_gpt = {k: v for k, v in company.items() if k not in ['embedding', 'summary']}
        reason = generate_reason_individual(user_text, company_info_for_gpt, score)
        explanations.append(f"**ê¸°ì—…ëª…**: {company_info_for_gpt.get('name', 'ì •ë³´ ì—†ìŒ')}\n**ì±„ìš©ê³µê³ ëª…**: {company_info_for_gpt.get('summary', 'ì •ë³´ ì—†ìŒ')}\n**ì¢…í•© ì ìˆ˜**: {round(score,2)}\n**ì„¤ëª…**: {reason}\n")
    
    reply = "\n\n".join(explanations)
    reply += f"\n\nğŸ“Œ {additional_message if additional_message else 'ë” ê¶ê¸ˆí•œ ì ì´ë‚˜ ê³ ë ¤í•˜ê³  ì‹¶ì€ ì¡°ê±´ì´ ìˆë‹¤ë©´ ë§ì”€í•´ ì£¼ì„¸ìš”. ì¶”ê°€ë¡œ ë°˜ì˜í•´ ë“œë¦´ê²Œìš”!'}"
    return reply

# --- Chat ë¼ìš°íŠ¸ í•¸ë“¤ëŸ¬ í—¬í¼ í•¨ìˆ˜ë“¤ ---
def _handle_pdf_upload(file_stream, user_id, state):
    user_text = extract_text_from_pdf(file_stream)
    if user_text:
        state["user_text"] = user_text
        state["shown"] = set() # íŒŒì¼ ì—…ë¡œë“œ ì‹œ ì´ˆê¸°í™”
        _update_user_state(user_id, state)
        return {"reply": "ê°ì‚¬í•©ë‹ˆë‹¤. ì´ë ¥ì„œ/ìê¸°ì†Œê°œì„œ ë‚´ìš©ì´ ì„±ê³µì ìœ¼ë¡œ ë¶„ì„ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ì œ **ê´€ì‹¬ ë¶„ì•¼, í¬ë§ ê·¼ë¬´ì§€, ì—°ë´‰**ì„ 'í’ˆì§ˆ, ì„œìš¸, 3000ë§Œì›' ë˜ëŠ” 'ì—†ìŒ, ì—†ìŒ, ì—†ìŒ'ê³¼ ê°™ì´ ì…ë ¥í•´ ì£¼ì„¸ìš”."}
    else:
        return {"reply": "PDF íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. íŒŒì¼ì´ ìŠ¤ìº”ëœ ì´ë¯¸ì§€ ê¸°ë°˜ì´ê±°ë‚˜ ì†ìƒë˜ì—ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. í…ìŠ¤íŠ¸ë¥¼ ì§ì ‘ ì…ë ¥í•´ ì£¼ì‹œê±°ë‚˜ ë‹¤ë¥¸ íŒŒì¼ì„ ì‹œë„í•´ ì£¼ì‹œê² ì–´ìš”?"}

def _handle_initial_text_input(message, user_id, state):
    state["user_text"] = message
    state["shown"] = set() # í…ìŠ¤íŠ¸ ì§ì ‘ ì…ë ¥ ì‹œ ì´ˆê¸°í™”
    _update_user_state(user_id, state)
    return {"reply": "ì´ë ¥ì„œ/ìê¸°ì†Œê°œì„œ ë‚´ìš©ì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤. ì´ì œ **ê´€ì‹¬ ë¶„ì•¼, í¬ë§ ê·¼ë¬´ì§€, ì—°ë´‰**ì„ 'í’ˆì§ˆ, ì„œìš¸, 3000ë§Œì›' ë˜ëŠ” 'ì—†ìŒ, ì—†ìŒ, ì—†ìŒ'ê³¼ ê°™ì´ ì…ë ¥í•´ ì£¼ì„¸ìš”."}

def _handle_preference_input(message, user_id, state):
    if "," in message:
        parts = [p.strip() for p in message.split(",")]
        state["interest"] = parts[0] if len(parts) > 0 and parts[0].lower() != "ì—†ìŒ" else ""
        state["region"] = parts[1] if len(parts) > 1 and parts[1].lower() != "ì—†ìŒ" else ""
        state["salary"] = parts[2].replace("ë§Œì›", "") if len(parts) > 2 and parts[2].lower() != "ì—†ìŒ" else ""
        _update_user_state(user_id, state)

        new_recommendations = make_recommendations(
            user_text=state["user_text"],
            interest=state.get("interest"),
            region=state.get("region"),
            salary=state.get("salary"),
            shown_companies_set=state["shown"],
            top_n=3
        )
        return {"reply": _generate_recommendation_response(
            state["user_text"],
            new_recommendations,
            "ë” ê¶ê¸ˆí•œ ì ì´ë‚˜ ê³ ë ¤í•˜ê³  ì‹¶ì€ ì¡°ê±´ì´ ìˆë‹¤ë©´ ë§ì”€í•´ ì£¼ì„¸ìš”. ì¶”ê°€ë¡œ ë°˜ì˜í•´ ë“œë¦´ê²Œìš”! ì˜ˆë¥¼ ë“¤ì–´ 'ë” ì¶”ì²œí•´ì¤˜'ë¼ê³  ë§ì”€í•˜ì‹œë©´ ë‹¤ë¥¸ ê¸°ì—…ì„ ì°¾ì•„ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
        )}
    else:
        return {"reply": "ê´€ì‹¬ ë¶„ì•¼, í¬ë§ ê·¼ë¬´ì§€, ì—°ë´‰ì„ 'í’ˆì§ˆ, ì„œìš¸, 3000ë§Œì›' ë˜ëŠ” 'ì—†ìŒ, ì—†ìŒ, ì—†ìŒ'ê³¼ ê°™ì´ ì½¤ë§ˆ(,)ë¡œ êµ¬ë¶„í•´ì„œ ì…ë ¥í•´ ì£¼ì„¸ìš”."}

def _handle_more_recommendations(user_id, state):
    new_recommendations = make_recommendations(
        user_text=state["user_text"],
        interest=state.get("interest"),
        region=state.get("region"),
        salary=state.get("salary"),
        shown_companies_set=state["shown"],
        top_n=1
    )
    return {"reply": _generate_recommendation_response(
        state["user_text"],
        new_recommendations,
        "ë” ê¶ê¸ˆí•œ ì ì´ë‚˜ ê³ ë ¤í•˜ê³  ì‹¶ì€ ì¡°ê±´ì´ ìˆë‹¤ë©´ ë§ì”€í•´ ì£¼ì„¸ìš”. ì¶”ê°€ë¡œ ë°˜ì˜í•´ ë“œë¦´ê²Œìš”! ë˜ëŠ” 'ì¶”ì²œ ì´ˆê¸°í™”'ë¼ê³  ë§ì”€í•˜ì‹œë©´ ì²˜ìŒë¶€í„° ë‹¤ì‹œ ì‹œì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
    )}

def _handle_reset(user_id):
    user_states.pop(user_id, None)
    return {"reply": "ì¶”ì²œ ìƒíƒœê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤. ìƒˆë¡œìš´ ìê¸°ì†Œê°œì„œ/ì´ë ¥ì„œ íŒŒì¼ì„ ì²¨ë¶€í•˜ì‹œê±°ë‚˜ ë‚´ìš©ì„ ì§ì ‘ ì…ë ¥í•´ ì£¼ì„¸ìš”."}

# --- Flask ë¼ìš°íŠ¸ ì„¤ì • ---
@app.route("/")
def index():
    return render_template("index.html")

@app.route("/chat", methods=["POST"])
def chat():
    if 'user_id' not in session:
        session['user_id'] = str(uuid.uuid4())
    user_id = session['user_id']
    state = _get_user_state(user_id)

    message = request.form.get("message", "").strip()
    file = request.files.get("file")

    try:
        # 1. íŒŒì¼ ì²¨ë¶€ ì²˜ë¦¬
        if file and file.filename != '':
            return jsonify(_handle_pdf_upload(file, user_id, state))

        # 2. ìê¸°ì†Œê°œì„œ/ì´ë ¥ì„œ í…ìŠ¤íŠ¸ ì…ë ¥ ì²˜ë¦¬ (ì•„ì§ ìê¸°ì†Œê°œì„œê°€ ì—†ëŠ” ê²½ìš°)
        if state["user_text"] is None:
            if len(message.split()) > 30 or "ì´ë ¥ì„œ" in message or "ìê¸°ì†Œê°œì„œ" in message:
                return jsonify(_handle_initial_text_input(message, user_id, state))
            else:
                return jsonify({"reply": "ê°œì¸ë³„ ë§ì¶¤ ë¶„ì„ì„ ìœ„í•´ì„œëŠ” ìê¸°ì†Œê°œì„œ í˜¹ì€ ì´ë ¥ì„œê°€ í•„ìš”í•©ë‹ˆë‹¤. íŒŒì¼ì„ ì²¨ë¶€í•´ ì£¼ì‹œê±°ë‚˜ ë‚´ìš©ì„ ì§ì ‘ ì…ë ¥í•´ ì£¼ì‹œë©´ ìƒì„¸í•œ ë¶„ì„ì„ ì œê³µí•´ ë“œë¦¬ê² ìŠµë‹ˆë‹¤."})

        # 3. ì‚¬ìš©ì ì„ í˜¸ë„(ê´€ì‹¬ ë¶„ì•¼, ì§€ì—­, ì—°ë´‰) ì…ë ¥ ì²˜ë¦¬ (ìê¸°ì†Œê°œì„œê°€ ìˆê³  ì„ í˜¸ë„ê°€ ì—†ëŠ” ê²½ìš°)
        if state["interest"] is None:
            return jsonify(_handle_preference_input(message, user_id, state))

        # 4. "ë” ì¶”ì²œí•´ì¤˜" ìš”ì²­ ì²˜ë¦¬
        if "ë” ì¶”ì²œ
