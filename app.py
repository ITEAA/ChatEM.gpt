import os
import json
import fitz # PyMuPDF
import openai
import xml.etree.ElementTree as ET
import requests
import pandas as pd

from flask import Flask, request, jsonify, render_template
from flask_cors import CORS
from werkzeug.utils import secure_filename
from sklearn.metrics.pairwise import cosine_similarity

# KoBERT ì‚¬ìš©ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
import torch
from transformers import AutoTokenizer, AutoModel

app = Flask(__name__)
CORS(app)

# í™˜ê²½ ë³€ìˆ˜ì—ì„œ OpenAI API í‚¤ ë¡œë“œ ë˜ëŠ” ê¸°ë³¸ê°’ ì„¤ì •
openai.api_key = os.getenv("OPENAI_API_KEY") or "your-api-key"
GG_CACHED_FILE = "gg_employment_cached.json"

user_states = {}

# GG ìºì‹œëœ ë°ì´í„° ë¡œë“œ (íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´ ì—ëŸ¬ ë°œìƒ ê°€ëŠ¥ì„± ìˆìŒ)
try:
    with open(GG_CACHED_FILE, "r", encoding="utf-8") as f:
        cached_companies = json.load(f)
except FileNotFoundError:
    print(f"ê²½ê³ : '{GG_CACHED_FILE}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë¹ˆ íšŒì‚¬ ëª©ë¡ìœ¼ë¡œ ì‹œì‘í•©ë‹ˆë‹¤.")
    cached_companies = []
except json.JSONDecodeError:
    print(f"ì˜¤ë¥˜: '{GG_CACHED_FILE}' íŒŒì¼ì´ ìœ íš¨í•œ JSON í˜•ì‹ì´ ì•„ë‹™ë‹ˆë‹¤.")
    cached_companies = []


# KoBERT ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ
# ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹œì‘ ì‹œ í•œ ë²ˆë§Œ ë¡œë“œí•˜ì—¬ íš¨ìœ¨ì„±ì„ ë†’ì…ë‹ˆë‹¤.
try:
    tokenizer = AutoTokenizer.from_pretrained("monologg/kobert")
    model = AutoModel.from_pretrained("monologg/kobert")
    # GPU(CUDA) ì‚¬ìš© ê°€ëŠ¥ ì‹œ ëª¨ë¸ì„ GPUë¡œ ì´ë™
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)
    model.eval() # ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì„¤ì • (ë“œë¡­ì•„ì›ƒ/ë°°ì¹˜ ì •ê·œí™” ë¹„í™œì„±í™”)
    print("KoBERT ëª¨ë¸ ë¡œë“œ ì„±ê³µ!")
except Exception as e:
    print(f"âŒ KoBERT ëª¨ë¸ ë¡œë“œ ì˜¤ë¥˜: {e}")
    print("KoBERT ì‚¬ìš©ì„ ìœ„í•´ 'transformers', 'torch', 'sentencepiece' ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
    # ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ ì‹œ, KoBERT ê´€ë ¨ ê¸°ëŠ¥ì„ ì‚¬ìš©í•˜ì§€ ì•Šê±°ë‚˜ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ì¢…ë£Œí•˜ëŠ” ë“±ì˜ ì˜ˆì™¸ ì²˜ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤.
    # ì—¬ê¸°ì„œëŠ” ì˜¤ë¥˜ ë©”ì‹œì§€ ì¶œë ¥ í›„ ê³„ì† ì§„í–‰ (KoBERT ê´€ë ¨ í•¨ìˆ˜ì—ì„œ ì¶”ê°€ ì˜¤ë¥˜ ë°œìƒ ê°€ëŠ¥)


# --- PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ í•¨ìˆ˜ ---
def extract_text_from_pdf(pdf_file):
    """PDF íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    doc = fitz.open(stream=pdf_file.read(), filetype="pdf")
    text = "\n".join(page.get_text() for page in doc)
    return text.strip()

# --- GPTë¥¼ ì‚¬ìš©í•˜ì—¬ í‚¤ì›Œë“œ ì¶”ì¶œ í•¨ìˆ˜ ---
def extract_keywords(text):
    """GPT ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    prompt = f"""
    ë‹¤ìŒ ìê¸°ì†Œê°œì„œ ë˜ëŠ” ì´ë ¥ì„œì—ì„œ í•µì‹¬ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•´ì¤˜.
    - 5~10ê°œ ì •ë„ ë½‘ì•„ì¤˜.
    - í‚¤ì›Œë“œëŠ” ì½¤ë§ˆ(,)ë¡œ êµ¬ë¶„í•´ì„œ ì¶œë ¥í•´ì¤˜.

    ë‚´ìš©:
    {text}
    """
    try:
        response = openai.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.4
        )
        result = response.choices[0].message.content
        return [kw.strip() for kw in result.split(",") if kw.strip()]
    except Exception as e:
        print(f"âŒ GPT í‚¤ì›Œë“œ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        return []

# --- KoBERT ì„ë² ë”© ìƒì„± í•¨ìˆ˜ ---
def get_kobert_embedding(text):
    """ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ì— ëŒ€í•œ KoBERT ì„ë² ë”©(ë²¡í„°)ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    if not text:
        # ë¹ˆ í…ìŠ¤íŠ¸ì¸ ê²½ìš° KoBERT ëª¨ë¸ì˜ hidden_sizeì™€ ë™ì¼í•œ í¬ê¸°ì˜ 0 ë²¡í„° ë°˜í™˜
        return torch.zeros(model.config.hidden_size).to(device)
    try:
        # í…ìŠ¤íŠ¸ë¥¼ í† í°í™”í•˜ê³  PyTorch í…ì„œë¡œ ë³€í™˜
        # max_length=512ëŠ” KoBERTì˜ ìµœëŒ€ ì…ë ¥ ê¸¸ì´ì— ë§ì¶¥ë‹ˆë‹¤.
        inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=512)
        # ì…ë ¥ì„ ëª¨ë¸ì´ ìˆëŠ” ì¥ì¹˜(CPU/GPU)ë¡œ ì´ë™
        inputs = {k: v.to(device) for k, v in inputs.items()}

        with torch.no_grad(): # ê¸°ìš¸ê¸° ê³„ì‚° ë¹„í™œì„±í™” (ë©”ëª¨ë¦¬ ì ˆì•½, ì†ë„ í–¥ìƒ)
            outputs = model(**inputs)

        # ë§ˆì§€ë§‰ ì€ë‹‰ ìƒíƒœ(Last Hidden State)ì˜ í‰ê· ì„ ë¬¸ì¥ ì„ë² ë”©ìœ¼ë¡œ ì‚¬ìš©
        # outputs.last_hidden_stateì˜ í˜•íƒœëŠ” (batch_size, sequence_length, hidden_size)
        # dim=1ë¡œ í‰ê· ì„ ë‚´ì–´ (batch_size, hidden_size) í˜•íƒœë¡œ ë§Œë“­ë‹ˆë‹¤.
        # squeeze()ë¥¼ í†µí•´ ì°¨ì› 1ì„ ì œê±°í•˜ì—¬ (hidden_size,) í˜•íƒœë¡œ ë§Œë“­ë‹ˆë‹¤.
        embedding = torch.mean(outputs.last_hidden_state, dim=1).squeeze()
        return embedding
    except Exception as e:
        print(f"âŒ KoBERT ì„ë² ë”© ìƒì„± ì˜¤ë¥˜ (í…ìŠ¤íŠ¸ ì•ë¶€ë¶„: '{text[:50]}...'): {e}")
        # ì˜¤ë¥˜ ë°œìƒ ì‹œ 0 ë²¡í„° ë°˜í™˜í•˜ì—¬ í”„ë¡œê·¸ë¨ ì¤‘ë‹¨ ë°©ì§€
        return torch.zeros(model.config.hidden_size).to(device)


# --- KoBERTë¥¼ ì‚¬ìš©í•œ ìœ ì‚¬ë„ ê³„ì‚° í•¨ìˆ˜ ---
def kobert_similarity(user_text, companies):
    """
    ì‚¬ìš©ì í…ìŠ¤íŠ¸ì™€ íšŒì‚¬ ìš”ì•½ ê°„ì˜ KoBERT ê¸°ë°˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•˜ê³  ì •ë ¬í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    if not user_text:
        return []

    user_embedding = get_kobert_embedding(user_text)
    # NumPy ë°°ì—´ë¡œ ë³€í™˜ ë° sklearn cosine_similarity í•¨ìˆ˜ë¥¼ ìœ„í•œ ì°¨ì› ì¡°ì •
    user_embedding_np = user_embedding.cpu().numpy().reshape(1, -1)

    company_data_with_summaries = []
    for c in companies:
        summary = f"{c.get('ì±„ìš©ê³µê³ ëª…', '')} {c.get('íšŒì‚¬ëª…', '')}"
        company_data_with_summaries.append((c, summary))

    if not company_data_with_summaries:
        return []

    # ëª¨ë“  íšŒì‚¬ ìš”ì•½ì— ëŒ€í•œ ì„ë² ë”©ì„ ì¼ê´„ì ìœ¼ë¡œ ê³„ì‚° (ì„±ëŠ¥ í–¥ìƒ)
    company_summaries = [item[1] for item in company_data_with_summaries]
    
    # KoBERT ì„ë² ë”©ì„ ìƒì„±í•  ë•Œ ë¹ˆ ë¬¸ìì—´ì´ ìˆìœ¼ë©´ torch.stackì—ì„œ ì˜¤ë¥˜ê°€ ë°œìƒí•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ,
    # ìœ íš¨í•œ ì„ë² ë”©ë§Œ ìŠ¤íƒí•˜ë„ë¡ í•„í„°ë§í•©ë‹ˆë‹¤.
    company_embeddings_list = [get_kobert_embedding(s) for s in company_summaries]
    
    # ìœ íš¨í•œ ì„ë² ë”©ë§Œ í•„í„°ë§ (get_kobert_embeddingì—ì„œ 0 ë²¡í„°ë¥¼ ë°˜í™˜í–ˆëŠ”ì§€ í™•ì¸)
    valid_embeddings_and_indices = []
    for i, emb in enumerate(company_embeddings_list):
        # 0 ë²¡í„°ê°€ ì•„ë‹ˆê±°ë‚˜, 0 ë²¡í„°ì—¬ë„ ì‹¤ì œ ê¸¸ì´ê°€ ìˆëŠ” ê²½ìš°ë§Œ ìœ íš¨í•˜ë‹¤ê³  íŒë‹¨
        if not torch.all(emb == 0) or len(company_summaries[i]) > 0:
            valid_embeddings_and_indices.append((emb, i))
    
    if not valid_embeddings_and_indices:
        return []

    # ìœ íš¨í•œ ì„ë² ë”©ë“¤ë§Œ ìŠ¤íƒ
    valid_company_embeddings_tensor = torch.stack([item[0] for item in valid_embeddings_and_indices])
    # CPUë¡œ ì˜®ê¸´ í›„ NumPy ë°°ì—´ë¡œ ë³€í™˜
    valid_company_embeddings_np = valid_company_embeddings_tensor.cpu().numpy()

    # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚° (ì‚¬ìš©ì ì„ë² ë”©ê³¼ ëª¨ë“  ìœ íš¨í•œ íšŒì‚¬ ì„ë² ë”© ê°„)
    cosine_sim_scores = cosine_similarity(user_embedding_np, valid_company_embeddings_np).flatten()

    results = []
    # ìœ ì‚¬ë„ ì ìˆ˜ì™€ ì›ë³¸ íšŒì‚¬ ì •ë³´ë¥¼ ë§¤í•‘
    for i, score in enumerate(cosine_sim_scores):
        original_index = valid_embeddings_and_indices[i][1]
        results.append((company_data_with_summaries[original_index][0], score))

    # ìœ ì‚¬ë„ ì ìˆ˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
    return sorted(results, key=lambda x: x[1], reverse=True)


# --- GPTë¥¼ ì‚¬ìš©í•˜ì—¬ ì¶”ì²œ ì´ìœ  ìƒì„± í•¨ìˆ˜ ---
def generate_reason_individual(user_text, company, score):
    """GPT ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ íŠ¹ì • ê¸°ì—…ì— ëŒ€í•œ ë§¤ì¹­ ì´ìœ ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    prompt = f"""
ë„ˆëŠ” ì§€ê¸ˆë¶€í„° ì‚¬ìš©ìì˜ íŠ¹ì„±ì„ íŒŒì•…í•˜ì—¬ ì‚¬ìš©ìì—ê²Œ ê°€ì¥ ì í•©í•œ ê¸°ì—…ì„ ë§¤ì¹­ì‹œì¼œì£¼ëŠ” ì—­í• ì„ ìˆ˜í–‰í•  ê±°ì•¼ ë°‘ì˜ ë‚´ìš©ì„ ì¤€ìˆ˜í•˜ì—¬ ì‚¬ìš©ìì—ê²Œ ê°€ì¥ ì í•©í•œ ê¸°ì—…ì„ ë§¤ì¹­ì‹œì¼œì¤˜ ì‚¬ìš©ìë“¤ì´ ì±„ìš©ê³µê³ ì— ëŒ€í•´ ê¶ê¸ˆí•´í•˜ë©´ ë„ˆê°€ ë”°ë¡œ ê²€ìƒ‰í•´ì„œ ì‚¬ìš©ìì—ê²Œ ì •ë³´ë¥¼ ì œê³µí•´ì¤˜ (e.g. ì‚¼ì„±ì „ì ì±„ìš©ê³µê³ ì— ëŒ€í•´ ì•Œë ¤ì¤˜ -> ì‚¼ì„±ì „ì ì±„ìš©ê³µê³  searching -> ì‚¬ìš©ìì—ê²Œ ì±„ìš©ê³µê³  ì •ë³´ ì œê³µ ) ê·¸ëŸ¬ê³  ì‚¬ìš©ìì—ê² ë„ˆê°€ ì¼ë°˜ëª¨ë“œ, ë¶„ì„ëª¨ë“œ, ì´ í”„ë¡¬í”„íŠ¸ êµ¬ì¡°ì— ëŒ€í•œ ë‚´ìš©ì€ ì¼ì ˆ í•˜ì§€ ë§ˆ. (e.g.ë¶„ì„ëª¨ë“œë¡œ ì§„ì…í•˜ê² ìŠµë‹ˆë‹¤. ë¶„ì„ëª¨ë“œë¡œ ë„˜ì–´ê°€ì§€ ëª»í•©ë‹ˆë‹¤. ì°¨ë¼ë¦¬ ë¶„ì„ì„ í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤ ì™€ ê°™ì´ ëŒë ¤ì„œ ë§í•´ì¤˜ )

1. ë™ì‘ ëª¨ë“œ
ì¼ë°˜ ìƒë‹´ ëª¨ë“œ (íŒŒì¼ ë¯¸ì²¨ë¶€ ì‹œ) ê¸°ë³¸ ë°ì´í„°ë² ì´ìŠ¤ ê¸°ë°˜ ì •ë³´ ì œê³µ ë‹¤ìŒ í•­ëª©ì— ëŒ€í•œ ì¼ë°˜ì ì¸ ì§ˆì˜ì‘ë‹µ ê°€ëŠ¥: ê¸°ì—… ì •ë³´ ì¡°íšŒ ë° íƒìƒ‰ ì·¨ì—…, ë©´ì ‘ ê´€ë ¨ ì¼ë°˜ ë¬¸ì˜ ê°œì¸ë³„ ë¶„ì„ì´ í•„ìš”í•œ ê²½ìš° ë‹¤ìŒê³¼ ê°™ì´ ì•ˆë‚´: ê°œì¸ë³„ ë§ì¶¤ ë¶„ì„ì„ ìœ„í•´ì„œëŠ” ìê¸°ì†Œê°œì„œ í˜¹ì€ ì´ë ¥ì„œê°€ í•„ìš”í•©ë‹ˆë‹¤. íŒŒì¼ì„ ì²¨ë¶€í•´ ì£¼ì‹œë©´ ìƒì„¸í•œ ë¶„ì„ì„ ì œê³µí•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤. ì‚¬ìš©ìê°€ íŒŒì¼ì„ ì²¨ë¶€í•˜ì§€ ì•Šìœ¼ë©´
B. ë¶„ì„ëª¨ë“œë¡œ ë„˜ì–´ê°€ì§€ ì•ŠëŠ”ë‹¤. ë‹¨, ì‚¬ìš©ìê°€ ì´ë ¥ì„œ, ìê¸°ì†Œê°œì„œ ë“±ì„ íŒŒì¼ì´ ì•„ë‹Œ ë©”ì‹œì§€ë¡œ ë³´ëƒˆì„ ê²½ìš°ì—ëŠ” ìê¸°ì†Œê°œì„œ, ì´ë ¥ì„œë¡œ ì¸ì‹í•˜ê³  ì˜ˆì™¸ì ìœ¼ë¡œ ë„˜ì–´ê°„ë‹¤. B. ë¶„ì„ ëª¨ë“œ (íŒŒì¼ ì²¨ë¶€ ì‹œ)

2. ê³µí†µ ê¸°ëŠ¥ ë°ì´í„°ë² ì´ìŠ¤(íŒŒì¼) ì°¸ì¡°

3. ëŒ€í™” ê·œì¹™ ì¹œì ˆí•˜ê³  ì „ë¬¸ì ì¸ ì–´ì¡° ìœ ì§€ ë‹µë³€ ê°€ëŠ¥í•œ ë²”ìœ„ë¥¼ ëª…í™•íˆ ì•ˆë‚´ í•„ìš” ì‹œ ì¶”ê°€ ì§ˆë¬¸ì„ í†µí•œ ì •í™•í•œ ì •ë³´ ì œê³µ ë¶„ì„ì´ í•„ìš”í•œ ì§ˆë¬¸ì˜ ê²½ìš° íŒŒì¼ ì²¨ë¶€ ì•ˆë‚´

4. ì˜ˆì™¸ ì²˜ë¦¬ ë¶ˆëª…í™•í•œ ì§ˆë¬¸ì— ëŒ€í•´ì„œëŠ” êµ¬ì²´í™” ìš”ì²­

5. ë¶„ì„ ëª¨ë“œ ì§„í–‰ ìˆœì„œ ìê¸°ì†Œê°œì„œ or ì´ë ¥ì„œ í™•ì¸(íŒŒì¼ or ì‚¬ìš©ìê°€ ì´ë ¥ì„œ or ìê¸°ì†Œê°œì„œë¼ê³  ë³´ë‚¸ ë©”ì‹œì§€) ì‚¬ìš©ì ì •ë³´ ë¶„ì„ ë° ì„±í–¥ ì¶œë ¥ ì‚¬ìš©ì ì„ í˜¸ë„ ì¡°ì‚¬ ì§„í–‰ ì‚¬ìš©ì ì‘ë‹µì„ ë°›ê¸° ì „ê¹Œì§€ ë‹¤ìŒ ë‹¨ê³„ë¡œ ì§„í–‰í•˜ì§€ ì•ŠìŒ ì‘ë‹µ ê±°ë¶€ ì‹œ ê¸°ë³¸ ì¶”ì²œ ë¡œì§ ì‚¬ìš©ì„ ì•ˆë‚´í•˜ê³  í™•ì¸ ìš”ì²­ ì„ í˜¸ë„ ê¸°ë°˜ ê¸°ì—… ì¶”ì²œ

6. ì‚¬ìš©ì ì„ í˜¸ë„ ì¡°ì‚¬ í”„ë¡œì„¸ìŠ¤ ê¸°ë³¸ ì •ë³´ ì¶œë ¥ í›„ ë°˜ë“œì‹œ ì¤‘ë‹¨ ë‹¤ìŒ ì•ˆë‚´ ë©”ì‹œì§€ ì¶œë ¥: ì§€ê¸ˆê¹Œì§€ ë¶„ì„í•œ ë‚´ìš©ì„ í† ëŒ€ë¡œ ë§ì¶¤í˜• êµê³¼ëª©ì„ ì¶”ì²œí•´ë“œë¦¬ê³ ì í•©ë‹ˆë‹¤. ì¶”ì²œì„ ìœ„í•´ ëª‡ ê°€ì§€ ì—¬ì­¤ë³´ê² ìŠµë‹ˆë‹¤. 1. ì–´ë–¤ ì‚°ì—…ì´ë‚˜ ë¶„ì•¼ì— ê´€ì‹¬ì´ ìˆìœ¼ì‹ ê°€ìš”? 2. ì„ í˜¸í•˜ëŠ” ë©´ì ‘ ë°©ì‹ì´ë‚˜ íŠ¹ë³„íˆ ê³ ë ¤í•´ì•¼ í•  ì‚¬í•­ì´ ìˆìœ¼ì‹ ê°€ìš”? ìœ„ ì§ˆë¬¸ë“¤ì— ëŒ€í•´ ë‹µë³€í•´ ì£¼ì‹œë©´ ê·¸ì— ë§ì¶° ê¸°ì—…ì„ ì¶”ì²œí•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤. íŠ¹ë³„í•œ ì„ í˜¸ë„ê°€ ì—†ìœ¼ì‹œë‹¤ë©´ "ì—†ìŒ"ì´ë¼ê³  ë‹µë³€í•´ ì£¼ì„¸ìš”. ì‚¬ìš©ì ì‘ë‹µ ëŒ€ê¸° ì‘ë‹µ ë°›ì€ í›„ì—ë§Œ ì¶”ì²œ ì§„í–‰

7. ê¸°ë³¸ ì—­í•  ìê¸°ì†Œê°œì„œ, ì´ë ¥ì„œ ê¸°ë°˜ ê¸°ì—… ë§¤ì¹­ ì‚¬ìš©ìì™€ ê¸°ì—… ê°„ì˜ ë§¤ì¹­ ê·¼ê±° ì œê³µ ì‚¬ìš©ì ì„ í˜¸ë„ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ ì¶”ê°€ ê¸°ì—… ì œì‹œ

ê¸°ì—…ëª…: {company.get('íšŒì‚¬ëª…')}
ì—…ë¬´: {company.get('ì±„ìš©ê³µê³ ëª…')}
ìœ ì‚¬ë„ ì ìˆ˜: {round(score, 2)}

[ìê¸°ì†Œê°œì„œ]
{user_text}

[ì„¤ëª…]
"""
    try:
        response = openai.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.4
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        print(f"âŒ GPT ì„¤ëª… ì˜¤ë¥˜: {e}")
        return "ì„¤ëª…ì„ ìƒì„±í•˜ëŠ” ë° ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

# --- ê¸°ì—… ì¶”ì²œ ë¡œì§ í•¨ìˆ˜ ---
def make_recommendations(user_text, interest, region, salary, shown=set(), top_n=3):
    """
    ì‚¬ìš©ì í…ìŠ¤íŠ¸ì™€ ì„ í˜¸ë„(ê´€ì‹¬ ë¶„ì•¼, ì§€ì—­, ì—°ë´‰)ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê¸°ì—…ì„ ì¶”ì²œí•©ë‹ˆë‹¤.
    """
    if not user_text:
        return []

    # 1. GPTë¥¼ ì´ìš©í•´ ìê¸°ì†Œê°œì„œ/ì´ë ¥ì„œì—ì„œ í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ
    keywords = extract_keywords(user_text)

    # 2. í‚¤ì›Œë“œ, ê´€ì‹¬ ë¶„ì•¼, ì§€ì—­, ì—°ë´‰ ë“±ì„ ê³ ë ¤í•œ ì´ˆê¸° ìŠ¤ì½”ì–´ë§
    def score_company(company):
        s = 0
        summary = company.get("ì±„ìš©ê³µê³ ëª…", "") + " " + company.get("íšŒì‚¬ëª…", "")
        # ì¶”ì¶œëœ í‚¤ì›Œë“œê°€ íšŒì‚¬ ìš”ì•½ì— í¬í•¨ë˜ë©´ ì ìˆ˜ ë¶€ì—¬
        if any(kw in summary for kw in keywords):
            s += 1
        # ê´€ì‹¬ ë¶„ì•¼ê°€ íšŒì‚¬ ìš”ì•½ì— í¬í•¨ë˜ë©´ ì ìˆ˜ ë¶€ì—¬
        if interest and interest in summary:
            s += 0.3
        # í¬ë§ ê·¼ë¬´ ì§€ì—­ì´ íšŒì‚¬ ì •ë³´ì— í¬í•¨ë˜ë©´ ì ìˆ˜ ë¶€ì—¬
        if region and region in company.get("ê·¼ë¬´ì§€ì—­", ""):
            s += 0.3
        # í¬ë§ ì—°ë´‰ì´ íšŒì‚¬ ê¸‰ì—¬ ì •ë³´ì— í¬í•¨ë˜ë©´ ì ìˆ˜ ë¶€ì—¬ (ê°„ë‹¨í•œ ë¬¸ìì—´ ì¼ì¹˜)
        if salary and str(salary) in company.get("ê¸‰ì—¬", ""):
            s += 0.2
        return s

    # ì´ˆê¸° ìŠ¤ì½”ì–´ë§ì„ ê¸°ë°˜ìœ¼ë¡œ íšŒì‚¬ ëª©ë¡ì„ ì •ë ¬í•˜ì—¬ KoBERT ìœ ì‚¬ë„ ê³„ì‚°ì˜ í›„ë³´êµ°ìœ¼ë¡œ ì‚¬ìš©
    filtered_companies = sorted(cached_companies, key=score_company, reverse=True)
    
    # 3. KoBERT ìœ ì‚¬ë„ë¥¼ ì‚¬ìš©í•˜ì—¬ ìµœì¢… ìˆœìœ„ ê²°ì •
    # ì´ë¯¸ ì¶”ì²œëœ ê¸°ì—…ì„ ì œì™¸í•˜ê³  KoBERT ìœ ì‚¬ë„ ê³„ì‚°ì„ ìˆ˜í–‰
    # Kobert_similarity í•¨ìˆ˜ëŠ” ìœ ì‚¬ë„ì— ë”°ë¼ ì •ë ¬ëœ (íšŒì‚¬, ìœ ì‚¬ë„ ì ìˆ˜) íŠœí”Œ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    kobert_ranked_companies = kobert_similarity(user_text, filtered_companies)
    
    results = []
    for comp, sim in kobert_ranked_companies:
        # ìœ ì‚¬ë„ ì ìˆ˜ê°€ 0ë³´ë‹¤ í¬ê³ , ì´ë¯¸ ì¶”ì²œ ëª©ë¡ì— ì—†ëŠ” ê¸°ì—…ë§Œ ì¶”ê°€
        if sim > 0.0 and (comp.get("íšŒì‚¬ëª…"), comp.get("ì±„ìš©ê³µê³ ëª…")) not in shown:
            shown.add((comp.get("íšŒì‚¬ëª…"), comp.get("ì±„ìš©ê³µê³ ëª…"))) # ì¶”ì²œëœ ê¸°ì—…ì„ shown setì— ì¶”ê°€
            results.append((comp, sim))
        if len(results) >= top_n: # ìš”ì²­ëœ ê°œìˆ˜ë§Œí¼ ì¶”ì²œë˜ë©´ ì¤‘ë‹¨
            break
            
    return results

# --- Flask ë¼ìš°íŠ¸ ì„¤ì • ---
@app.route("/")
def index():
    """ë©”ì¸ í˜ì´ì§€ë¥¼ ë Œë”ë§í•©ë‹ˆë‹¤."""
    return render_template("index.html")

@app.route("/chat", methods=["POST"])
def chat():
    """ì‚¬ìš©ìì™€ì˜ ì±„íŒ… ìƒí˜¸ì‘ìš©ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤."""
    user_id = request.remote_addr # ì‚¬ìš©ì ì‹ë³„ (IP ì£¼ì†Œ ì‚¬ìš©)
    message = request.form.get("message", "").strip() # ì‚¬ìš©ì ë©”ì‹œì§€
    file = request.files.get("file") # ì²¨ë¶€ëœ íŒŒì¼ (PDF)
    state = user_states.get(user_id, {"shown": set(), "user_text": None, "interest": None, "region": None, "salary": None}) # ì‚¬ìš©ì ìƒíƒœ ê´€ë¦¬ ë”•ì…”ë„ˆë¦¬

    try:
        # 1. PDF íŒŒì¼ ì—…ë¡œë“œ ì²˜ë¦¬
        if file:
            user_text = extract_text_from_pdf(file)
            state["user_text"] = user_text
            user_states[user_id] = state # ìƒíƒœ ì—…ë°ì´íŠ¸
            return jsonify({"reply": "ê°ì‚¬í•©ë‹ˆë‹¤. ì´ë ¥ì„œ/ìê¸°ì†Œê°œì„œ ë‚´ìš©ì´ ì„±ê³µì ìœ¼ë¡œ ë¶„ì„ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ì œ **ê´€ì‹¬ ë¶„ì•¼, í¬ë§ ê·¼ë¬´ì§€, ì—°ë´‰**ì„ ì…ë ¥í•´ ì£¼ì„¸ìš”. (ì˜ˆì‹œ: í’ˆì§ˆ, ì„œìš¸, 3000ë§Œì› ë˜ëŠ” ì—†ìŒ, ì—†ìŒ, ì—†ìŒ)"})

        # 2. íŒŒì¼ ì—†ì´ ì‚¬ìš©ìê°€ ì´ë ¥ì„œ/ìê¸°ì†Œê°œì„œ ë‚´ìš©ì„ ì§ì ‘ ë©”ì‹œì§€ë¡œ ë³´ë‚¸ ê²½ìš°
        # user_textê°€ ì•„ì§ ì—†ê³ , ë©”ì‹œì§€ê°€ ì´ë ¥ì„œ/ìì†Œì„œë¡œ íŒë‹¨ë  ë§Œí¼ ê¸¸ê±°ë‚˜ íŠ¹ì • í‚¤ì›Œë“œë¥¼ í¬í•¨í•  ë•Œ
        if state["user_text"] is None and message:
            # ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±: ë©”ì‹œì§€ ê¸¸ì´ê°€ ê¸¸ê±°ë‚˜ "ì´ë ¥ì„œ", "ìê¸°ì†Œê°œì„œ" í‚¤ì›Œë“œ í¬í•¨ ì‹œ
            if len(message.split()) > 30 or "ì´ë ¥ì„œ" in message or "ìê¸°ì†Œê°œì„œ" in message:
                state["user_text"] = message
                user_states[user_id] = state
                return jsonify({"reply": "ì´ë ¥ì„œ/ìê¸°ì†Œê°œì„œ ë‚´ìš©ì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤. ì´ì œ **ê´€ì‹¬ ë¶„ì•¼, í¬ë§ ê·¼ë¬´ì§€, ì—°ë´‰**ì„ ì…ë ¥í•´ ì£¼ì„¸ìš”. (ì˜ˆì‹œ: í’ˆì§ˆ, ì„œìš¸, 3000ë§Œì› ë˜ëŠ” ì—†ìŒ, ì—†ìŒ, ì—†ìŒ)"})
            # ì¼ë°˜ ëŒ€í™”ë¡œ ê°„ì£¼
            else:
                return jsonify({"reply": "ê°œì¸ë³„ ë§ì¶¤ ë¶„ì„ì„ ìœ„í•´ì„œëŠ” ìê¸°ì†Œê°œì„œ í˜¹ì€ ì´ë ¥ì„œê°€ í•„ìš”í•©ë‹ˆë‹¤. íŒŒì¼ì„ ì²¨ë¶€í•´ ì£¼ì‹œê±°ë‚˜ ë‚´ìš©ì„ ì§ì ‘ ì…ë ¥í•´ ì£¼ì‹œë©´ ìƒì„¸í•œ ë¶„ì„ì„ ì œê³µí•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤."})


        # 3. ì‚¬ìš©ì ì„ í˜¸ë„ ì •ë³´ (ê´€ì‹¬ ë¶„ì•¼, í¬ë§ ê·¼ë¬´ì§€, ì—°ë´‰) ì…ë ¥ ì²˜ë¦¬
        # user_textê°€ ì´ë¯¸ ìˆê³ , ê´€ì‹¬ ë¶„ì•¼, ì§€ì—­, ì—°ë´‰ ì •ë³´ê°€ ì•„ì§ ì„¤ì •ë˜ì§€ ì•Šì•˜ì„ ë•Œ
        if state["user_text"] is not None and state["interest"] is None and "," in message:
            parts = [p.strip() for p in message.split(",")]
            state["interest"] = parts[0] if len(parts) > 0 and parts[0] != "ì—†ìŒ" else ""
            state["region"] = parts[1] if len(parts) > 1 and parts[1] != "ì—†ìŒ" else ""
            state["salary"] = parts[2].replace("ë§Œì›", "") if len(parts) > 2 and parts[2] != "ì—†ìŒ" else ""
            user_states[user_id] = state

            # ì„ í˜¸ë„ ì •ë³´ ì…ë ¥ì´ ì™„ë£Œë˜ë©´ ì¦‰ì‹œ ì¶”ì²œ ì‹œì‘ (ê¸°ë³¸ 3ê°œ)
            new_recommendations = make_recommendations(
                user_text=state["user_text"],
                interest=state.get("interest"),
                region=state.get("region"),
                salary=state.get("salary"),
                shown=state["shown"],
                top_n=3
            )

            if not new_recommendations:
                return jsonify({"reply": "ì•„ì‰½ê²Œë„ í˜„ì¬ ì¡°ê±´ì— ë§ëŠ” ìƒˆë¡œìš´ ê¸°ì—…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì¡°ê±´ì„ ë§ì”€í•´ì£¼ì‹œê² ì–´ìš”?"})

            explanations = []
            for company, score in new_recommendations:
                reason = generate_reason_individual(state["user_text"], company, score)
                explanations.append(f"**ê¸°ì—…ëª…**: {company.get('íšŒì‚¬ëª…', 'ì •ë³´ ì—†ìŒ')}\n**ì±„ìš©ê³µê³ ëª…**: {company.get('ì±„ìš©ê³µê³ ëª…', 'ì •ë³´ ì—†ìŒ')}\n**ìœ ì‚¬ë„ ì ìˆ˜**: {round(score,2)}\n**ì„¤ëª…**: {reason}\n")

            reply = "\n\n".join(explanations)
            reply += "\n\nğŸ“Œ ë” ê¶ê¸ˆí•œ ì ì´ë‚˜ ê³ ë ¤í•˜ê³  ì‹¶ì€ ì¡°ê±´ì´ ìˆë‹¤ë©´ ë§ì”€í•´ ì£¼ì„¸ìš”. ì¶”ê°€ë¡œ ë°˜ì˜í•´ ë“œë¦´ê²Œìš”! ì˜ˆë¥¼ ë“¤ì–´ 'ë” ì¶”ì²œí•´ì¤˜'ë¼ê³  ë§ì”€í•˜ì‹œë©´ ë‹¤ë¥¸ ê¸°ì—…ì„ ì°¾ì•„ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
            return jsonify({"reply": reply})

        # 4. ì¶”ê°€ ì¶”ì²œ ìš”ì²­ ì²˜ë¦¬ ("ë” ì¶”ì²œí•´ì¤˜" ë©”ì‹œì§€)
        # ì´ë¯¸ user_textì™€ ì„ í˜¸ë„ ì •ë³´ê°€ ì„¤ì •ë˜ì–´ ìˆê³ , ì‚¬ìš©ìê°€ ì¶”ê°€ ì¶”ì²œì„ ìš”ì²­í–ˆì„ ë•Œ
        if state["user_text"] is not None and state["interest"] is not None and "ë” ì¶”ì²œí•´ì¤˜" in message:
            new_recommendations = make_recommendations(
                user_text=state["user_text"],
                interest=state.get("interest"),
                region=state.get("region"),
                salary=state.get("salary"),
                shown=state["shown"],
                top_n=1 # í•œ ë²ˆì— í•˜ë‚˜ì”©ë§Œ ì¶”ê°€ ì¶”ì²œ
            )

            if not new_recommendations:
                return jsonify({"reply": "ë” ì´ìƒ ì¶”ì²œí•  ê¸°ì—…ì´ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì¡°ê±´ì„ ë§ì”€í•´ì£¼ì‹œê² ì–´ìš”?"})

            explanations = []
            for company, score in new_recommendations:
                reason = generate_reason_individual(state["user_text"], company, score)
                explanations.append(f"**ê¸°ì—…ëª…**: {company.get('íšŒì‚¬ëª…', 'ì •ë³´ ì—†ìŒ')}\n**ì±„ìš©ê³µê³ ëª…**: {company.get('ì±„ìš©ê³µê³ ëª…', 'ì •ë³´ ì—†ìŒ')}\n**ìœ ì‚¬ë„ ì ìˆ˜**: {round(score,2)}\n**ì„¤ëª…**: {reason}\n")

            reply = "\n\n".join(explanations)
            reply += "\n\nğŸ“Œ ë” ê¶ê¸ˆí•œ ì ì´ë‚˜ ê³ ë ¤í•˜ê³  ì‹¶ì€ ì¡°ê±´ì´ ìˆë‹¤ë©´ ë§ì”€í•´ ì£¼ì„¸ìš”. ì¶”ê°€ë¡œ ë°˜ì˜í•´ ë“œë¦´ê²Œìš”! ë˜ëŠ” 'ì¶”ì²œ ì´ˆê¸°í™”'ë¼ê³  ë§ì”€í•˜ì‹œë©´ ì²˜ìŒë¶€í„° ë‹¤ì‹œ ì‹œì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
            return jsonify({"reply": reply})
        
        # 5. ì¶”ì²œ ì´ˆê¸°í™” ìš”ì²­ (ìƒˆë¡œìš´ PDF/ìì†Œì„œë¡œ ì‹œì‘)
        if "ì¶”ì²œ ì´ˆê¸°í™”" in message:
            user_states[user_id] = {"shown": set(), "user_text": None, "interest": None, "region": None, "salary": None}
            return jsonify({"reply": "ì¶”ì²œ ìƒíƒœê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤. ìƒˆë¡œìš´ ìê¸°ì†Œê°œì„œ/ì´ë ¥ì„œ íŒŒì¼ì„ ì²¨ë¶€í•˜ì‹œê±°ë‚˜ ë‚´ìš©ì„ ì§ì ‘ ì…ë ¥í•´ ì£¼ì„¸ìš”."})


        # ëª¨ë“  ì¡°ê±´ì— í•´ë‹¹í•˜ì§€ ì•Šì„ ê²½ìš°
        return jsonify({"reply": "ë¬´ìŠ¨ ë§ì”€ì´ì‹ ì§€ ì •í™•íˆ ì´í•´í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ìê¸°ì†Œê°œì„œ/ì´ë ¥ì„œë¥¼ ì²¨ë¶€í•´ ì£¼ì‹œê±°ë‚˜, 'ì¶”ì²œ ì´ˆê¸°í™”'ë¥¼ í†µí•´ ë‹¤ì‹œ ì‹œì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."})

    except Exception as e:
        print(f"âŒ ì„œë²„ ì—ëŸ¬: {e}")
        # ì˜ˆì™¸ ë°œìƒ ì‹œ ì‚¬ìš©ìì—ê²Œ ì˜¤ë¥˜ ë©”ì‹œì§€ ë°˜í™˜
        return jsonify({"reply": f"âŒ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)} ë¶ˆí¸ì„ ë“œë ¤ ì£„ì†¡í•©ë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”."}), 500

if __name__ == "__main__":
    app.run(debug=True, host="0.0.0.0", port=8080)
