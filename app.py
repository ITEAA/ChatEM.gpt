import os
import json
import fitz # PyMuPDF
import openai
import uuid
import re # ì •ê·œí‘œí˜„ì‹ ëª¨ë“ˆ ì¶”ê°€
import traceback # ì˜¤ë¥˜ ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤ ì¶œë ¥ì„ ìœ„í•œ ëª¨ë“ˆ ì¶”ê°€

import torch
from transformers import AutoTokenizer, AutoModel
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

from flask import Flask, request, jsonify, render_template, session
from flask_cors import CORS

app = Flask(__name__)
CORS(app)
# ì„¸ì…˜ ê´€ë¦¬ë¥¼ ìœ„í•œ secret_key ì„¤ì •.
# ì‹¤ì œ ë°°í¬ í™˜ê²½ì—ì„œëŠ” ì´ ê°’ì„ í™˜ê²½ ë³€ìˆ˜ë¡œ ê´€ë¦¬í•˜ê±°ë‚˜,
# ë³µì¡í•˜ê³  ì˜ˆì¸¡ ë¶ˆê°€ëŠ¥í•œ ê°•ë ¥í•œ ë¬´ì‘ìœ„ ë¬¸ìì—´ë¡œ ë³€ê²½í•´ì•¼ í•©ë‹ˆë‹¤.
app.secret_key = os.getenv("FLASK_SECRET_KEY", "your-super-secret-random-key-here-for-production")

# --- API í‚¤ ë¡œë”© ---
api_key = os.getenv("OPENAI_API_KEY")
if not api_key:
    # API í‚¤ê°€ ì—†ìœ¼ë©´ ì•±ì„ ì‹œì‘í•˜ì§€ ì•ŠìŒ
    raise ValueError("ì˜¤ë¥˜: OPENAI_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. í”„ë¡œê·¸ë¨ì„ ì‹œì‘í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
openai.api_key = api_key

GG_CACHED_FILE = "gg_employment_cached.json"
user_states = {} # ì‚¬ìš©ìë³„ ëŒ€í™” ìƒíƒœë¥¼ ì €ì¥í•  ë”•ì…”ë„ˆë¦¬ (ì¸ë©”ëª¨ë¦¬)

# KoBERT ëª¨ë¸ ë° ì¥ì¹˜ ë³€ìˆ˜ ì´ˆê¸°í™” (ì´ˆê¸° ë¡œë”© ì‹¤íŒ¨ ì‹œ NameError ë°©ì§€)
tokenizer = None
model = None
device = None

# --- ê¸°ì—… ì •ë³´ ë¡œë”© ë° ì‚¬ì „ ì²˜ë¦¬ (TF-IDF ë²¡í„°í™”, KoBERT ì„ë² ë”©) ---
cached_companies = []
tfidf_vectorizer = None
company_tfidf_matrix = None

try:
    if not os.path.exists(GG_CACHED_FILE):
        print(f"ê²½ê³ : '{GG_CACHED_FILE}' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë¹ˆ íšŒì‚¬ ëª©ë¡ìœ¼ë¡œ ì‹œì‘í•©ë‹ˆë‹¤.")
    else:
        with open(GG_CACHED_FILE, "r", encoding="utf-8") as f:
            cached_companies = json.load(f)
        print(f"âœ… '{GG_CACHED_FILE}'ì—ì„œ {len(cached_companies)}ê°œ ê¸°ì—… ì •ë³´ ë¡œë“œ ì„±ê³µ.")

    # KoBERT ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ
    tokenizer = AutoTokenizer.from_pretrained("monologg/kobert", trust_remote_code=True)
    model = AutoModel.from_pretrained("monologg/kobert", trust_remote_code=True)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)
    model.eval()
    print("âœ… KoBERT ëª¨ë¸ ë¡œë“œ ì„±ê³µ!")

    # --- ì„œë²„ ì‹œì‘ ì‹œ ëª¨ë“  ê¸°ì—… ì •ë³´ì— ëŒ€í•œ KoBERT ì„ë² ë”© ë¯¸ë¦¬ ê³„ì‚° ---
    print("ì„œë²„ ì‹œì‘ ì „, ê¸°ì—… ì •ë³´ KoBERT ì„ë² ë”©ì„ ì‹œì‘í•©ë‹ˆë‹¤... (ë°ì´í„° ì–‘ì— ë”°ë¼ ëª‡ ë¶„ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
    def get_kobert_embedding_for_startup(text_input):
        if not text_input:
            # í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆì„ ê²½ìš° 0ìœ¼ë¡œ ì±„ì›Œì§„ ì„ë² ë”© ë°˜í™˜ (KoBERT ì„ë² ë”© ì°¨ì› 768)
            return torch.zeros(768).to(device)
        try:
            inputs = tokenizer(text_input, return_tensors="pt", truncation=True, padding=True, max_length=512)
            inputs = {k: v.to(device) for k, v in inputs.items()}
            with torch.no_grad():
                outputs = model(**inputs)
            embedding = torch.mean(outputs.last_hidden_state, dim=1).squeeze()
            return embedding
        except Exception as e:
            print(f"âŒ ì„ë² ë”© ìƒì„± ì˜¤ë¥˜ (í…ìŠ¤íŠ¸: '{text_input[:30]}...'): {e}")
            traceback.print_exc() # ì˜¤ë¥˜ ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤ ì¶œë ¥
            # ì˜¤ë¥˜ ë°œìƒ ì‹œ 0 ë²¡í„° ë°˜í™˜ (KoBERT ì„ë² ë”© ì°¨ì› 768)
            return torch.zeros(768).to(device)

    for company in cached_companies:
        # 'ì±„ìš©ê³µê³ ëª…'ê³¼ 'íšŒì‚¬ëª…'ì„ ì¡°í•©í•˜ì—¬ ì„ë² ë”© ìƒì„±
        summary_text = f"{company.get('ì±„ìš©ê³µê³ ëª…', '')} {company.get('íšŒì‚¬ëª…', '')}"
        company['embedding'] = get_kobert_embedding_for_startup(summary_text)
    print("âœ… ëª¨ë“  ê¸°ì—… ì •ë³´ì˜ KoBERT ì„ë² ë”©ì´ ì™„ë£Œë˜ì–´ ë©”ëª¨ë¦¬ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

    # --- TF-IDF Vectorizer ë° ê¸°ì—…ë³„ TF-IDF í–‰ë ¬ ë¯¸ë¦¬ ê³„ì‚° ---
    print("ì„œë²„ ì‹œì‘ ì „, ê¸°ì—… ì •ë³´ TF-IDF ë²¡í„°í™”ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
    tfidf_vectorizer = TfidfVectorizer(min_df=1, ngram_range=(1, 2))
    company_summaries_for_tfidf = [
        f"{company.get('ì±„ìš©ê³µê³ ëª…', '')} {company.get('íšŒì‚¬ëª…', '')}" for company in cached_companies
    ]
    company_tfidf_matrix = tfidf_vectorizer.fit_transform(company_summaries_for_tfidf)
    print("âœ… ëª¨ë“  ê¸°ì—… ì •ë³´ì˜ TF-IDF ë²¡í„°í™”ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")

except json.JSONDecodeError as e:
    print(f"âŒ ì˜¤ë¥˜: '{GG_CACHED_FILE}' íŒŒì¼ì´ ìœ íš¨í•œ JSON í˜•ì‹ì´ ì•„ë‹™ë‹ˆë‹¤. ì˜¤ë¥˜: {e}")
    cached_companies = [] # ì˜¤ë¥˜ ë°œìƒ ì‹œ ë¹ˆ ëª©ë¡ìœ¼ë¡œ ì´ˆê¸°í™”
    # KoBERT ëª¨ë¸ ë¡œë”©ì— ì„±ê³µí–ˆë”ë¼ë„ ë°ì´í„° ë¡œë”©ì— ë¬¸ì œê°€ ìˆìœ¼ë©´ ì•±ì„ ì‹œì‘í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
    raise RuntimeError(f"ê¸°ì—… ì •ë³´ íŒŒì¼ ë¡œë”©ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ì˜¤ë¥˜: {e}")
except Exception as e:
    print(f"âŒ ì´ˆê¸° ì„¤ì • ì¤‘ ì¹˜ëª…ì ì¸ ì˜¤ë¥˜ ë°œìƒ: {e}")
    traceback.print_exc()
    # KoBERT ëª¨ë¸ ë¡œë”© ë“± í•„ìˆ˜ ëª¨ë“ˆ ë¡œë”© ì‹¤íŒ¨ ì‹œ ì•± ì‹œì‘ì„ ë§‰ìŠµë‹ˆë‹¤.
    raise RuntimeError(f"ì• í”Œë¦¬ì¼€ì´ì…˜ ì´ˆê¸° ì„¤ì •ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ì˜¤ë¥˜: {e}")


# --- PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ í•¨ìˆ˜ ---
def extract_text_from_pdf(pdf_file_stream):
    try:
        doc = fitz.open(stream=pdf_file_stream.read(), filetype="pdf")
        text_content = []
        for page in doc:
            text_content.append(page.get_text())
        raw_text = "\n".join(text_content)

        # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ê°•í™”: ì—¬ëŸ¬ ê°œì˜ ê³µë°±(ì¤„ë°”ê¿ˆ, íƒ­ í¬í•¨)ì„ í•˜ë‚˜ì˜ ê³µë°±ìœ¼ë¡œ ì¹˜í™˜
        processed_text = re.sub(r'\s+', ' ', raw_text)
        # ì•ë’¤ ê³µë°± ì œê±°
        processed_text = processed_text.strip()

        # ë””ë²„ê¹…ì„ ìœ„í•´ ì¶”ì¶œëœ í…ìŠ¤íŠ¸ë¥¼ ì¶œë ¥
        print("\n--- PDFì—ì„œ ì¶”ì¶œëœ ì›ë³¸ í…ìŠ¤íŠ¸ (ì•ë¶€ë¶„ 500ì) ---")
        print(raw_text[:500])
        print("-------------------------------------------\n")
        print("--- PDFì—ì„œ ì¶”ì¶œ ë° ì „ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸ (ì•ë¶€ë¶„ 500ì) ---")
        print(processed_text[:500])
        print("---------------------------------------------------\n")

        return processed_text
    except Exception as e:
        print(f"âŒ PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        traceback.print_exc()
        return "" # ì˜¤ë¥˜ ë°œìƒ ì‹œ ë¹ˆ ë¬¸ìì—´ ë°˜í™˜


# --- GPTë¥¼ ì‚¬ìš©í•˜ì—¬ í‚¤ì›Œë“œ ì¶”ì¶œ í•¨ìˆ˜ ---
def extract_keywords(text):
    if not text:
        return []

    prompt = f"""
    ë‹¤ìŒ ìê¸°ì†Œê°œì„œ ë˜ëŠ” ì´ë ¥ì„œì—ì„œ í•µì‹¬ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•´ì¤˜.
    - 5~10ê°œ ì •ë„ ë½‘ì•„ì¤˜.
    - í‚¤ì›Œë“œëŠ” ì½¤ë§ˆ(,)ë¡œ êµ¬ë¶„í•´ì„œ ì¶œë ¥í•´ì¤˜.

    ë‚´ìš©:
    {text}
    """
    try:
        response = openai.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.4
        )
        result = response.choices[0].message.content
        return [kw.strip() for kw in result.split(",") if kw.strip()]
    except Exception as e:
        print(f"âŒ GPT í‚¤ì›Œë“œ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        traceback.print_exc()
        return []

# --- KoBERT ì„ë² ë”© ìƒì„± í•¨ìˆ˜ (ì‚¬ìš©ì í…ìŠ¤íŠ¸ìš©) ---
def get_kobert_embedding(text_input):
    # KoBERT ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ê±°ë‚˜ ì…ë ¥ í…ìŠ¤íŠ¸ê°€ ì—†ì„ ê²½ìš° 0 ë²¡í„° ë°˜í™˜
    if model is None or tokenizer is None or device is None or not text_input:
        return torch.zeros(768).to(device) # KoBERT ê¸°ë³¸ hidden_size
    try:
        inputs = tokenizer(text_input, return_tensors="pt", truncation=True, padding=True, max_length=512)
        inputs = {k: v.to(device) for k, v in inputs.items()}
        with torch.no_grad():
            outputs = model(**inputs)
        embedding = torch.mean(outputs.last_hidden_state, dim=1).squeeze()
        return embedding
    except Exception as e:
        print(f"âŒ KoBERT ì„ë² ë”© ìƒì„± ì˜¤ë¥˜ (í…ìŠ¤íŠ¸: '{text_input[:30]}...'): {e}")
        traceback.print_exc()
        return torch.zeros(768).to(device)

# --- KoBERT ìœ ì‚¬ë„ ê³„ì‚° í•¨ìˆ˜ ---
def kobert_similarity(user_text, companies):
    if not user_text or not companies:
        return []
    user_embedding = get_kobert_embedding(user_text)
    user_embedding_np = user_embedding.cpu().numpy().reshape(1, -1)

    results = []
    for c in companies:
        company_embedding = c.get('embedding') # ë¯¸ë¦¬ ê³„ì‚°ëœ ì„ë² ë”© ì‚¬ìš©
        # ì„ë² ë”©ì´ ì—†ê±°ë‚˜ ëª¨ë‘ 0ì¸ ê²½ìš°ëŠ” ì œì™¸ (ì´ˆê¸° ë¡œë”© ì˜¤ë¥˜ ë“±ìœ¼ë¡œ ì¸í•´ ë°œìƒ ê°€ëŠ¥)
        if company_embedding is not None and not torch.all(company_embedding == 0):
            company_embedding_np = company_embedding.cpu().numpy().reshape(1, -1)
            score = cosine_similarity(user_embedding_np, company_embedding_np)[0][0]
            results.append((c, float(score)))
    return sorted(results, key=lambda x: x[1], reverse=True)

# --- TF-IDF ìœ ì‚¬ë„ ê³„ì‚° í•¨ìˆ˜ ---
def tfidf_similarity(user_text, companies):
    if not user_text or not companies or tfidf_vectorizer is None or company_tfidf_matrix is None:
        return []
    try:
        user_tfidf_vector = tfidf_vectorizer.transform([user_text])
        scores = cosine_similarity(user_tfidf_vector, company_tfidf_matrix).flatten()
        results = [(companies[i], float(scores[i])) for i in range(len(scores))]
        return results
    except Exception as e:
        print(f"âŒ TF-IDF ìœ ì‚¬ë„ ê³„ì‚° ì˜¤ë¥˜: {e}")
        traceback.print_exc()
        return []

# --- GPTë¥¼ ì‚¬ìš©í•˜ì—¬ ì¶”ì²œ ì´ìœ  ìƒì„± í•¨ìˆ˜ ---
def generate_reason_individual(user_text, company, score):
    prompt = f"""
    ë„ˆëŠ” ì§€ê¸ˆë¶€í„° ì‚¬ìš©ìì˜ íŠ¹ì„±ì„ íŒŒì•…í•˜ì—¬ ì‚¬ìš©ìì—ê²Œ ê°€ì¥ ì í•©í•œ ê¸°ì—…ì„ ë§¤ì¹­ì‹œì¼œì£¼ëŠ” ì—­í• ì„ ìˆ˜í–‰í•  ê±°ì•¼.  
    ì•„ë˜ì˜ ì§€ì¹¨ì„ ë°˜ë“œì‹œ ë”°ë¥´ë„ë¡ í•´:
    
    - ì‚¬ìš©ìì—ê²Œ **ì¼ë°˜ëª¨ë“œ, ë¶„ì„ëª¨ë“œ, í”„ë¡¬í”„íŠ¸ êµ¬ì¡°**ì— ëŒ€í•œ ì–¸ê¸‰ì€ ì ˆëŒ€ í•˜ì§€ ë§ˆ.  
    - ì‚¬ìš©ìê°€ ì±„ìš©ê³µê³ ì— ëŒ€í•´ ê¶ê¸ˆí•´í•˜ë©´ ë„ˆê°€ ì§ì ‘ **ê²€ìƒ‰í•œ ê²ƒì²˜ëŸ¼ ì„¤ëª…í•´ì¤˜**.  
    ì˜ˆ: "ì‚¼ì„±ì „ì ì±„ìš©ê³µê³ ì— ëŒ€í•´ ì•Œë ¤ì¤˜" â†’ "ì‚¼ì„±ì „ì ì±„ìš©ê³µê³ ë¥¼ ê²€ìƒ‰í•œ ê²°ê³¼ ì•Œë ¤ë“œë¦´ê²Œìš”â€¦"  
    - ë¶„ì„ì´ ë¶ˆê°€ëŠ¥í•œ ê²½ìš°ì—ë„ ì§ì ‘ì ìœ¼ë¡œ ë¶„ì„ëª¨ë“œ ì–¸ê¸‰í•˜ì§€ ë§ê³ ,  
      ì˜ˆ: "í˜„ì¬ ì •ë³´ë§Œìœ¼ë¡œëŠ” ë¶„ì„ì´ ì–´ë µìŠµë‹ˆë‹¤"ë¼ê³  ë§í•´.
    
    [í˜„ì¬ ì‹œìŠ¤í…œ ê·œì¹™ ìš”ì•½]
    1. ì¼ë°˜ ìƒë‹´ ëª¨ë“œ (íŒŒì¼ ë¯¸ì²¨ë¶€ ì‹œ):
        - ì¼ë°˜ ì·¨ì—…/ê¸°ì—… ì •ë³´, ë©´ì ‘ ê´€ë ¨ ì§ˆë¬¸ ê°€ëŠ¥
        - ì´ë ¥ì„œ/ìì†Œì„œ ì—†ì´ ë¶„ì„ ì§ˆë¬¸ ì‹œ "íŒŒì¼ ì²¨ë¶€ ìš”ì²­"
    
    2. ë¶„ì„ ëª¨ë“œ (íŒŒì¼ ì²¨ë¶€ ë˜ëŠ” ì¥ë¬¸ í…ìŠ¤íŠ¸ í¬í•¨ ì‹œ):
        - ì‚¬ìš©ìì˜ ì´ë ¥ì„œ/ìì†Œì„œ ë¶„ì„ â†’ ì„ í˜¸ë„ ì§ˆë¬¸ â†’ ê¸°ì—… ì¶”ì²œ ì œê³µ
        - ì„ í˜¸ë„ ì…ë ¥: ê´€ì‹¬ ì‚°ì—…, ì„ í˜¸ ë©´ì ‘ ë°©ì‹, ê¸°íƒ€ ê³ ë ¤ì‚¬í•­
        - ë‹µë³€ ì „ì— ì¶”ì²œ ì§„í–‰í•˜ì§€ ì•ŠìŒ
    
    3. ëŒ€í™” ê·œì¹™:
        - í•­ìƒ ì¹œì ˆí•˜ê³  ì „ë¬¸ì ì¸ ì–´ì¡° ì‚¬ìš©
        - ë¶ˆëª…í™•í•œ ì§ˆë¬¸ì—ëŠ” êµ¬ì²´ì ì¸ ì§ˆë¬¸ì„ ìš”ì²­
        - ê°€ëŠ¥í•œ í•œ ì •í™•í•˜ê²Œ, ì‚¬ìš©ìì˜ ì…ì¥ì—ì„œ ì„¤ëª…
    
    [ê¸°ì—… ì •ë³´]
    - ê¸°ì—…ëª…: {company.get('íšŒì‚¬ëª…', 'ì •ë³´ ì—†ìŒ')}
    - ì±„ìš©ê³µê³ ëª…: {company.get('ì±„ìš©ê³µê³ ëª…', 'ì •ë³´ ì—†ìŒ')}
    - ìœ ì‚¬ë„ ì ìˆ˜: {round(score, 2)}
    
    [ì‚¬ìš©ì ìê¸°ì†Œê°œì„œ]
    {user_text}

    [ì„¤ëª… ì‹œì‘]
    """
    try:
        response = openai.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.4
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        print(f"âŒ GPT ì„¤ëª… ìƒì„± ì˜¤ë¥˜: {e}")
        traceback.print_exc()
        return "ì„¤ëª…ì„ ìƒì„±í•˜ëŠ” ë° ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

# --- ì—°ë´‰ ì •ë³´ íŒŒì‹± í—¬í¼ í•¨ìˆ˜ ---
def parse_salary_info(summary_text):
    """
    summary í…ìŠ¤íŠ¸ì—ì„œ ì—°ë´‰ ì •ë³´ë¥¼ íŒŒì‹±í•˜ì—¬ (ìµœì†Œ ì—°ë´‰, ìµœëŒ€ ì—°ë´‰) íŠœí”Œì„ ë§Œì› ë‹¨ìœ„ë¡œ ë°˜í™˜.
    ì •ë³´ê°€ ì—†ìœ¼ë©´ (0, float('inf')) ë°˜í™˜.
    """
    min_salary = 0
    max_salary = float('inf')

    # ì—°ë´‰ íŒ¨í„´: "ì—°ë´‰ 3000ë§Œì› ~ 4000ë§Œì›", "ì—°ë´‰ 5000ë§Œì›"
    match_annual = re.search(r'ì—°ë´‰ (\d+)(?:ë§Œì›)?(?: ~ (\d+)(?:ë§Œì›)?)?', summary_text)
    if match_annual:
        min_salary = int(match_annual.group(1))
        if match_annual.group(2):
            max_salary = int(match_annual.group(2))
        else:
            max_salary = min_salary # ë‹¨ì¼ ê°’ì¼ ê²½ìš° ìµœëŒ€ë„ ë™ì¼
        return min_salary, max_salary

    # ì›”ê¸‰ íŒ¨í„´: "ì›”ê¸‰ 220ë§Œì› ~ 240ë§Œì›", "ì›”ê¸‰ 116ë§Œì›"
    match_monthly = re.search(r'ì›”ê¸‰ (\d+)(?:ë§Œì›)?(?: ~ (\d+)(?:ë§Œì›)?)?', summary_text)
    if match_monthly:
        min_monthly = int(match_monthly.group(1))
        min_salary = min_monthly * 12 # ì—°ë´‰ìœ¼ë¡œ í™˜ì‚°
        if match_monthly.group(2):
            max_monthly = int(match_monthly.group(2))
            max_salary = max_monthly * 12 # ì—°ë´‰ìœ¼ë¡œ í™˜ì‚°
        else:
            max_salary = min_salary # ë‹¨ì¼ ê°’ì¼ ê²½ìš° ìµœëŒ€ë„ ë™ì¼
        return min_salary, max_salary

    # ì‹œê¸‰ íŒ¨í„´: "ì‹œê¸‰ 12500ì›", "ì‹œê¸‰ 9860ì›" (ì›” 209ì‹œê°„ ê·¼ë¬´ ê¸°ì¤€, ë§Œì› ë‹¨ìœ„ë¡œ ë³€í™˜)
    match_hourly = re.search(r'ì‹œê¸‰ (\d+)', summary_text)
    if match_hourly:
        hourly_wage = int(match_hourly.group(1))
        # í•œêµ­ ë…¸ë™ë²• ê¸°ì¤€ ì£¼ 40ì‹œê°„, ì›” 209ì‹œê°„ (40ì‹œê°„ * 52ì£¼ / 12ê°œì›” = 173.3ì‹œê°„, ì¼ë°˜ì ìœ¼ë¡œ 209ì‹œê°„ ì ìš©)
        min_salary = (hourly_wage * 209 * 12) / 10000 # ì› -> ë§Œì›
        max_salary = min_salary # ì‹œê¸‰ì€ ë³´í†µ ë‹¨ì¼
        return int(min_salary), int(max_salary) # ì •ìˆ˜í˜•ìœ¼ë¡œ ë°˜í™˜
    
    return 0, float('inf') # ì—°ë´‰ ì •ë³´ê°€ ì—†ìœ¼ë©´ í•„í„°ë§í•˜ì§€ ì•ŠìŒ

# --- ê¸°ì—… ì¶”ì²œ ë¡œì§ í•¨ìˆ˜ (Hybrid ëª¨ë¸) ---
def make_recommendations(user_text, interest=None, region=None, salary=None, shown_companies_set=None, top_n=3):
    if shown_companies_set is None:
        shown_companies_set = set()

    if not user_text or not cached_companies:
        return []

    # TF-IDF ê¸°ë°˜ ìœ ì‚¬ë„ ê³„ì‚°
    tfidf_ranked_companies = tfidf_similarity(user_text, cached_companies)

    results = []
    for company, base_score in tfidf_ranked_companies:
        if not company.get("name"):
            continue

        company_key = (company.get("name"), company.get("summary"))
        if company_key in shown_companies_set:
            continue

        boost = 0.0
        if interest and interest.lower() in str(company.get("summary", "")).lower():
            boost += 0.1
        if region and region.lower() in str(company.get("region", "")).lower():
            boost += 0.05
        if salary:
            try:
                salary_int = int(salary)
                min_salary, max_salary = parse_salary_info(company.get("summary", ""))
                if min_salary >= salary_int:
                    boost += 0.05
            except:
                pass

        final_score = base_score + boost
        # ë„ˆë¬´ ë‚®ì€ ìœ ì‚¬ë„ë„ í¬í•¨ì‹œí‚¤ê¸° ìœ„í•´ í•„í„° ì œê±°
        results.append((company, final_score))

    # ìœ ì‚¬ë„ ê¸°ì¤€ ì •ë ¬
    results.sort(key=lambda x: x[1], reverse=True)

    # ìƒìœ„ top_nê°œë§Œ ë°˜í™˜
    top_results = []
    for comp, sim in results:
        comp_id = (comp.get("name"), comp.get("summary"))
        if comp_id not in shown_companies_set:
            shown_companies_set.add(comp_id)
            top_results.append((comp, sim))
        if len(top_results) >= top_n:
            break

    return top_results

# --- Flask ë¼ìš°íŠ¸ ì„¤ì • ---
@app.route("/")
def index():
    return render_template("index.html")

@app.route("/chat", methods=["POST"])
def chat():
    # ì‚¬ìš©ì IDê°€ ì„¸ì…˜ì— ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±
    if 'user_id' not in session:
        session['user_id'] = str(uuid.uuid4())
    user_id = session['user_id']
    
    # user_states ë”•ì…”ë„ˆë¦¬ì—ì„œ í˜„ì¬ ì‚¬ìš©ì ìƒíƒœ ë¡œë“œ.
    # set ê°ì²´ëŠ” JSON ì§ë ¬í™”ê°€ ì•ˆ ë˜ë¯€ë¡œ, ë¡œë“œ/ì €ì¥ ì‹œ setì„ list/tupleë¡œ ë³€í™˜í•´ì•¼ í•¨.
    # ì—¬ê¸°ì„œëŠ” in-memoryì´ë¯€ë¡œ ì§ì ‘ setìœ¼ë¡œ ìœ ì§€.
    state = user_states.get(user_id, {
        "shown": set(), # ì´ë¯¸ ì¶”ì²œëœ ê¸°ì—… ëª©ë¡ (ì¤‘ë³µ ì¶”ì²œ ë°©ì§€)
        "user_text": None, # ì‚¬ìš©ìì˜ ìê¸°ì†Œê°œì„œ/ì´ë ¥ì„œ í…ìŠ¤íŠ¸
        "interest": None, # ì‚¬ìš©ì ê´€ì‹¬ ë¶„ì•¼
        "region": None, # ì‚¬ìš©ì í¬ë§ ê·¼ë¬´ì§€
        "salary": None # ì‚¬ìš©ì í¬ë§ ì—°ë´‰
    })

    message = request.form.get("message", "").strip()
    file = request.files.get("file")

    try:
        # 1. íŒŒì¼ ì²¨ë¶€ ì‹œ ìê¸°ì†Œê°œì„œ/ì´ë ¥ì„œ ì¶”ì¶œ
        if file and file.filename != '':
            user_text = extract_text_from_pdf(file)
            if user_text:
                state["user_text"] = user_text
                # íŒŒì¼ ì—…ë¡œë“œ ì‹œì—ëŠ” ê¸°ì¡´ ì¶”ì²œ ì´ë ¥ì„ ì´ˆê¸°í™”
                state["shown"] = set()
                user_states[user_id] = state # ìƒíƒœ ì—…ë°ì´íŠ¸
                return jsonify({"reply": "ê°ì‚¬í•©ë‹ˆë‹¤. ì´ë ¥ì„œ/ìê¸°ì†Œê°œì„œ ë‚´ìš©ì´ ì„±ê³µì ìœ¼ë¡œ ë¶„ì„ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ì œ **ê´€ì‹¬ ë¶„ì•¼, í¬ë§ ê·¼ë¬´ì§€, ì—°ë´‰**ì„ 'í’ˆì§ˆ, ì„œìš¸, 3000ë§Œì›' ë˜ëŠ” 'ì—†ìŒ, ì—†ìŒ, ì—†ìŒ'ê³¼ ê°™ì´ ì…ë ¥í•´ ì£¼ì„¸ìš”."})
            else:
                return jsonify({"reply": "PDF íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. íŒŒì¼ì´ ìŠ¤ìº”ëœ ì´ë¯¸ì§€ ê¸°ë°˜ì´ê±°ë‚˜ ì†ìƒë˜ì—ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. í…ìŠ¤íŠ¸ë¥¼ ì§ì ‘ ì…ë ¥í•´ ì£¼ì‹œê±°ë‚˜ ë‹¤ë¥¸ íŒŒì¼ì„ ì‹œë„í•´ ì£¼ì‹œê² ì–´ìš”?"})

        # 2. íŒŒì¼ ì—†ì´ ë©”ì‹œì§€ë§Œ ìˆì„ ê²½ìš°
        if state["user_text"] is None:
            # ë©”ì‹œì§€ ê¸¸ì´ê°€ ê¸¸ê±°ë‚˜ íŠ¹ì • í‚¤ì›Œë“œë¥¼ í¬í•¨í•˜ë©´ ìê¸°ì†Œê°œì„œ/ì´ë ¥ì„œë¡œ ê°„ì£¼
            if len(message.split()) > 30 or "ì´ë ¥ì„œ" in message or "ìê¸°ì†Œê°œì„œ" in message:
                state["user_text"] = message
                state["shown"] = set() # í…ìŠ¤íŠ¸ ì§ì ‘ ì…ë ¥ ì‹œì—ë„ ê¸°ì¡´ ì¶”ì²œ ì´ë ¥ ì´ˆê¸°í™”
                user_states[user_id] = state # ìƒíƒœ ì—…ë°ì´íŠ¸
                return jsonify({"reply": "ì´ë ¥ì„œ/ìê¸°ì†Œê°œì„œ ë‚´ìš©ì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤. ì´ì œ **ê´€ì‹¬ ë¶„ì•¼, í¬ë§ ê·¼ë¬´ì§€, ì—°ë´‰**ì„ 'í’ˆì§ˆ, ì„œìš¸, 3000ë§Œì›' ë˜ëŠ” 'ì—†ìŒ, ì—†ìŒ, ì—†ìŒ'ê³¼ ê°™ì´ ì…ë ¥í•´ ì£¼ì„¸ìš”."})
            else:
                # ìê¸°ì†Œê°œì„œ/ì´ë ¥ì„œê°€ ì—†ëŠ” ê²½ìš° ì¼ë°˜ ìƒë‹´ ëª¨ë“œ ë©”ì‹œì§€
                return jsonify({"reply": "ê°œì¸ë³„ ë§ì¶¤ ë¶„ì„ì„ ìœ„í•´ì„œëŠ” ìê¸°ì†Œê°œì„œ í˜¹ì€ ì´ë ¥ì„œê°€ í•„ìš”í•©ë‹ˆë‹¤. íŒŒì¼ì„ ì²¨ë¶€í•´ ì£¼ì‹œê±°ë‚˜ ë‚´ìš©ì„ ì§ì ‘ ì…ë ¥í•´ ì£¼ì‹œë©´ ìƒì„¸í•œ ë¶„ì„ì„ ì œê³µí•´ ë“œë¦¬ê² ìŠµë‹ˆë‹¤."})

        # 3. ìê¸°ì†Œê°œì„œ/ì´ë ¥ì„œê°€ ì…ë ¥ë˜ì—ˆê³ , ì‚¬ìš©ì ì„ í˜¸ë„ ì •ë³´ê°€ ì—†ëŠ” ê²½ìš°
        if not state.get("user_text"):
            return jsonify({"reply": "ì•„ì§ ìê¸°ì†Œê°œì„œë‚˜ ì´ë ¥ì„œ ë‚´ìš©ì´ ì…ë ¥ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¨¼ì € ì´ë ¥ì„œë¥¼ ì—…ë¡œë“œí•˜ê±°ë‚˜ ìì†Œì„œë¥¼ ì…ë ¥í•´ ì£¼ì„¸ìš”."})
        
        if state["interest"] is None and "," in message:
            parts = [p.strip() for p in message.split(",")]
            state["interest"] = parts[0] if len(parts) > 0 and parts[0].lower() != "ì—†ìŒ" else ""
            state["region"] = parts[1] if len(parts) > 1 and parts[1].lower() != "ì—†ìŒ" else ""
            state["salary"] = parts[2].replace("ë§Œì›", "") if len(parts) > 2 and parts[2].lower() != "ì—†ìŒ" else ""
            user_states[user_id] = state

            # ì„ í˜¸ë„ ì…ë ¥ í›„ ì²« ì¶”ì²œ ì‹œì‘
            new_recommendations = make_recommendations(
                user_text=state["user_text"],
                interest=state.get("interest"),
                region=state.get("region"),
                salary=state.get("salary"),
                shown_companies_set=state["shown"],
                top_n=3  # ì²« ì¶”ì²œì€ 3ê°œ
            )

            if not new_recommendations:
                return jsonify({"reply": "ì•„ì‰½ê²Œë„ í˜„ì¬ ì¡°ê±´ì— ë§ëŠ” ê¸°ì—…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì¡°ê±´ì„ ë§ì”€í•´ì£¼ì‹œê±°ë‚˜ 'ì¶”ì²œ ì´ˆê¸°í™”'ë¥¼ í†µí•´ ë‹¤ì‹œ ì‹œì‘í•´ ì£¼ì‹œê² ì–´ìš”?"})

            explanations = []
            for company, score in new_recommendations:
                # ì„ë² ë”© ì •ë³´ëŠ” í´ë¼ì´ì–¸íŠ¸ì—ê²Œ ë³´ë‚¼ í•„ìš” ì—†ìœ¼ë¯€ë¡œ ì œê±°
                company_info_for_gpt = {
                    "íšŒì‚¬ëª…": company.get("íšŒì‚¬ëª…", company.get("name", "ì •ë³´ ì—†ìŒ")),
                    "ì±„ìš©ê³µê³ ëª…": company.get("ì±„ìš©ê³µê³ ëª…", company.get("summary", "ì •ë³´ ì—†ìŒ")),
                    "ì§€ì—­": company.get("ì§€ì—­", company.get("region", "")),
                    "ì‚°ì—…": company.get("ì‚°ì—…", company.get("industry", "")),
                }
                reason = generate_reason_individual(state["user_text"], company_info_for_gpt, score)
                explanations.append(
                    f"**ê¸°ì—…ëª…**: {company_info_for_gpt['íšŒì‚¬ëª…']}\n"
                    f"**ì±„ìš©ê³µê³ ëª…**: {company_info_for_gpt['ì±„ìš©ê³µê³ ëª…']}\n"
                    f"**ì¢…í•© ì ìˆ˜**: {round(score,2)}\n"
                    f"**ì„¤ëª…**: {reason}\n"
                )

            reply = "\n\n".join(explanations)
            reply += "\n\nğŸ“Œ ë” ê¶ê¸ˆí•œ ì ì´ë‚˜ ê³ ë ¤í•˜ê³  ì‹¶ì€ ì¡°ê±´ì´ ìˆë‹¤ë©´ ë§ì”€í•´ ì£¼ì„¸ìš”. ì¶”ê°€ë¡œ ë°˜ì˜í•´ ë“œë¦´ê²Œìš”! ì˜ˆë¥¼ ë“¤ì–´ 'ë” ì¶”ì²œí•´ì¤˜'ë¼ê³  ë§ì”€í•˜ì‹œë©´ ë‹¤ë¥¸ ê¸°ì—…ì„ ì°¾ì•„ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
            return jsonify({"reply": reply})

        # 4. "ë” ì¶”ì²œí•´ì¤˜" ìš”ì²­ ì²˜ë¦¬
        if state["user_text"] is not None and state["interest"] is not None and "ë” ì¶”ì²œí•´ì¤˜" in message:
            new_recommendations = make_recommendations(
                user_text=state["user_text"],
                interest=state.get("interest"),
                region=state.get("region"),
                salary=state.get("salary"),
                shown_companies_set=state["shown"],
                top_n=1  # ì¶”ê°€ ì¶”ì²œì€ 1ê°œì”©
            )

            if not new_recommendations:
                return jsonify({"reply": "ë” ì´ìƒ ì¶”ì²œí•  ê¸°ì—…ì´ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì¡°ê±´ì„ ë§ì”€í•´ì£¼ì‹œê±°ë‚˜ 'ì¶”ì²œ ì´ˆê¸°í™”'ë¥¼ í†µí•´ ë‹¤ì‹œ ì‹œì‘í•´ ì£¼ì‹œê² ì–´ìš”?"})

            explanations = []
            for company, score in new_recommendations:
                company_info_for_gpt = {
                    "íšŒì‚¬ëª…": company.get("íšŒì‚¬ëª…", company.get("name", "ì •ë³´ ì—†ìŒ")),
                    "ì±„ìš©ê³µê³ ëª…": company.get("ì±„ìš©ê³µê³ ëª…", company.get("summary", "ì •ë³´ ì—†ìŒ")),
                    "ì§€ì—­": company.get("ì§€ì—­", company.get("region", "")),
                    "ì‚°ì—…": company.get("ì‚°ì—…", company.get("industry", "")),
                }
                reason = generate_reason_individual(state["user_text"], company_info_for_gpt, score)
                explanations.append(
                    f"**ê¸°ì—…ëª…**: {company_info_for_gpt['íšŒì‚¬ëª…']}\n"
                    f"**ì±„ìš©ê³µê³ ëª…**: {company_info_for_gpt['ì±„ìš©ê³µê³ ëª…']}\n"
                    f"**ì¢…í•© ì ìˆ˜**: {round(score,2)}\n"
                    f"**ì„¤ëª…**: {reason}\n"
                )

            reply = "\n\n".join(explanations)
            reply += "\n\nğŸ“Œ ë” ê¶ê¸ˆí•œ ì ì´ë‚˜ ê³ ë ¤í•˜ê³  ì‹¶ì€ ì¡°ê±´ì´ ìˆë‹¤ë©´ ë§ì”€í•´ ì£¼ì„¸ìš”. ì¶”ê°€ë¡œ ë°˜ì˜í•´ ë“œë¦´ê²Œìš”! ë˜ëŠ” 'ì¶”ì²œ ì´ˆê¸°í™”'ë¼ê³  ë§ì”€í•˜ì‹œë©´ ì²˜ìŒë¶€í„° ë‹¤ì‹œ ì‹œì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
            return jsonify({"reply": reply})

        # 5. "ì¶”ì²œ ì´ˆê¸°í™”" ìš”ì²­ ì²˜ë¦¬
        if "ì¶”ì²œ ì´ˆê¸°í™”" in message:
            # user_statesì—ì„œ í•´ë‹¹ ì‚¬ìš©ì IDì˜ ìƒíƒœë¥¼ ì™„ì „íˆ ì œê±°
            user_states.pop(user_id, None)
            return jsonify({"reply": "ì¶”ì²œ ìƒíƒœê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤. ìƒˆë¡œìš´ ìê¸°ì†Œê°œì„œ/ì´ë ¥ì„œ íŒŒì¼ì„ ì²¨ë¶€í•˜ì‹œê±°ë‚˜ ë‚´ìš©ì„ ì§ì ‘ ì…ë ¥í•´ ì£¼ì„¸ìš”."})

        # ê¸°íƒ€ ì¼ë°˜ ë©”ì‹œì§€ ì²˜ë¦¬
        return jsonify({"reply": "ë¬´ìŠ¨ ë§ì”€ì´ì‹ ì§€ ì •í™•íˆ ì´í•´í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ìê¸°ì†Œê°œì„œ/ì´ë ¥ì„œë¥¼ ì²¨ë¶€í•´ ì£¼ì‹œê±°ë‚˜, 'ì¶”ì²œ ì´ˆê¸°í™”'ë¥¼ í†µí•´ ë‹¤ì‹œ ì‹œì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."})

    except Exception as e:
        print(f"âŒ ì„œë²„ ì—ëŸ¬: {e}")
        traceback.print_exc() # ì„œë²„ ì „ì²´ ì˜¤ë¥˜ ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤ ì¶œë ¥
        return jsonify({"reply": f"âŒ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)} ë¶ˆí¸ì„ ë“œë ¤ ì£„ì†¡í•©ë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì„¸ìš”."}), 500
        
@app.route("/health", methods=["GET"])
def health_check():
    return "OK", 200
    
if __name__ == "__main__":
    app.run(debug=True, host="0.0.0.0", port=8080)
